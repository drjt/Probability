<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="BernoulliBasedDistributions" xmlns:xi="http://www.w3.org/2001/XInclude">

<title>Distributions based upon Bernoulli Trials</title>

<section xml:id="BernoulliIntroduction"><title>Introduction</title>
	<p>
	Many practical problems involve measuring simply whether something was a success or a failure. In these situations, "success" should not be interpreted as having any moral or subjective meaning but only construed to mean that something you are looking for actually occurs.
	</p>
	
	<p>
	In situation where a single trial is performed and the result is determined only to be a success or failure is called a Bernoulli event. Indeed, one could create a corresponding probability function using a random variable <m>X</m> over the space <m>R = \{0, 1 \}</m> mapping <m>X</m>(success) = 1 and <m>X</m>(Failure)=0. If p = P(Success) then 
	<me>f(x) = p^x \cdot (1-p)^{1-x}</me>
	would be a formula but which only related to two values: 
	<me>P(\text{Failure}) = f(0) = (1-p)</me>
	<me>P(\text{Success}) = f(1) = p</me>  
	</p>
	<p>
	Notice that p=0 means that you will always get a failure and that p=1 means that you will always get a success. In these cases, <m>X</m> would no longer be a random variable since the outcome for <m>X</m> could be predicted with certainty. Therefore, we will always assume that <m>0 &lt; p &lt; 1</m>.
	</p>
	<p>
	The Bernoulli distribution on its own is not extremely useful but serves as a starting point for several others that are useful.  Indeed, in this chapter you will investigate distributions that relate some number of successes in multiple trials to some number of independent trials. The difference between these distributions will be that one of these variables will be fixed and the other one will be variable.
</p>
<p>
In this chapter, you will investigate the following distributions:
<ol>
	<li>Binomial - the number of trials is fixed and <m>X</m> measures the variable number of successes</li>
	<li>Geometric - the number of successes is fixed--at 1--and <m>X</m> measures the variable number of trials</li>
	<li>Negative Binomial - the number of successes is fixed and <m>X</m> measures the variable number of trials</li>
</ol>
</p>

</section>

<section xml:id="BinomialDistribution">
 	<title>Binomial Distribution</title>	
	<p>Consider a sequence  of n independent Bernoulli trials with the likelihood of a success p on each individual trial stays constant from trial to trial with <m> 0 \lt p \lt 1 </m>. If we let the variable <m>X</m> measure the number of successes obtained when doing a fixed number of trials n with <m>R = \{ 0, 1, ..., n \}</m>, then the resulting distribution of probabilities is called a Binomial Distribution.</p>

<p>
Now, let's determine the actual probability function for this distribution.
</p>
<p>
<theorem xml:id="BinomialProbabilityFunctionTheorem"><title>Derivation of Binomial Probability Function</title>
	<statement>
	<p xml:id="BinomialProbabilityFunction">
	For R = {0, 1, ..., n},
	<me>f(x) = \binom{n}{x}p^x(1-p)^{n-x}</me>
	</p>
	</statement>
		<proof>
		<p> Since successive trials are independent, then the probability of X successes occurring within n trials is given by 
		<me>P(X=x) = \binom{n}{x}P(SS...SFF...F) = 		
						\binom{n}{x}p^x(1-p)^{n-x}</me>
		</p>
		</proof>
</theorem>
</p>

<p>
<theorem><title>Verification of Binomial Distribution Formula</title>
	<statement>
	<p>
	<me>\sum_{x \in R} f(x) = \sum_{x=0}^n \binom{n}{x}p^x(1-p)^{n-x} = 1.</me>
	</p>
	</statement>
	<proof>
	<p>
		Using the Binomial Theorem with a = p and b = 1-p yields
		<me>\sum_{x=0}^n \binom{n}{x}p^x(1-p)^{n-x} = (p + (1-p))^n = 1</me>
	</p>
	</proof>
</theorem>
</p>

<p>
The following interactive cell illustrates the range of choices for a Binomial setup with fixed N and fixed p:
</p>
<p>
<sage>
<input>
# Binomial distribution over 0 .. N
# N and p need to be given
var('x')
@interact(layout=dict(top=[['N','p']]))
def _(N=input_box(10,width=5,label='$$ N $$'),
    p=input_box(3/10,width=5,label='$$ p $$')):
    f = binomial(N,x)*p^x*(1-p)^(N-x)
    @interact
    def __(X = slider(0,N,1)):
        Px = f(x=X)
        Pxapprox = Px.n(digits=8)
        formula = "\\binom{%s}"%str(N)+"{%s}"%str(X)+"({%s})"%str(p)+"^{%s}"%str(X)+"(1-{%s})"%str(p)+"^{%s}"%str(N-X)
        pretty_print(html("P(X = %s"%str(X)+") = $ %s $"%str(formula)+" = $ %s $"%str(latex(Px))+"$ \\approx %s $"%str(Pxapprox)))
        G = polygon([(0,0), (1,0), (1,1), (0,1)],thickness=10,color='blue',alpha=0.7,zorder=1)

        if X &gt; 0:
            R =[j for j in range(N)] 
            selected = sample(R, X)
            for k in selected:
                xk = sqrt(k)*cos(k)/N^(4/5)+0.5
                yk = sqrt(k)*sin(k)/N^(4/5)+0.5
                G += circle((xk,yk),0.05,fill=True,facecolor='red',zorder=3) 
                G += text('p',(xk,yk))

        for k in range(N):
            xk = sqrt(k)*cos(k)/N^(4/5)+0.5
            yk = sqrt(k)*sin(k)/N^(4/5)+0.5
            G += circle((xk,yk),0.05,zorder=2,fill=True, facecolor="white")
   
        show(G, axes=0, figsize=(4,4) )
</input>
</sage>
</p>

<example><title>Flipping a coin a fixed number of times</title>
<p>Let's consider a simple example for flipping coins. Indeed, suppose you flip a coin exactly 20 times and need to determine the probability of getting exactly 10 heads.
</p>

<p>
This is binomial with n = 20, p = 1/2 and you are looking for f(10). With these values
<me>f(10) = \binom{20}{10} \cdot \left ( \frac{1}{2} \right )^{10} \cdot \left ( \frac{1}{2} \right 	)^{20-10} = \frac{46189}{262144} \approx 0.176</me>

Note that the mean for this distribution is also 10 so one might expect 10 heads in general
</p>

<p>
If you rather would prefer to determine the probability of getting 10 or fewer heads requires F(10) = f(0) + f(1) + ... + f(10). There is no "nice" formula for F but this calculation can be performed using a graphing calculator, such as the TI-84 with F(x) = binomcdf(n,p,x). In this case, F(10) = binomcdf(20,1/2,10) = 0.588.
</p>
</example>

<p>
<exercise><title>WebWork - Binomial</title>

		<webwork source="Library/Mizzou/Finite_Math/Probability_Introduction/Coin.pg">
		</webwork>
</exercise>
</p>


<example><title>Testing critical components</title>
<p>
Often one will test a critical system components for failure and toward that end collect a sample of 100 of these components from the manufacturer. Suppose the component is listed as having a p = 0.01 probability of breaking and you want to know the likelihood that at most 1 of the tested components actually fails when tested. You find it reasonable to presume that different components succeed or fail independently of each other. So, you can model this situation with a binomial distribution. 
</p>
<p>
If <m>X</m> measures the number of components that fail when tested, the specific probability function is given by
<me>f(x) = \binom{100}{x} 0.01^x 0.99^{100-x}.</me>
The probability that at most one component fails is then given by
<me>F(1) = f(0) + f(1) = \binom{100}{0} 0.01^0 0.99^{100} + \binom{100}{1} 0.01^1 0.99^99 \\ = 0.99^{100} + 100 \cdot 0.01 \cdot 0.99^{99} = 0.99^{99}(0.99 + 100 \cdot 0.01 \\ = 0.99^{99} \cdot 1.99 \approx 0.73576.</me>
</p>
</example>

	
<p>
Utilize the interactive cell below to compute <m>f(x)</m> and <m>F(x)</m> for the Binomial distribution
</p>

<p>
<sage>
	<input>
# Binomial calculator
@interact
def _(p=input_box(3/10,width=15),n=input_box(10,width=15)):
    R = range(n+1)
    f(x) = binomial(n,x)*p^x*(1-p)^(n-x)
    acc = 0
    pretty_print("Binomial Calculator")
    for k in R:
        prob = f(x=k)
        acc = acc+prob
        pretty_print('f(%s) = '%k,' %.8f'%prob,' and F(%s) = '%k,' %.8f'%acc)
	</input>
</sage>
</p>

<p>
<theorem xml:id="BinomialStatisticsTheorem"><title>Binomial Distribution Statistics</title>
	<statement>
	<p xml:id="BinomialStatistics">For the Binomial Distribution
	<me>\mu = np</me>
	<me>\sigma^2 = np(1-p)</me>
	<me>\gamma_1 = \frac{1-2p}{\sqrt{np(1-p)}}</me>
	<me>\gamma_2 = \frac{1-6p(1-p)}{np(1-p)} + 3</me>
	</p>
	</statement>

<proof>
	<p>
		For the <xref ref="TheoreticalMean">mean</xref>,
		<md>
			<mrow> \mu &amp; = E[X] </mrow>
			<mrow> &amp; = \sum_{x=0}^{n} {x \binom{n}{x} p^x (1-p)^{n-x}}</mrow>
			<mrow> &amp; = \sum_{x=1}^{n} {x \frac{n(n-1)!}{x(x-1)!(n-x)!} p^x (1-p)^{n-x}}</mrow>
			<mrow> &amp; = np \sum_{x=1}^{n} {\frac{(n-1)!}{(x-1)!((n-1)-(x-1))!} p^{x-1} (1-p)^{(n-1)-(x-1)}}</mrow>
		</md>

		Using the change of variables <m>k=x-1</m> and <m>m = n-1</m> yields a binomial series
		<md>
			<mrow> &amp; = np \sum_{k=0}^{m} {\frac{m!}{k!(m-k)!} p^k (1-p)^{m-k}}</mrow>
			<mrow> &amp; = np (p + (1-p))^m = np</mrow>
		</md>
	</p>

	<p>For the <xref ref="TheoreticalVariance">variance</xref>,
		<md>
		<mrow> \sigma^2 &amp; = E[X(X-1)] + \mu - \mu^2 </mrow>
		<mrow> &amp; = \sum_{x=0}^{n} {x(x-1) \binom{n}{x} p^x (1-p)^{n-x}} + np - n^2p^2</mrow>
		<mrow> &amp; = \sum_{x=2}^{n} {x(x-1) \frac{n(n-1)(n-2)!}{x(x-1)(x-2)!(n-x)!} p^x (1-p)^{n-x}}  + np - n^2p^2</mrow>
		<mrow> = n(n-1)p^2 \sum_{x=2}^{n} {\frac{(n-2)!}{(x-2)!((n-2)-(x-2))!} p^{x-2} (1-p)^{(n-2)-(x-2)}} + np - n^2p^2</mrow>
		</md>
	</p>

	<p>Using the change of variables <m>k=x-2</m> and <m>m = n-2</m> yields a binomial series
	<md>
		<mrow> &amp; = n(n-1)p^2  \sum_{k=0}^{m} {\frac{m!}{k!(m-k)!} p^k (1-p)^{m-k}} + np - n^2p^2</mrow>
		<mrow> &amp; = n(n-1)p^2 + np - n^2p^2 = np - np^2 = np(1-p)</mrow>
	</md>
	</p>


	<p>The <xref ref="TheoreticalSkewness">skewness</xref> and <xref ref="TheoreticalKurtosis">kurtosis</xref> can be found similarly using formulas involving E[X(X-1)(X-2)] and E[X(X-1)(X-2)(X-3)]. The complete determination is performed using Sage below.
	</p>

</proof>
</theorem>
</p>

<p>The following uses Sage to symbolically confirm the general formulas for the Binomial distribution.
</p>

<p>
<sage>
<input>
var('x,n,p')
assume(x,'integer')
f(x) = binomial(n,x)*p^x*(1-p)^(n-x)
mu = sum(x*f,x,0,n)
M2 = sum(x^2*f,x,0,n)
M3 = sum(x^3*f,x,0,n)
M4 = sum(x^4*f,x,0,n)

pretty_print('Mean = ',mu)

v = (M2-mu^2).factor()
pretty_print('Variance = ',v)
stand = sqrt(v)

sk = ((M3 - 3*M2*mu + 2*mu^3)).factor()/stand^3
pretty_print('Skewness = ',sk)

kurt = (M4 - 4*M3*mu + 6*M2*mu^2 -3*mu^4).factor()/stand^4
pretty_print('Kurtosis = ',(kurt-3).factor(),'+3')
</input>
</sage>
</p>


<p>
<exercise><title>WebWork - Binomial</title>

		<webwork source="Library/Rochester/setProbability8BinomialDist/ur_pb_8_6.pg">
		</webwork>
</exercise>
</p>

<p>
<sage language='r'>
<input>
n = 10
p = 0.3
X = 0:n    # the space R of the random variable 
mu = n*p      # the formula for mean of the Binomial Distributions
sdev = sqrt(n*p*(1-p))  # the formula for the standard deviation
if(n &lt; 101){
dbinom( X, n, p )   # let's print out a bunch of actual probs if N reasonable
}

Pbinom = dbinom(X, n, p )  # create the probability function over X

Psample = rbinom(10^6, n, p)  # to create a histogram, sample a lot
Xtop=max(Psample)          # for scaling the x-axis. Shift by 1/2 below.
hist(Psample, prob=TRUE, br=(-1:Xtop)+0.5, col="skyblue2", xlab="X", 
  main="Binomial Probability Function vs Approximating 'Bell Curve'")

points(X, Pbinom, pch=19, col="darkgreen")  # to create actual (x,f(x))

Pnormal &lt;- function(X){dnorm(X, mean=mu, sd=sdev)}   # to overlap a bell curve
curve(Pnormal, col="red", lwd=2, add=TRUE) 
</input>
</sage>
</p>

<p>
You can of course get specific values and graph the Binomial Distribution using R as well...
</p>

<p>
<sage language='r'>
<input>
n &lt;- 10
p &lt;- 0.3

paste('Probability Function')
dbinom(0:n, n, p)   # gives the probability function
paste('Distribution function')
pbinom(0:n, n, p)   # gives the distribution function
paste('A random sample')
rbinom(15, n, p)    # gives a random sample of 15 items from b(n,p)

x &lt;- dbinom(0:n, size=n, prob=p)
barplot(x,names.arg=0:n, main=sprintf(paste('n=',n,' and p= ',p)))

</input>
</sage>
</p>

</section>



<section xml:id="GeometricDistribution">
	<title>Geometric Distribution</title>
	<p>Consider the situation where one can observe a sequence  of independent
	trials where the likelihood of a success on each individual trial
	stays constant from trial to trial. Call this likelihood the probably of
	"success" and denote its value by <m>p</m> 
	where <m> 0 \lt p \lt 1 </m>.  
	If we let the variable <m>X</m> measure the number of trials needed in order
	to obtain the first success with <m>R = \{1, 2, 3, ... \}</m>, 
	then the resulting distribution of probabilities is called a 
	Geometric Distribution.</p>

	<p>
	<theorem xml:id="GeometricProbabilityFunction">
	<statement>
	<p>For a Geometric variable <m>X</m> with <m>R = \{1, 2, 3, ... \}</m>,
	<me>f(x) = (1-p)^{x-1} \cdot p</me>
	</p>
	</statement>
	<proof>
	<p> Since successive trials are independent, then the probability 
	of the first success occurring on the mth trial presumes that
	the previous m-1 trials were all failures.  Therefore the 
	desired probability is given by 
		<me>f(x) = P(X = x) = P(FF...FS) = (1-p)^{x-1}p</me>
	</p>
	</proof>
	</theorem>
	</p>

<example><title>Flipping a die until getting a success</title>
<p>Let's consider a simple example for rolling a 24-sided die until you get a multiple of 9...that is, either a 9 or an 18. Successive rolls of a die would appear to be independent events and the probability of getting a 9 or 18 on any given roll is <m>p = \frac{1}{12}</m>. What is then the likelihood that it takes more than three rolls in order for you to get your first success?
</p>

<p>
This is easily modeled by a geometric distribution and you are looking for 
<me>P(X > 3) = 1 - P(X \le 3) = 1 - F(3) = 1 - f(1) - f(2) - f(3) \\
1 - \frac{1}{12} - \frac{11}{12} \cdot \frac{1}{12} - (\frac{11}{12})^2 \frac{1}{12} \approx 0.77025.</me> 
</p>
</example>

<example><title>Testing a critical component until failure</title>	
<p>
Often one will test a critical system component until it fails to see how long the component works. Suppose you have a particular component that on any given trial has a p=0.01 probability of breaking. You also might find it reasonable to presume that succesive trials are independent which could be the case if the component shows no wear from trial to trial. So, you can model this situation with a geometric distribution and the probability that the component fails on the x-th trial is given by
<me>f(x) = 0.99^{x-1} \cdot 0.01.</me>
For some reason, you might be interested in whether the component fails on the 5th trial. The probability of this outcome is given by
<me>f(5) = 0.99^4 \cdot 0.01 \approx 0.0096</me>
so it is unlikely that the component will fail on exactly the 5th trial. However, what about failing on one of the first five trials? Then, you would need
<me>F(5) = f(1)+f(2)+f(3)+f(4)+f(5) \\ = 0.01 + 0.99 \cdot 0.01 + 0.99^2 \cdot 0.01 + 0.99^3 \cdot 0.01 + 0.99^4 \cdot 0.01 \approx 0.049</me>
which is still relatively small. Indeed, with such a small probability of failure, you might expect the component to last for some time. Indeed, we will uncover a formula below for the number of trials, on average, you might expect before failure.
</p>
</example>

<p>
Let's go ahead and verify this probability function and investigate some of the geometric distribution's properties.
</p>
	
<p>
<theorem><title>Geometric Distribution sums to 1</title>
<statement>
		<p>
		<me>f(x) = (1-p)^{x-1}p</me> 
		sums to 1 over <m>R = \{ 1, 2, ... \}</m>
		</p>
</statement>
<proof>
		<p>
		<me>\sum_{x=1}^{\infty} {f(x)} = \sum_{x=1}^{\infty} {(1-p)^{x-1} p} \\ = p \sum_{j=0}^{\infty} {(1-p)^j} = p \frac{1}{1-(1-p)} = 1</me>
		using a change of variables j = x-1 and the known value for the sum of the geometric series.
		</p>
</proof>
</theorem>
</p>
	
<p>	
<sage>
<input>
# Geometric distribution over 0 .. n
# Probability of success on one independent trial = p must also be given
var('x')
# n = 50 by default. actually should be infinite
@interact
def _(p=input_box(1/10,label='p = ',width=10),n=[25,50,75,100,200]):
    np1 = n+1
    R = range(1,np1)
    f(x) = (1-p)^(x-1)*p
    F(x) = 1 - (1-p)^x
    pretty_print(html('Density Function: $f(x) =%s$'%str(latex(f(x)))+' over the space $R = %s$'%str(R)))
    points((k,f(x=k)) for k in R).show(title="Probability Function")
    print
    points((k,F(x=k)) for k in R).show(title="Distribution Function")
    if (n == 25):
        for k in R:
            pretty_print(html('$f(%s'%k+') = %s'%latex(f(x=k))+' \\approx %s$'%f(x=k).n(digits=5)))
</input>
</sage>
</p>

<p>	
	<theorem xml:id="GeometricStatistics"><title>Geometric Statistics Theorem</title>
		<statement>
		<p>
		For the geometric distribution, 
			<me>\mu = 1/p</me>
			<me>\sigma^2  = \frac{1-p}{p^2}</me>
			<me>\gamma_1 = \frac{2-p}{\sqrt{1-p}}</me>
			<me>\gamma_2 = \frac{p^2-6p+6}{1-p} + 3</me>

		</p>
		</statement>
		<proof>
			<p> For the <xref ref="TheoreticalMean">mean</xref>,
			<md>
				<mrow>\mu &amp; = E[X] = \sum_{x=1}^{\infty} {x(1-p)^{x-1}p}</mrow>
				<mrow> &amp; = p \sum_{x=1}^{\infty} {x(1-p)^{x-1}}</mrow>
				<mrow> &amp; = p \frac{1}{(1-(1-p))^2}</mrow>
				<mrow> &amp; = p \frac{1}{p^2} = \frac{1}{p}</mrow>
			</md>
			</p>

			<p> For the <xref ref="TheoreticalVariance">variance</xref>,
			<md>
				<mrow>\sigma^2 &amp; = E[X(X-1)] + \mu - \mu^2 </mrow>
				<mrow> &amp; = \sum_{x=1}^{\infty} {x(x-1)(1-p)^{x-1}p} + \mu - \mu^2 </mrow>
				<mrow> &amp; = (1-p)p \sum_{x=2}^{\infty} {x(x-1)(1-p)^{x-2}} + \frac{1}{p} - \frac{1}{p^2}</mrow>
				<mrow> &amp; = (1-p)p \frac{2}{(1-(1-p))^3} + \frac{1}{p} - \frac{1}{p^2}</mrow>
				<mrow> &amp; = \frac{1-p}{p^2}</mrow>
			</md>
			</p>
			</proof>
	</theorem> 
</p>
	
<p>
	<theorem>
		<title>Geometric Distribution Function</title>
		<statement>
		<p>
			<me>F(x) =  1- (1-p)^{x}</me>
		</p>
		</statement>
		<proof>
		<p> Consider the accumulated probabilities over a range of values...
		<md>
			<mrow> P(X \le x) &amp; = 1 - P(X \gt x)</mrow>
			<mrow> &amp; = 1- \sum_{k={x+1}}^{\infty} {(1-p)^{k-1}p}</mrow>
			<mrow> &amp; = 1- p \frac{(1-p)^{x}}{1-(1-p)}</mrow>
			<mrow> &amp; = 1- (1-p)^{x}</mrow>
		</md>
		</p>
		</proof>
	</theorem> 
</p>

<p>	
	<theorem>
		<title>Statistics for Geometric Distribution</title>
		<statement>
		<p>Mean, Variance, <xref ref="TheoreticalSkewness">Skewness</xref>, <xref ref="TheoreticalKurtosis">Kurtosis</xref> computed by Sage.
		</p>
		</statement>
<proof>
<p>
See the interactive Sage cell below...
</p>
</proof>
</theorem>
<sage>
	<input>
var('x,n,p')
assume(x,'integer')
f(x) = p*(1-p)^(x-1)
mu = sum(x*f,x,0,oo).full_simplify()
M2 = sum(x^2*f,x,0,oo).full_simplify()
M3 = sum(x^3*f,x,0,oo).full_simplify()
M4 = sum(x^4*f,x,0,oo).full_simplify()

pretty_print('Mean = ',mu)

v = (M2-mu^2).factor().full_simplify()
pretty_print('Variance = ',v)
stand = sqrt(v)

sk = (((M3 - 3*M2*mu + 2*mu^3))/stand^3).full_simplify()
pretty_print('Skewness = ',sk)

kurt = (M4 - 4*M3*mu + 6*M2*mu^2 -3*mu^4).factor()/stand^4
pretty_print('Kurtosis = ',(kurt-3).factor(),'+3')
	</input>
</sage>
</p>



<p>
	<theorem xml:id="TheoremGeometricMemoryless"><title>The Geometric Distribution yields a memoryless model.</title>
	<statement>
	<p>
	If X has a geometric distribution and a and b are nonnegative integers, then
	<me>P( X &gt; a + b | X &gt; b ) = P( X &gt; a)</me>
	</p>
	</statement>
	<proof>
	<p>
	Using the definition of conditional probability,
	<md>
		<mrow>P( X &gt; a + b | X &gt; b ) &amp; = \frac{P( X &gt; a + b \cap X &gt; b )}{P( X &gt; b)}</mrow>
		<mrow> &amp; = \frac{P( X &gt; a + b )}{P( X &gt; b)}</mrow>
		<mrow> &amp; = \frac{(1-p)^{a+b}}{(1-p)^b}</mrow>
		<mrow> &amp; = (1-p)^a</mrow>
		<mrow> &amp; = P(X &gt; a)</mrow>
	</md>
	</p>
	</proof>
	</theorem>
</p>

</section>

<section xml:id="NegativeBinomial"><title>Negative Binomial</title>	
	<p>Consider the situation where one can observe a sequence  of independent
	trials where the likelihood of a success on each individual trial
	stays constant from trial to trial. Call this likelihood the probably of
	"success" and denote its value by <m>p</m> 
	where <m> 0 \lt p \lt 1 </m>.  
	If we let the variable <m>X</m> measure the number of trials needed in order
	to obtain the rth success, <m>r \ge 1</m>, with 
	<m>R = \{r, r+1, r+2, ... \}</m>
	then the resulting distribution of probabilities is called a 
	Geometric Distribution.</p>
	<p>Note that r=1 gives the Geometric Distribution.</p>

<p>	
<theorem xml:id='NegBinomSeries'><title>Negative Binomial Series</title>
	<statement> 
		<p>
			<me>\displaystyle \frac{1}{(a+b)^n} = \sum_{k=0}^{\infty} {(-1)^k \binom{n + k - 1}{k} a^k b^{-n-k}}</me> 
		</p>
	</statement>
	<proof>					
		<p>First, convert the problem to a slightly different form:		
		<m> \frac{1}{(a+b)^n} = \frac{1}{b^n} \frac{1}{(\frac{a}{b}+1)^n} 
					 = \frac{1}{b^n} \sum_{k=0}^{\infty} {(-1)^k \binom{n + k - 1}{k} \left ( \frac{a}{b} \right ) ^k}
		</m>
		</p>

<p>So, let's replace <m>\frac{a}{b} = x</m> and ignore for a while the term factored out. Then, we only need to show 
	<me>\sum_{k=0}^{\infty} {(-1)^k \binom{n + k - 1}{k} x^k} = \left ( \frac{1}{1+x} \right )^n </me>
However
	<md>
	<mrow> \left ( \frac{1}{1+x} \right )^n &amp; = \left ( \frac{1}{1 - (-x)} \right )^n </mrow>
	<mrow> &amp; = \left ( \sum_{k=0}^{\infty} {(-1)^k x^k} \right )^n</mrow>
	</md>
</p>
<p>This infinite sum raised to a power can be expanded by distributing terms in the standard way. In doing so, the various powers of x multiplied together
will create a series in powers of x involving <m>x^0, x^1, x^2, ...</m>.  
To detemine the final coefficients notice that the number of time <m>m^k</m> will appear in this product depends upon the number of ways one can write k as a sum of nonnegative integers.
</p>
			
<p>For example, the coefficient of <m>x^3</m> will come from the n ways of multiplying the coefficients 
<m>x^3, x^0, ..., x^0</m> and <m>x^2, x^1, x^0, ..., x^0</m>
and <m>x^1, x^1, x^1, x^0,..., x^0</m>. This is equivalent to finding the number of ways to write the number k as a sum of nonnegative integers. The possible set of nonnegative integers is {0,1,2,...,k} and one way to count the combinations is to separate k *'s by n-1 |'s.  For example, if k = 3 then *||** means 
<m>x^1 x^0 x^2 = x^3</m>. Similarly for k = 5 and |**|*|**| implies <m> x^0 x^2 x^1 x^2 x^0 = x^5</m>.  The number of ways to interchange the identical *'s among the idential |'s is <m>\binom{n+k-1}{k}</m>. </p>
			 
<p>Furthermore, to obtain an even power of x will require an even number of odd powers and an odd power of x will require an odd number of odd powers. So, the coefficient of the odd terms stays odd and the coefficient of the even terms remains even. Therefore,	
<me> \left ( \frac{1}{1+x} \right )^n = \sum_{k=0}^{\infty} {(-1)^k \binom{n + k - 1}{k} x^k}</me>	
Similarly,
<me> \left ( \frac{1}{1-x} \right )^n = \left ( \sum_{k=0}^{\infty} {x^k} \right )^n = \sum_{k=0}^{\infty} {\binom{n + k - 1}{k} x^k}</me>
</p>

</proof>	
</theorem>	
</p>
			
<p>Consider the situation where one can observe a sequence of independent trials with the likelihood of a success on each individual trial <m>p</m> where 
	<m> 0 \lt p \lt 1 </m>.  
For a positive integer r, let the variable <m>X</m> measure the number of trials needed in order to obtain the rth success. Then the resulting distribution of probabilities is called a Negative Binomial Distribution.</p>

<p>
<theorem xml:id="NegativeBinomialProbabilityFunction"><title>Negative Binomial Probability Function</title>
<statement>
	<p>
	<me>f(x) = \binom{x - 1}{r-1}(1-p)^{x-r}p^r,</me>
	for <m>x \in R = \{r, r+1, ... \}</m>.
	</p>
</statement>
<proof>
	<p> Since successive trials are independent, then the probability of the rth success occurring on the x-th trial presumes that in the previous x-1 trials were r-1 successes and x-r failures. You can arrange these indistinguishable successes (and failures) in <m>\binom{x-1}{r-1}</m> unique ways. Therefore the desired probability is given by 
			<me>P(X=x) = \binom{x - 1}{r-1}(1-p)^{x-r}p^r</me>
	</p>
</proof>
</theorem>
</p>

<example><title>Flipping a die until getting a third success</title>
<p>Once again, consider rolling our 24-sided die until you get a multiple of 9...that is, either a 9 or an 18...for the third time. Once again, the probability of getting a 9 or 18 on any given roll is <m>p = \frac{1}{12}</m> but since we will continue rolling until we get a success for the third time, this is modeled by a negative binomial distribution and you are looking for 
<me>f(x) = \binom{x-1}{2}(\frac{11}{12})^{x-3} (\frac{1}{12})^3  \\
= \frac{(x-1)(x-2)}{2}(\frac{11}{12})^{x-3} (\frac{1}{12})^3</me>  
</p>
<p>
Computing f(x) for any given x is relatively painless but computing F(x) could take some effort. There is generally not a graphing calculator distribution option for negative binomial but the interactive cells below can be utilized to help with the tedious computations. For example, if you were interested in <m>P(X \lt 20) = P(X \le 19 ) = F(19)</m> then the interactive calculator below using r=3 and <m>p = \frac{1}{12}</m> gives <m>F(19) \approx 0.20737.</m> 
</p>
</example>

<example><title>Testing a critical component until failure</title>	
<p>
Let's once again test a series of critical system components until you find two that fail. Again, suppose a particular component has a p=0.01 probability of breaking on any given trial. Since you will stop when you encounter the third failure, you can model this situation with a negative binomial distribution and the probability that the 2nd component fails on the x-th trial is given by
<me>f(x) = \binom{x-1}{2-1} 0.99^{x-2} \cdot 0.01^2 = (x-1) \cdot 0.99^{x-2} \cdot 0.01^2.</me>
Once again, let's compute the likelihood that you get the second failure on one of the first five trials. Then, 
<me>F(5) = f(2)+f(3)+f(4)+f(5) \\ = 0.01^2 + 2 \cdot 0.99 \cdot 0.01^2 + 3 \cdot 0.99^2 \cdot 0.01^2 + 4 \cdot 0.99^3 \cdot 0.01^2 \approx some nice number</me>
which is still relatively small. 
</p>
</example>

<p>
<theorem xml:id="NegativeBinomialSumsToOne"><title>Negative Binomial Distribution Sums to 1</title>
<statement>>
	<p>
		<me>\sum_{x=r}^{\infty} {\binom{x - 1}{r-1}(1-p)^{x-r}p^r} = 1</me>
	</p>
</statement>
<proof>
	<p>
	<me>\sum_{x=r}^{\infty} {\binom{x - 1}{r-1}(1-p)^{x-r}p^r} = p^r \sum_{x=r}^{\infty} {\binom{x - 1}{r-1}(1-p)^{x-r}}</me>
		<p>and by using <m>k = x-r</m></p>
	<md>
		<mrow> &amp; = p^r \sum_{k=0}^{\infty} {\binom{r + k - 1}{k}(1-p)^k}</mrow>
		<mrow> &amp; = p^r \frac{1}{(1-(1-p))^r}</mrow>
		<mrow> &amp; = 1</mrow>
	</md>
	</p>
</proof>
</theorem>
</p>	

<p>Below, the interactive cell symbolically computes f(x) and F(x) for the negative binomial distribution.</p>

<p>
<sage>
<input>
# Negative Binomial calculator
@interact
def _(p=input_box(1/12,width=15),r=slider(1,10,1,2)):
    n = 4*(floor(r/p)+1)
    np1 = n+1
    R = range(r,np1)
    f(x) = (factorial(x-1)/(factorial(r-1)*factorial(x-r)))*(1-p)^(x-r)*p^r
    acc = 0
    for k in R:
        prob = f(x=k)
        acc = acc+prob
        pretty_print('f(%s) = '%k,' %.8f'%prob,' and F(%s) = '%k,' %.8f'%acc)
</input>
</sage>
</p>


<!-- Based upon
<webwork source="Library/UBC/STAT/STAT302/HW06/HW06-02.pg">
-->
<exercise>
<webwork>
<statement>
<p>
A telephone saleswoman arranges a sequence of interviews of potential customers in order to sell them an insurance policy. She believes that her success rate in completing a sale in any interview is 11 %. Provide answers to at least 3 decimal places.
</p>
<p>Determine the probability that she fails to make a sale on the first five interviews:  <var name="0.558" width="10" />
</p>

<p>
Determine the probability that she makes her first sale on the fourth interview: <var name="0.078" width="10" />
</p>

<p>
Determine the probability that the second sale is made on the sixth interview: <var name="0.04209" width="10" />
</p>
</statement>
<solution>
<p>Using geometric,
<me>P(X \gt 5) = 1 - F(5) = (1-p)^5 = 0.89^5 \approx 0.558</me>
</p>
<p>Using geometric,
<me>P(X = 4) = f(4) = (1-p)^3 \cdot p = 0.89^3 \cdot 0.11 \approx 0.078</me>
</p>
<p>Using negative binomial with r=2,
<me>P(X = 6) = f(6) = 5 (1-p)^4 \cdot p^2 = 5 \cdot 0.89^4 \cdot 0.11^2 \approx 0.04209</me>
</p>

</solution>
</webwork>
</exercise>

<p>
<theorem xml:id="NegativeBinomialStatistics"><title>Statistics for Negative Binomial Distribution</title>
<statement>
	<p>
		For the Negative Binomial Distribution, 
		<me>\mu = \frac{r}{p}</me>
		<me>\sigma^2 = r \frac{1-p}{p^2}</me>
		<me>\gamma_1 = \frac{2-p}{\sqrt{r(1-p)}}</me>
		<me>\gamma_2 = \frac{p^2-6p+6}{r(1-p)} + 3</me>
	</p>
</statement>
<proof>	
See interactive cell below...
</proof>
</theorem>
</p>
<p>
<sage>
<input>
# Negative Binomial
var('x,n,p,r,alpha')
assume(x,'integer')
assume(alpha,'integer')
assume(alpha &gt; 2)
assume(0 &lt; p &lt; 1)
@interact
def _(r=[2,5,10,15,alpha]):
    f(x) = binomial(x-1,r-1)*p^r*(1-p)^(x-r)
    mu = sum(x*f,x,r,oo).full_simplify()
    M2 = sum(x^2*f,x,r,oo).full_simplify()
    M3 = sum(x^3*f,x,r,oo).full_simplify()
    M4 = sum(x^4*f,x,r,oo).full_simplify()
        
    pretty_print('Mean = ',mu)
    
    v = (M2-mu^2).full_simplify()
    pretty_print('Variance = ',v)
    stand = sqrt(v)
    
    sk = (((M3 - 3*M2*mu + 2*mu^3)).full_simplify()/stand^3).factor()
    pretty_print('Skewness = ',sk)
    
    kurt = ((M4 - 4*M3*mu + 6*M2*mu^2 -3*mu^4)/v^2).full_simplify()
    pretty_print('Kurtosis = ',(kurt-3).factor(),'+3')
</input>
</sage>
</p>


<p>
<sage language='r'>
<input>
r = 3
p = 0.4
M = 20*r
X = r:M    # the space R of the random variable should be infinite 
Xshift = 0:(M-r)   # because this is the way r does dbinom, etc. by letting
                   # x measure the number of failures rather than the number
                   # of total trials.  So, our x=r would be 0 failures and so
                   # r would count that as x=0.
mu = r/p      # the formula for mean of the Binomial Distributions
sdev = sqrt(r*(1-p)/p^2)  # the formula for the standard deviation
if(M &lt; 101){
dnbinom( Xshift, r, p )   # let's print out a bunch of actual probs
}
Pnbinom = dnbinom(Xshift, r, p )  # create the probability function over X

Psample = rnbinom(10^6, r, p)  # to create a histogram, sample a lot

Xtop=max(Psample)          # for scaling the x-axis. Shift by 1/2 below.
hist(Psample, prob=TRUE, br=(-1:Xtop)+0.5, col="skyblue2", xlab="X - r", 
  main="Negative Binomial Probability Function vs Approximating 'Bell Curve'")

points(Xshift, Pnbinom, pch=19, col="darkgreen")  # to create actual (x,f(x))

Pnormal &lt;- function(X){dnorm(X, mean=mu-r, sd=sdev)}   
# to overlap a bell curve, shifting our mean by r in order to fit the r protocol
curve(Pnormal, col="red", lwd=2, add=TRUE)
</input>
</sage>
</p>

</section>

<section xml:id="BernoulliGeneratingFunctions"><title>Generating Functions for Bernoulli-based Distributions</title>

<p>
<xref ref="DefnMomentGeneratingFunction">Moment Generating Functions</xref> can be derived for each of the distributions in this chapter.
</p>


<theorem xml:id="MGFBernoulli"><title>Moment Generating Function for Bernoulli</title>
<statement>
<p>
<me>M(t) = (1-p) + p e^t</me>
</p>
</statement>
<proof>
<p>
<me>M(t) = f(0) + e^t f(1) = (1-p) + p e^t</me>
</p>
</proof>
</theorem>

<theorem xml:id="MGFGeometric"><title>Moment Generating Function for Geometric</title>
<statement>
<p>
<me>M(t) = \frac{p e^t }{1 - e^t (1-p)} = \frac{p}{e^{-t} - (1-p)}</me>
</p>
</statement>
<proof>
<p>Presuming <m>e^t (1-p) \lt 1</m>,
<md>
<mrow>M(t) &amp; = \sum_{x=1}^{\infty} e^{tx} p (1-p)^{x-1}</mrow>
<mrow> &amp; = \frac{p}{1-p} \sum_{x=1}^{\infty} (e^t (1-p))^x</mrow>
<mrow> &amp; = \frac{p}{1-p} \frac{e^t (1-p)}{1 - e^t (1-p)}</mrow>
<mrow> &amp; = \frac{pe^t }{1 - e^t (1-p)}.</mrow>
</md>
where we used the geometric series to convert the sum.  The second form comes by dividing through by <m>e^t</m>.
</p>
</proof>
</theorem>

<corollary><title>Geometric Properties via Moment Generating Function</title>
<statement>
<p>For the Geometric variable X,
<me> M(0) = 1</me>
<me> M'(0) = \frac{1}{p} = \mu</me>
<me> M''(0) = \frac{1-p}{p^2} + \frac{1}{p^2} = \sigma^2 + \mu^2</me>
</p>
</statement>
<proof>
<p>
<me> M(0) = p \frac{e^0 }{1 - e^0 (1-p)} = \frac{p}{1-(1-p)} = 1.</me>
Using the second form for M(t),
<me> M'(t) = \frac{e^{-t} p}{(e^{-t} - (1-p))^2}</me>
and therefore
<me> M'(0) = \frac{p}{(1-(1-p))^2} = \frac{1}{p}.</me>
Continuing with the second derivative,
<me> M''(t) = -\frac{p e^{-t} }{{\left(p + e^{-t} - 1\right)}^2} + \frac{2 p e^{-2t}}{{\left(p + e^{-t} - 1 \right)}^{3}}</me>
and therefore
<me> M''(0) = -\frac{p}{{\left(p + 1 - 1 \right)}^2} + \frac{2 p}{{\left(p + 1 - 1 \right)}^{3}} = -\frac{1}{p} + \frac{2}{p^2} = \frac{1}{p^2} + \frac{1-p}{p^2} </me>
which is the squared mean plus the variance for the geometric distribution.
</p>
</proof>
</corollary>

<p>For the two uniform distributions and basic Bernoulli, there is really nothing much that can be done as n (or a and b) vary.  However, for all distributions that have the opportunity to "scale" larger and larger, we will first determine the M(t) and then demonstrate the a surprising relationship as their variables to grow appropriately.</p>

<theorem xml:id="MGFBinomial"><title>Moment Generating Function for Binomial</title>
<statement>
<p>
<me>M(t) = \left ( p e^t + (1-p) \right )^n</me>
</p>
</statement>
<proof>
<p>
<md>
<mrow>M(t) &amp; = \sum_{x=0}^n e^{tx} \binom{n}{x} p^x (1-p)^{n-x} </mrow>
<mrow> &amp; = \sum_{x=0}^n \binom{n}{x} (pe^t)^x (1-p)^{n-x} </mrow>
<mrow> &amp; = \left ( p e^t + (1-p) \right )^n</mrow>
</md>
where we used the binomial theorem to simplify the sum.
</p>
</proof>
</theorem>

<p>
Notice that the moment generating function for Bernoulli is simply the Binomial moment generating function with n=1.
</p>

<corollary><title>Binomial Properties via Moment Generating Function</title>
<statement>
<p><me>M(0) = 1</me>
<me>M'(0) = np</me>
<me>M''(0) = np(1-p) + (np)^2</me>
</p>
</statement>
<proof>
<p>
<me>M(0) = \left ( p e^0 + (1-p) \right )^n = 1^n = 1.</me>
Taking the derivative with respect to t,
<me>M'(t) = n \left ( p e^t + (1-p) \right )^{n-1} p e^t</me>
and evaluating at t=0 gives
<me>M'(0) = n \left ( p + (1-p) \right )^{n-1} p = n 1^{n-1} p = np.</me>
Again, taking another derivative with respect to t, 
<me>M''(t) = n(n-1) \left ( p e^t + (1-p) \right )^{n-2} p^2 e^{2t} + n \left ( p e^t + (1-p) \right )^{n-1} p e^t </me>
and evaluating at t=0 gives
<md>
<mrow>M''(0) &amp; = n(n-1) ( p + (1-p))^{n-2} p^2 + n ( p + (1-p)  )^{n-1} p </mrow>
<mrow> &amp; = n(n-1)p^2 + np = (np)^2 + np - np^2 = np(1-p) + (np)^2.</mrow>
</md>
</p>
</proof>
</corollary>


<!--
<corollary><title>Limiting function for Binomial M(t)</title>
<statement>
<p>
</p>
</statement>
<proof>
<p>As we let <m>\mu \rightarrow \infty</m> and noting that <m>\mu = np </m>, we can rewrite M(t) in the form 
<me>M(t) = \left ( \frac{\mu}{n} e^t + (1- \frac{\mu}{n}) \right )^n</me> and note that <m>\mu</m> grows proportionally to n. Thus,
<md>
<mrow>\lim_{n  \rightarrow \infty} M(t) &amp; = \lim_{n  \rightarrow \infty} \left ( \frac{\mu}{n} e^t + (1- \frac{\mu}{n}) \right )^n</mrow> 
<mrow> &amp; = e^{\lim_{n  \rightarrow \infty} \frac{\ln \left ( \frac{\mu}{n} e^t + (1- \frac{\mu}{n}) \right )}{1/n}}</mrow> 
<mrow> &amp; = e^{\lim_{n  \rightarrow \infty} \frac{\frac{1}{\left ( \frac{\mu}{n} e^t + (1- \frac{\mu}{n}) \right )} \left ( \frac{-\mu}{n^2} e^t + \frac{-\mu}{n^2} \right )}{-1/n^2}}</mrow>
<mrow> &amp; =  e^{\lim_{n  \rightarrow \infty} \frac{1}{\left ( \frac{\mu}{n} e^t + (1- \frac{\mu}{n}) \right )} \left(  \mu e^t + \mu \right )}</mrow>
<mrow> &amp; = e^{\frac{1}{\left ( 0 + (1- 0) \right )} \left(  \mu e^t + \mu \right )}</mrow>
<mrow> &amp; = e^{\mu (e^t-1)}.</mrow>
</md>
(WAIT...mu depends upon n as well...NEED to review this.  SHOULD BE IN THE FORM <m>e^{\mu t - \frac{1}{2} \sigma^2 t^2.}</m>
where we have used L'Hopital's rule to evaluate the indeterminate form. We will see this final form repeating itself as we continue through the text.
</p>
</proof>
</corollary>
-->

<theorem xml:id="MGFNegativeBinomial"><title>Moment Generating Function for Negative Binomial</title>
<statement>
<p>
<me>M(t)  = \frac{p^r}{(1 - e^t(1-p))^r}</me>
</p>
</statement>
<proof>
<p>Using the <xref ref="NegativeBinomialSumsToOne">a previous theorem</xref> justifying the Negative Binomial probability function  with <m>a = p e^t</m> and <m>b = 1-p</m> and by changing variables to <m>u = x-r</m> gives
<md>
<mrow>M(t) &amp; = \sum_{u=0}^{\infty} e^{tu} \binom{u+r - 1}{r-1}(1-p)^{u}p^r</mrow>
<mrow> &amp; =  p^r \sum_{u=0}^{\infty} \binom{u + r - 1}{r-1}(e^t(1-p))^u </mrow>
<mrow> &amp; = \frac{ p^{r}}{(1 - e^t(1-p))^r} \sum_{u=0}^{\infty} \binom{u + r - 1}{r-1}(1 - e^t(1-p))^r (e^t(1-p))^u </mrow>
<mrow> &amp; = \frac{p^{r}}{(1 - e^t(1-p))^r}  </mrow>
</md>
noting that the last summation is the the sum of a negative binomial probability function over its entire range.
</p>
<p>It should be noted one may also rewrite the summation and appeal directly to the <xref ref="NegBinomSeries">Negative Binomial Series</xref> to also prove this result.
</p>
</proof>
</theorem>

<p>
Notice that the moment generating function for Geometric is simply the Negative Binomial moment generating function with r=1.
</p>

<corollary><title>Negative Binomial Properties via Moment Generating Function</title>
<statement>
<p><me>M(0) = 1</me>
<me>M'(0) = \frac{r}{p}</me>
<me>M''(0) = \left ( \frac{r(1-p)}{p^2} \right )^2 + \left ( \frac{r}{p} \right )^2</me>
</p>
</statement>
<proof>
<p>
<me>M(0) = \frac{(p)^r}{(1 - (1-p))^r} = \frac{p^r}{p^r} = 1.</me>
Taking the derivative with respect to t,
<me>M'(t) = -\frac{{\left(p e^{t} - e^{t} + 1\right)}^{r - 1} {\left(p - 1\right)} p^{r} r e^{t}}{{\left(p e^{t} - e^{t} + 1\right)}^{2 r}}</me>
and evaluating at t=0 gives
<me>M'(0) = -\frac{{p}^{r - 1} {p - 1} p^{r} r }{{p}^{2 r}} = \frac{r(1-p)}{p}.</me> OOPS...don't need the (1-p) on top.
Again, taking another derivative with respect to t, 
<me>M''(t) =  -\frac{{\left(p e^{t} - e^{t}\right)} {\left(p e^{t} - e^{t} + 1\right)}^{r - 2} {\left(p - 1\right)} p^{r} {\left(r - 1\right)} r e^{t}}{{\left(p e^{t} - e^{t} + 1\right)}^{2r}}
\\ + \frac{2 {\left(p e^{t} - e^{t}\right)} {\left(p e^{t} - e^{t} + 1\right)}^{2r - 2} {\left(p - 1\right)} p^{r} r^{2} e^{t}}{{\left(p e^{t} - e^{t} + 1\right)}^{3r}}
\\ - \frac{{\left(p e^{t} - e^{t} + 1\right)}^{r - 1} {\left(p - 1\right)} p^{r} r e^{t}}{{\left(p e^{t} - e^{t} + 1\right)}^{2r}}
</me>
and evaluating at t=0 gives
<md>
<mrow>M''(0) &amp; = \frac{{\left(p^{2 \, r - 1} r - p^{2 \, r - 2} r - p^{2 \, r - 2}\right)} {\left(p - 1\right)} r}{p^{2 \, r}} </mrow>
<mrow> &amp; = \left ( \frac{r(1-p)}{p^2} \right )^2 + \left ( \frac{r}{p} \right )^2.</mrow>
</md>
</p>
</proof>
</corollary>

<p>Just in case you are wondering about the derivations and especially taking the derivatives above, Sage will do the heavy lifting for you.

<sage>
<input>
var('t,x,n,p,mu,r')
assume(p &lt; 1)
assume(p &gt; 0)
f = binomial(x+r-1,r-1)*p^r*(1-p)^x
M = sum(e^(t*x)*f,x,0,infinity).factor()
show(M)
show(M(t=0))
Mt = derivative(M,t).factor()
show(Mt(t=0).simplify())
Mtt = derivative(Mt,t)
show(Mtt)
show(Mtt(t=0).simplify())
show((Mtt(t=0)-Mt(t=0)^2).factor())
</input>
</sage>

</p>

</section>

<section xml:id="BernoulliBasedSummary"><title>Summary</title>
<p>
Here is a summary of the major results dealing with Bernoulli-based variables:
</p>
<p>
<xref ref="BinomialProbabilityFunction">Binomial Distribution</xref>
</p>
<p>
<xref ref="BinomialStatistics">Binomial Distribution Statistics</xref>
</p>
<p>
<xref ref="GeometricProbabilityFunction">Geometric Distribution</xref>
</p>
<p>
<xref ref="GeometricStatistics">Geometric Distribution Statistics</xref>
</p>
<p>
<xref ref="NegativeBinomialProbabilityFunction">Negative Binomial Distribution</xref>
</p>
<p>
<xref ref="NegativeBinomialStatistics">Negative Binomial Distribution Statistics</xref>
</p>
</section>

<section xml:id="BernoulliBasedExercises"><title>Exercises</title>

<p>
<exercise><title> - Gallup Consumer Confidence Polling</title>
<p>
A January 2008 Gallup poll on consumer confidence asked the question "How would you rate economic conditions in this country today" and 22% responded "Excellent" or "Good", 45% responded "Only Fair", and 33% responded "Poor". If you pick a representative sample of 25 people and ask them the same question, if X measures the number of responses that are "Excellent" or "Good", determine
<ul>
	<li>P(X is at most 5).</li>
	<li>P(X is at least 5).</li>
	<li>the expected number of Excellent or Good responses.</li>
</ul>
</p>
<solution>
<p>
This is a Binomial distribution with n=25 and p = 0.22.

<ul>	
	<li>P(X is at most 5) = F(5) = f(0)+f(1)+f(2)+f(3)+f(4)+f(5) = 0.51843</li>
	<li>P(X is at least 5) = 1 - F(4) = 0.67183</li>
	<li><m>\mu = np = 25 \cdot .22 = 5.5</m></li>
</ul>
</p>
</solution>
</exercise>
</p>

<p>
<exercise><title> - Rolling Dice</title>
<p>
You keep on rolling a pair of dice and let X be the number of rolls needed until you get a sum of 7 or 11 for the second time. Determine:
<ol>
	<li>P(7 or 11 on one roll)</li>
	<li>The expected number of rolls until you get the second 7 or 11 sum.</li>
	<li>P(X = 12)</li>
	<li><m>P(X \ge 4)</m></li>
</ol>
</p>
<solution>
<p> For this problem, use the Negative Binomial Distribution when looking for the number of trials till the 2nd success. p is determined in the first answer.
<ol>
	<li>P(7 or 11 on one roll) = 8/36 = 2/9, using equally likely outcomes.</li>
	<li><m>\mu = \frac{r}{p} = \frac{2}{2/9} = 9</m></li>
	<li><m>P( X = 12) = \binom{11}{1} (7/9)^10 \cdot (2/9)^2 = 0.044</m></li>
	<li><md>
		<mrow>P(X \ge 4) &amp; = 1- P(x \le 3) </mrow>
		<mrow> &amp; = 1 - [ f(2) + f(3) ]</mrow>
		<mrow> &amp; = 1 - \left[ \binom{1}{1} (2/9)^2 + \binom{2}{1} (7/9)^1 \cdot (2/9)^2 \right ]</mrow>
		<mrow> &amp; = 1 - 0.1262 = 0.8738.</mrow>
	</md></li>
</ol>
</p>
</solution>
</exercise>
</p>

<p>
<exercise><title> - Collecting Kids Meal Prizes</title>
<p>
You love to eat at Chick-Fil-A with your kids and want to collect all of the five new book titles that come randomly included with each kids meal.  If the promotion with these books starts today, determine:
<ol>
	<li>The probability that you get a book you don't have when purchasing the first children's meal.</li>
	<li>The probability that you it takes more than four purchases in order to get a second title.</li>
	<li>The expected total number of children's meals you would expect to purchase in order to get all five titles.</li>
</ol>
</p>
<solution>
<p>
<ol>
	<li>One. The first meal will certainly have a book that you have not received yet.</li>
	<li>This is a geometric distribution with p=4/5.  P(X &gt; 4) = 1 - F(4) = <m>(1 - 4/5)^4 = \frac{1}{625}</m> which is very small. Note, in this case you would have needed to receive the same title randomly for all of the first four kids meal purchases. If this were to ever happen, please let the people at the counter know and it will be their pleasure to swap out for a new title. </li>
	<li>Use the geometric distribution five times with changing values for p. For the first book p = 1 means you are certain to get a new title. For the second book title the probability of success is p=4/5; for the third book title the probability of success is p=3/5; for the fourth the probability is p=2/5; and for the last the probability is p = 1/5. Using the mean as 1/p in each case and accumulating these gives the total expected number of meals to purchase as 
	<md>
		<mrow> &amp; 1 + \frac{1}{\frac{4}{5}} + \frac{1}{\frac{3}{5}} + \frac{1}{\frac{2}{5}} + \frac{1}{\frac{1}{5}} </mrow>
		<mrow> &amp; = 1 + \frac{5}{4} + \frac{5}{3} + \frac{5}{2} + \frac{5}{1} </mrow>
		<mrow> &amp; = \frac{12 + 15 + 25 + 30 + 60}{12} </mrow>
		<mrow> &amp; = \frac{142}{12} = 11.833</mrow>
	</md> 
	and so you would need 12 kids meals.  If this were to happen, please be certain to donate the "extra" books to an organization that works with kids or directly to some kids that you might know.</li>
</ol>
</p>
</solution>
</exercise>
</p>

<p>
<exercise><title> - Rolling Dice</title>
<p>Suppose you roll a standard pair of 6-sided dice 20 times and let X measure the number of outcomes which result in a sum of 7 or 11. Determine:
<ol>
	<li>the expected number of rolls which have a sum of 7 or 11</li>
	<li>P(X=5)</li>
	<li>P( X &gt; 5)</li>
	<li>P( X &lt; 5)</li>
</ol>
</p>
</exercise>
</p>

<p>
<exercise><title> - Rolling Dice yet again</title>
<p>Suppose you roll a standard pair of 6-sided dice X times until you get a sum of 7 or 11 a third time. Determine:
<ol>
	<li>the expected number of rolls needed on average.</li>
	<li>P(X=5)</li>
	<li>P( X &gt; 5)</li>
	<li>P( X &lt; 5)</li>
</ol>
</p>
</exercise>
</p>

<p>
<exercise>
<title> - 2 standard deviations from the mean</title>
<p>Given p = 0.3 determine the following:
<ol>
	<li>For Binomial with n = 50, <m>P(\mu - 2\sigma \le X \le \mu + 2\sigma)</m></li>
	<li>For Negative Binomial with r = 2, <m>P(\mu - 2\sigma \le X \le \mu + 2\sigma)</m></li>
</ol>
</p>
<solution>
<p>
For Binomial with p = 0.3 and n = 50, <m>\mu = n \cdot p = 15</m> and <m>\sigma^2 = n \cdot p \cdot (1-p) = 10.5</m>.  So, <m>\sigma = \sqrt{10.5}</m>.  Therefore,
<md>
	<mrow>P(\mu - 2\sigma \le X \le \mu + 2\sigma) &amp; = P(15 - 2 \sqrt{10.5} \le X \le 15 + 2 \sqrt{10.5})</mrow>
	<mrow> &amp; P( X \in \{9, 10, 11, ... , 19, 20, 21 \} )</mrow>
</md>
Then 
<me>F(21) - F(8) \approx 0.97491 - 0.01825 = 0.95666 </me>
</p>
<p>
For Negative Binomial with p = 0.3 and r = 2, <m>\mu = \frac{2}{0.3} = \frac{20}{3}</m> and <m>\sigma^2 = 2 \frac{0.7}{0.3^2} = \frac{140}{9}</m> and so <m>\sigma \approx 3.9</m>.  Therefore,
<md>
	<mrow>P(\mu - 2\sigma \le X \le \mu + 2\sigma) &amp; = P(6.7 - 7.8 \le X \le 6.7 + 7.8)</mrow>
	<mrow> &amp; P( X \in \{2, 3, ... , 14, 15, 16 \} )</mrow>
</md>
Then,
	<me>F(16) \approx 0.973888</me>
</p>
</solution>
</exercise>
</p>

<p>
<exercise><title>Chapter Experiment</title>
<p>
Take a die and roll it 4 times, keeping track on paper each time you get a 6 (say).  Repeat this 100 times. (Actually, you can use a standard 6-sided die or find a more exotic one with more sides.)  You should have gotten anywhere from 0 of the rolls to be a 6 up to all of the rolls to be a 6.  Collect the relative frequencies of each of the possible outcomes in R= \{0,1,2, ..., 100\} and plot.  Compare several of the experimental relative frequencies with the theoretical value you would expect using the proper distribution from this chapter. Comment on how well you did. 
</p>
</exercise>
</p>

</section>

</chapter>
