<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="IntervalEstimation" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Estimation</title>

<section><title>Introduction</title>
<p>
You should have noticed by now that repeatedly sampling from a given distribution will yield a variety of sample statistics such as <m>\overline{x}</m> as an estimate perhaps for the population mean <m>\mu</m> or <m>\frac{Y}{n}</m> as an estimate for the population likelihood of success p. In this section, you will see how these sample "point estimators" are actually the best possible choices.
</p>
<p>
In creating these point estimates repeatedly, you have noticed that the results will change somewhat over time. Indeed, flip a coin 20 times and you might expect 10 heads. However, in practice it is likely to 9 or 12 out of 20 and possible to get any of the other possible outcomes. This natural variation makes the point estimates noted above to almost certainly be in error. However one would expect that they should be close and the Central Limit Theorem does indicate that the distribution of sample means should be approximately normally distributed. Thus, instead of relying just on the value of the point estimate, you might want to investigate a way to determine a reasonable interval centered on the sample statistic in which you have some confidence the actual population statistic should belong. This leads to a discussion of interval estimates known as confidence intervals (using calculational tools) and statistical tolerance intervals (using order statistics).
</p>
<p>
In this chapter we first discuss how to determine appropriate methods for estimating the needed population statistics (point estimates) and then quantify how good they are (confidence intervals).
</p>
</section>

<section>	
	<title>Interval Estimates - Chebyshev</title>

	<p>An interval centered on the mean in which at least a certain proportion
	of the actual data must lie.
	</p>

	<theorem>
	<title>Chebyshev's Theorem</title>
	<statement>
	Given a random variable X with given mean <m>\mu</m> and standard deviation <m>\sigma</m>, for <m> a \in \mathbb{R}^+</m> , 
	
	<me>P( \big | X - \mu \big | \lt a ) \gt 1 - \frac{\sigma^2}{a^2}</me>
	</statement>
	<proof>
	<p>
	Notice that the variance of a continuous variable X is given by
	<md>
		<mrow>\sigma^2 &amp; = \int_{-\infty}^{\infty} (x - \mu)^2 f(x) dx</mrow>
		<mrow> &amp; \ge \int_{-\infty}^{\mu-a} (x - \mu)^2 f(x) dx + \int_{\mu + a}^{\infty} (x - \mu)^2 f(x) dx</mrow>
		<mrow> &amp; \ge \int_{-\infty}^{\mu-a} a^2 f(x) dx + \int_{\mu + a}^{\infty} a^2 f(x) dx</mrow>
		<mrow> &amp; = a^2 \big ( \int_{-\infty}^{\mu-a} f(x) dx + \int_{\mu + a}^{\infty} f(x) dx \big )</mrow>
		<mrow> &amp; = a^2 P( X \le \mu - a \text{or} X \ge \mu + a )</mrow>
		<mrow> &amp; = a^2 P( \big | \mu - a \big | \ge a)</mrow>
	</md>
	Dividing by <m>a^2</m> and taking the complement gives the result.
	</p>
	</proof>
	</theorem>
	
	<corollary><title>Alternate Form for Chebyshev's Theorem</title>
	<statement>
	<p>For positive k,
	<me>P( \big | X - \mu \big | \lt k \sigma ) \gt 1 - \frac{1}{k^2}</me>
	</p>
	</statement>
	</corollary>

	<corollary><title>Special Cases for Chebyshev's Theorem</title>
	<statement>For any distribution, it is not possible for f(x)=0 within one standard deviation of the mean. Aslo, at least 75% of the data for any distribution must lie within two standard deviations of the mean and at least 88% must lie within three.
	</statement>
	<proof>
	<p>
	Apply the Chebyshev Theorem with <m>a = \sigma</m> to get
	<me>P(\mu - \sigma \lt X \lt \mu + \sigma) \gt 1 - \frac{\sigma^2}{\sigma^2} = 0</me>
	</p>
	<p>
	Apply the Chebyshev Theorem with <m>a = 2 \sigma</m> to get <m>1 - \frac{1}{2^2} = 0.75</m> and with <m>k = 3 \sigma</m> to get <m>1 - \frac{1}{3^2} = \frac{8}{9} > 0.8888</m>.
	</p>
	</proof>
	</corollary>
	
	<example><title> - Comparing known distribution to Chebyshev</title>
	<p>
	</p>
	</example>
</section>

<section><title>Point Estimates</title>

<p>
For Binomial, Geometric, what is p? For exponential, what is the mean?  For normal, what are the mean and standard deviation? Each of these parameters are necessary before you can compute any probability values from their respective formulas. Since they might not be given in a particular instance, they will need to be estimated in some manner. 
</p>

<p>This estimate will have to be determined likely by utilizing sampling in some form. Since such an estimate will come from partial information (i.e. a sample) then it is very likely going to only be an approximation to the exact (but unknown) value. In general, an estimator is a numerical value which is used in the place of an unknown population statistic. To determine precisely what is a "best" estimator requires a multivariate approach and is beyond the scope of this text. Indeed, to justify why each of the following are good estimators look up the topic "Maximum Likelihood Estimators".
</p>

<p>From your previous experience with the Poisson, Exponential, and Gamma distributions, you should also remember that each required a known value for <m>\mu</m> before proceeding with calculations.  It is sensible to consider estimating the unknown population mean <m>\mu</m> using the sample mean
<me>\mu \approx \overline{x} = \frac{\sum x_k}{n}</me>
where the values <m>x_k</m> are the n individual sample values.
</p>

<p>
For any continous variable and indeed for <m>\overline{X}</m>), <m>P(\overline{X} = \mu) = 0</m>. In general, you should expect a sample statistic to be close but not precisely equal to the population statistic. Indeed, if you were so lucky as to have the sample statistic to land on the population statistic, doing one more trial would mess things up anyway and the sample statistic would certainly change some.
</p>

<p>In a similar manner with the Binomial, Geometric, and Negative Binomial distributions, you will remember that each required a known value for p before  proceeding with any calculations. From our experiments we saw that relative frequency appeared to stabilize around what you might expect for the true proportion of success and therefore estimating the unknown proportion of success p using relative frequency
<me>p \approx \tilde{p} = \frac{y}{n}</me>
where y is the number of successes in a collection of n bernoulli trials. Again, notice that the relative frequency <m>\tilde{p}</m> is technically an average as well so the probability that a given relative frequency will like exactly on the actual value of p is again zero.
</p>

<p>Finally, the Normal distribution requires a numerical value for <m>\sigma</m>, the population's standard deviation. It can be shown that the maximum likelihood estimator for <m>\sigma^2</m> is the variance v found in chapter one. However, you may remember that at that time we always adjusted this value somewhat using the formula <m>s^2 = \frac{n}{n-1} v</m> which increased the variance slightly. To uncover why you would not use the maximum likelihood estimator v requires you to look up the idea of "bias". As it turns out, v is maximum likelihood but exhibits mathematical bias whereas <m>s^2</m> is slightly suboptimal with respect to likelihood but exhibits no bias. Therefore, for estimating the unknown population variance <m>\sigma^2</m> you can use sample variance
<me>\sigma^2 \approx s^2</me> 
and similarly sample standard deviation
<me>\sigma \approx s</me>
to approximate the theoretical standard deviation. 
</p>

</section>



<section><title>Interval Estimates - Confidence Interval for p</title>
<p>
Sometimes selecting a value for p for a Binomial, Geometric, or Negative Binomial distribution problem can be done by using a theoretical value. Indeed, when flipping a coin it is reasonable to assume p = 1/2 is the probability of getting a head on one flip. Similarly, it is reasonable to assume p = 1/6 when you are looking for a particular side of a 6-sided die. However, many times you will want to deal with a problem in which it is not possible to determine exactly the precise value for the likelihood of success such as your true probability of making a free throw in basketball or knowing the true percentage of the electorate that will vote for your favorite candidate. 
</p>
<p>
In these later situations, we found in the previous section that relative frequency <m>\frac{Y}{n}</m> is generally a good way to estimate p. In this section, you will investigate how to measure the closeness--and thereby assure some confidence in that estimate--regarding how well the point estimate approximates the actual value of p.
</p>




<definition><title>Confidence Intervals for p</title>
<statement>
<p>Given a point estimate <m>\tilde{p}</m> for p, a confidence interval for p is a range of values which contains the actual value of p with high probability. In notation, a two-sided confidence interval for p is of the form
<me>P(\tilde{p} - E_1 \lt p \lt \tilde{p} + E_2) = 1 - \alpha</me>
where <m>\alpha</m> is near 0 and <m>E_k \gt 0</m>.  One-sided confidence intervals for p can be similarly described 
<me>P( p \lt \tilde{p} + E_2) = 1 - \alpha</me>
or
<me>P(\tilde{p} - E_1 \lt p) = 1 - \alpha.</me>
</p>
</statement>
</definition>

<p>
Generally, symmmetry is presumed when using a two-sided confidence interval so that <m>E_1 = E_2 = E</m> and therefore the interval looks like
<me>P(\tilde{p} - E \lt p \lt \tilde{p} + E) = 1 - \alpha.</me>
In this case, E is known as the margin of error.
</p>
<p>
To determine E carefully, note that from the central limit theorem
<me>\frac{Y-np}{\sqrt{np(1-p}} = \frac{\tilde{p} - p}{\sqrt{p(1-p)/n}}</me>
is approximately standard normal for large n.  Presuming that <m>\tilde{p} \approx p</m> and replacing the unknown p terms on the bottom with <m>\tilde{p}</m> gives 
<me>z = \frac{\tilde{p} - p}{\sqrt{\tilde{p}(1-\tilde{p})/n}}</me>
where z is a standard normal distribution variable. So, using the central limit theorem and the standard normal distribution, you can find the value <m> z_{ \alpha/2}</m> where
<me>P( -z_{ \alpha/2} \lt z \lt z_{ \alpha/2}) = 1 - \alpha</me>
<me>P( -z_{ \alpha/2} \lt \frac{\tilde{p} - p}{\sqrt{\tilde{p}(1-\tilde{p})/n}} \lt z_{ \alpha/2}) = 1 - \alpha</me>
or by rearranging the inside inequality
<me>P( \tilde{p} - z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n} \lt  p \lt \tilde{p} + z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n}) = 1 - \alpha.</me>
Setting <m>E = z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n}</m> gives a way to determine a confidence interval centered on <m>\tilde{p} = \frac{Y}{n}</m> for p with "confidence level" <m>1-\alpha</m>.  
</p>
<p>
To complete the interval, one needs a specific value for <m>z_{ \alpha/2}</m>.
Generally, one chooses confidence levels on the order of 90%, 95%, or 99% with 95% being the usual choice.
</p>
<p>
For 90% confidence level, you need to find a z-value so that
<me>P( -z_{ \alpha/2} \lt z \lt z_{ \alpha/2}) = 0.9 = 1 - 0.1 .</me>
Using the symmetry of the normal distribution, this can be rewritten
<me>F(z_{ \frac{0.1}{2}}) = P( z \lt z_{ \frac{0.1}{2}}) = 0.95 = 1 - \frac{0.1}{2} .</me>
Using the inverse of the standard normal distribution (on the TI calculator this is InvNorm(0.95)) gives <m>z_{ 0.05} \approx 1.645</m>.
</p>
<p>
Similarly, for a 95 % confidence level, find where
<me>F(z_{ \frac{0.05}{2}}) = P( z \lt z_{ \frac{0.05}{2}}) = 0.975 = 1 - \frac{0.05}{2} .</me>
The calculators InvNorm(0.975) gives <m>z_{ 0.025} \approx 1.960</m>.
</p>
<p>
For a 99 % confidence level, find where
<me>F(z_{ \frac{0.01}{2}}) = P( z \lt z_{ \frac{0.01}{2}}) = 0.995 = 1 - \frac{0.01}{2} .</me>
The calculators InvNorm(0.995) gives <m>z_{ 0.005} \approx 2.576</m>.
</p>
<p>
Notice that when computing the confidence intervals above that we choose to just replace some of the p terms with <m>\tilde{p}</m> so that only one p term was left and could be isolated in the middle. There are other ways to deal with this. The easiest is to take the worst case scenario for the p terms in the denominator above. Indeed, the confidence interval is made wider (and therefore more likely to contain the actual p) if the square root term is as large as possible, using basic calculus it is easy to see that p(1-p) is maximized when p = 1/2. Therefore, a second alternative is to create your confidence interval using
<me>z = \frac{\tilde{p} - p}{\frac{1}{2\sqrt{	n}}}</me>
and therefore <m>E = \frac{z_{ \alpha/2}}{2\sqrt{n}}</m>.
This method should be used only when trying to create the roughest and "safest" interval.
</p>
<p>
The Wilson Score is another way to determine a confidence interval the in some ways is betters than either of the two methods described above. This confidence interval is more complicated than just taking <m>\tilde{p}</m> and adding and subtracting E.  

<theorem><title>Wilson Score Confidence Interval for p</title>
<statement>
<p>
<me>\frac{\tilde{p} + \frac{z_{\alpha/2}^2}{2n} - z_{\alpha/2} \sqrt{\frac{\tilde{p}(1-\tilde{p}) + \frac{z_{\alpha/2}^2}{4n}}{n}}}{1 + \frac{z_{\alpha/2}^2}{n}} \lt p \lt \frac{\tilde{p} + \frac{z_{\alpha/2}^2}{2n} + z_{\alpha/2} \sqrt{\frac{\tilde{p}(1-\tilde{p}) + \frac{z_{\alpha/2}^2}{4n}}{n}}}{1 + \frac{z_{\alpha/2}^2}{n}}
</me>
</p>
</statement>
</theorem>

<example><title>Comparison of the three Confidence Interval methods for p</title>
<p>
Presume that from a sample of size n = 400 you get Y = 144 successes.  Determine 95% two-sided confidence intervals for the actual p using all three of the methods above. Note that for each you will utilize <m>z_{\alpha/2} = z_{0.025} = 1.960</m> and <m>\tilde{p} = \frac{144}{400} = 0.36</m>.
</p>
<p>Normal Interval:
<me>P( 0.36 - 1.960 \sqrt{0.36 \cdot 0.64) / 400} \lt  p \lt 0.36 + 1.960 \sqrt{0.36 \cdot 0.64) / 400}) = 1 - \alpha.</me>
or
<me>P( 0.36 - 1.960 \cdot 0.6 \cdot 0.8) / 20 \lt  p \lt 0.36 + 1.960 \cdot 0.6 \cdot 0.8) / 20) = 0.95 </me>
or
<me>P( 0.36 - 0.04704 \lt  p \lt 0.36 + 0.04704) = 0.95 .</me>
or
<me>P( 0.31296 \lt  p \lt 0.40704) = 0.95 .</me>
So, there is a 95% chance that the actual value for p lies inside the interval
<m>(0.31296 , 0.40704).</m>
</p>
<p>Maximal Interval:
<me>P( 0.36 - 1.960 \frac{1}{2\sqrt{400}} \lt  p \lt 0.36 + 1.960 \frac{1}{2\sqrt{400}} ) = 1 - \alpha.</me>
or
<me>P( 0.36 - 1.960 \frac{1}{40} \lt  p \lt 0.36 + 1.960 \frac{1}{40} ) = 1 - \alpha.</me>
or
<me>P( 0.311 \lt  p \lt 0.409 ) = 1 - \alpha.</me>
Notice the interval is only slightly wider than when using <m>\tilde{p}</m> to estimate p in the first case.
</p>
<p>Wilson Score Interval:  Let's do this on in parts...
<me>z_{\alpha/2} \sqrt{\frac{\tilde{p}(1-\tilde{p}) + \frac{z_{\alpha/2}^2}{4n}}{n}} = 1.96 \sqrt{ \frac{0.36 \cdot 0.64 + \frac{1.96^2}{1600}}{400}} \approx 0.04728
</me>
Therefore, 
<me>\frac{0.36 + \frac{1.96^2}{800} - 0.04728}{1 + \frac{1.96^2}{400}} \lt p \lt \frac{0.36 + \frac{1.96^2}{800} + 0.04728}{1 + \frac{1.96^2}{400}}
</me>
or
<me>0.3145 \lt p \lt 0.4082</me>
which is slightly different than the first and slightly smaller than the second.
</p>
</example>

</p>


<theorem><title>Determining Sample Size for proportions</title>
<statement>Given a margin of error E and preliminary relative frequency estimate <m>\tilde{p_0}</m> the sample size needed to create the corresponding confidence interval is given by 
<me>n \gt \left ( \frac{z_{\alpha /2}}{E} \right )^2 \tilde{p_0}(1-\tilde{p_0}).</me>
</statement>
<proof>
Solve for n in the confidence interval above.
</proof>
</theorem>

<example><title>Determining Sample Size for one proportion</title>
<p>
Given a 99% confidence level, margin of error E=0.03, and preliminary estimate <m>\tilde{p_0} = 0.35</m>, notice that <m>z_{\alpha / 2} = 2.58</m> gives
<me>n \gt \left ( \frac{2.58}{0.03} \right )^2 0.35 \cdot 0.65 \approx 1682.59</me>
or a sample size of at least 1683.
</p>
</example>


</section>

<section><title>Interval Estimates - Confidence Interval for <m>\mu</m></title>
<p>
As with the confidence intervals above for proportions, the Central Limit Theorem also allows you to create an interval centered on a sample mean for estimating the population mean <m>\mu</m>.
</p>


<definition><title>Confidence Interval for One Mean</title>
<statement>
<p>
Given a sample mean <m>\overline{x}</m>, a two-sided confidence interval for the mean with confidence level <m>1-\alpha</m> is an interval 
<me>\overline{x} - E_1 \lt \mu \lt \overline{x} + E_2</me>
such that
<me>P(\overline{x} - E_1 \lt \mu \lt \overline{x} + E_1) = 1-\alpha.</me>
Generally, the interval is symmetrical of the form <m>\overline{x} \pm E</m> with E again known as the margin of error.  One-sided confidence intervals can be determined in the same manner as in the previous section.
</p>
</statement>
</definition>

<p>
Once again, utilize the Central Limit Theorem.  Notice that the symmetrical confidence interval 
<me>P(\overline{x} - E \lt \mu \lt \overline{x} + E) = 1-\alpha.</me>
is equivalent to
<me>P \left ( \frac{-E}{\sigma / \sqrt{n}} \lt \frac{\overline{x} - \mu}{\sigma / \sqrt{n}} \lt \frac{E}{\sigma / \sqrt{n}} \right ) = 1 - \alpha</me>
in which the middle term can be approximated using a standard normal variable and therefore this statement is approximately
<me>P \left ( \frac{-E}{\sigma / \sqrt{n}} \lt Z \lt \frac{E}{\sigma / \sqrt{n}} \right ) = 1 - \alpha.</me>
Using the symmetry of the standard normal distribution about Z=0 gives
<me>\Phi \left ( \frac{E}{\sigma / \sqrt{n}} \right ) = P \left ( Z \lt \frac{E}{\sigma / \sqrt{n}} \right ) = 1 - \frac{\alpha}{2}</me>
and so to determine E requires the inverse of the standard normal distribution function. In this case, many people write <m>E = z_{\alpha / 2}.</m>  Fortunately this value is easily computed using graphing calculators or other automatic methods although your ancient teacher might have been required to use tables. On a TI calculator, use
<me>z_{\alpha /2} = \text{InvNorm}( 1 - \frac{\alpha}{2} )</me>
which gives a confidence interval for the mean
<me>\overline{x} - z_{\alpha / 2} \frac{\sigma}{\sqrt{n}} \lt \mu \lt \overline{x} + z_{\alpha / 2} \frac{\sigma}{\sqrt{n}}</me>
with confidence level <m>1-\alpha</m> and margin of error 
<me>E = z_{\alpha /2} \frac{\sigma}{\sqrt{n}}.</me>
</p>

<p>
It should be noted that the use of the Central Limit Theorem makes the use of  InvNorm an approximation. It can be shown that so long as n is larger than 30 then generally this approximation is reasonable.
</p>


<p>
Additionally, this derivation assumes that <m>\mu</m> is not known...indeed the goal is to approximate that mean using <m>\overline{x}</m>...but that <m>\sigma</m> is known. This is often not the case. It can however be shown that if n is larger than 30, replacing <m>\sigma</m> with the sample standard deviation s gives an acceptable confidence interval.
</p>

<theorem><title>Sample Size needed for <m>\mu</m> given Margin of Error</title>
<statement>
<p>
Given confidence level <m>1-\alpha</m> and margin of error E, the sample size needed to determine an appropriate confidence interval satisfies
<me> n \gt \left ( z_{\alpha /2} \frac{\sigma}{E} \right )^2</me>
</p>
</statement>
<proof>
Solve for n in the formula for E above. Notice that n must be an integer so you will need to round up. You will also need an estimate for the sample standard deviation s by using a preliminary sample. 
</proof>
</theorem>

<p>
Notice, in practice you might want to take n to be a little larger than the absolute minimum value prescribed above since you are dealing with approximations (Central Limit Theorem and the use of an estimate for s rather than the actual <m>\sigma</m>.)
</p>


<example><title>Determining Sample Size for one Mean</title>
<p>
Given a 95% confidence level, margin of error E=0.1, and preliminary sample with standard deviation s = 2, <m>z_{\alpha / 2} = 1.96</m> gives
<me>n \gt \left ( 1.96 \cdot \frac{2}{0.1} \right )^2 \approx 1536.64</me>
or a sample size of at least 1537.
</p>
</example>

</section>


<section><title>Interval Estimates - Confidence Interval for <m>\sigma^2</m></title>
<p>
Simple Interval using Chi-Square
</p>
</section>

<section><title>Exercises</title>
<p>

</p>
</section>


</chapter>