<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="IntervalEstimation" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Estimation</title>

<section xml:id="IntervalEstimationIntroduction"><title>How Close is Close?</title>
<p>
You should have noticed by now that repeatedly sampling from a given distribution will yield a variety of sample statistics such as <m>\overline{x}</m> as an estimate perhaps for the population mean <m>\mu</m> or <m>\frac{Y}{n}</m> as an estimate for the population likelihood of success p. In this section, you will see how these sample "point estimators" are actually the best possible choices.
</p>

<p>
In creating these point estimates repeatedly, you have noticed that the results will change somewhat over time. Indeed, flip a coin 20 times and you might expect 10 heads. However, in practice it is likely to 9 or 12 out of 20 and possible to get any of the other possible outcomes. This natural variation makes the point estimates noted above to almost certainly be in error. However one would expect that they should be close and the Central Limit Theorem does indicate that the distribution of sample means should be approximately normally distributed. Thus, instead of relying just on the value of the point estimate, you might want to investigate a way to determine a reasonable interval centered on the sample statistic in which you have some confidence the actual population statistic should belong. This leads to a discussion of interval estimates known as confidence intervals (using calculational tools) and statistical tolerance intervals (using order statistics).
</p>

<p>
In this chapter we first discuss how to determine appropriate methods for estimating the needed population statistics (point estimates) and then quantify how good they are (confidence intervals).
</p>
</section>

<section xml:id="IntervalEstimationChebyshev">	
	<title>Interval Estimates - Chebyshev</title>

	<p>An interval centered on the mean in which at least a certain proportion
	of the actual data must lie.
	</p>

<theorem><title>Chebyshev's Theorem</title>
<statement><p>
	Given a random variable X with given mean <m>\mu</m> and standard deviation <m>\sigma</m>, for <m> a \in \mathbb{R}^+</m> , 
	
	<me>P( \big | X - \mu \big | \lt a ) \gt 1 - \frac{\sigma^2}{a^2}</me>
	</p>
</statement>
<proof>
	<p>
	Notice that the variance of a continuous variable X is given by
	<md>
		<mrow>\sigma^2 &amp; = \int_{-\infty}^{\infty} (x - \mu)^2 f(x) dx</mrow>
		<mrow> &amp; \ge \int_{-\infty}^{\mu-a} (x - \mu)^2 f(x) dx + \int_{\mu + a}^{\infty} (x - \mu)^2 f(x) dx</mrow>
		<mrow> &amp; \ge \int_{-\infty}^{\mu-a} a^2 f(x) dx + \int_{\mu + a}^{\infty} a^2 f(x) dx</mrow>
		<mrow> &amp; = a^2 \big ( \int_{-\infty}^{\mu-a} f(x) dx + \int_{\mu + a}^{\infty} f(x) dx \big )</mrow>
		<mrow> &amp; = a^2 P( X \le \mu - a \text{  or  } X \ge \mu + a )</mrow>
		<mrow> &amp; = a^2 P( \big | X - \mu \big | \ge a)</mrow>
	</md>
	Dividing by <m>a^2</m> and taking the complement gives the result.
	</p>
</proof>
</theorem>
	
<corollary><title>Alternate Form for Chebyshev's Theorem</title>
<statement>
	<p>For positive k,
	<me>P( \big | X - \mu \big | \lt k \sigma ) \gt 1 - \frac{1}{k^2}</me>
	</p>
</statement>
</corollary>

<corollary><title>Special Cases for Chebyshev's Theorem</title>
<statement><p>
	For any distribution, it is not possible for f(x)=0 within one standard deviation of the mean. Aslo, at least 75% of the data for any distribution must lie within two standard deviations of the mean and at least 88% must lie within three.
	</p>
</statement>
<proof>
	<p>
	Apply the Chebyshev Theorem with <m>a = \sigma</m> to get
	<me>P(\mu - \sigma \lt X \lt \mu + \sigma) \gt 1 - \frac{\sigma^2}{\sigma^2} = 0</me>
	</p>
	<p>
	Apply the Chebyshev Theorem with <m>a = 2 \sigma</m> to get <m>1 - \frac{1}{2^2} = 0.75</m> and with <m>k = 3 \sigma</m> to get <m>1 - \frac{1}{3^2} = \frac{8}{9} > 0.8888</m>.
	</p>
</proof>
</corollary>
	
	

<exercise><title>WebWork - Chebyshev</title>
<webwork source="Library/NAU/setStatistics/cheby2.pg">
</webwork>
</exercise>


<exercise><title>WebWork - More Chebyshev</title>
<webwork source="Library/NAU/setStatistics/cheby1.pg">
</webwork>
</exercise>



<example><title> - Comparing known distribution to Chebyshev</title>
	<p>TBA
	</p>
</example>

</section>

<section xml:id="IntervalEstimationPointEstimates"><title>Point Estimates</title>

<p>
For Binomial, Geometric, what is p? For exponential, what is the mean?  For normal, what are the mean and standard deviation? Each of these parameters are necessary before you can compute any probability values from their respective formulas. Since they might not be given in a particular instance, they will need to be estimated in some manner. 
</p>

<p>This estimate will have to be determined likely by utilizing sampling in some form. Since such an estimate will come from partial information (i.e. a sample) then it is very likely going to only be an approximation to the exact (but unknown) value. In general, an estimator is a numerical value which is used in the place of an unknown population statistic. To determine precisely what is a "best" estimator requires a multivariate approach and is beyond the scope of this text. Indeed, to justify why each of the following are good estimators look up the topic "Maximum Likelihood Estimators".
</p>

<p>From your previous experience with the Poisson, Exponential, and Gamma distributions, you should also remember that each required a known value for <m>\mu</m> before proceeding with calculations.  It is sensible to consider estimating the unknown population mean <m>\mu</m> using the sample mean
<me>\mu \approx \overline{x} = \frac{\sum x_k}{n}</me>
where the values <m>x_k</m> are the n individual sample values.
</p>

<p>
For any continous variable and indeed for <m>\overline{X}</m>), <m>P(\overline{X} = \mu) = 0</m>. In general, you should expect a sample statistic to be close but not precisely equal to the population statistic. Indeed, if you were so lucky as to have the sample statistic to land on the population statistic, doing one more trial would mess things up anyway and the sample statistic would certainly change some.
</p>

<p>In a similar manner with the Binomial, Geometric, and Negative Binomial distributions, you will remember that each required a known value for p before  proceeding with any calculations. From our experiments we saw that relative frequency appeared to stabilize around what you might expect for the true proportion of success and therefore estimating the unknown proportion of success p using relative frequency
<me>p \approx \tilde{p} = \frac{y}{n}</me>
where y is the number of successes in a collection of n bernoulli trials. Again, notice that the relative frequency <m>\tilde{p}</m> is technically an average as well so the probability that a given relative frequency will like exactly on the actual value of p is again zero.
</p>

<p>Finally, the Normal distribution requires a numerical value for <m>\sigma</m>, the population's standard deviation. It can be shown that the maximum likelihood estimator for <m>\sigma^2</m> is the variance v found in chapter one. However, you may remember that at that time we always adjusted this value somewhat using the formula <m>s^2 = \frac{n}{n-1} v</m> which increased the variance slightly. To uncover why you would not use the maximum likelihood estimator v requires you to look up the idea of "bias". As it turns out, v is maximum likelihood but exhibits mathematical bias whereas <m>s^2</m> is slightly suboptimal with respect to likelihood but exhibits no bias. Therefore, for estimating the unknown population variance <m>\sigma^2</m> you can use sample variance
<me>\sigma^2 \approx s^2</me> 
and similarly sample standard deviation
<me>\sigma \approx s</me>
to approximate the theoretical standard deviation. 
</p>

</section>



<section xml:id="ConfidenceIntervalsForP"><title>Interval Estimates - Confidence Interval for p</title>
<p>
Sometimes selecting a value for p for a Binomial, Geometric, or Negative Binomial distribution problem can be done by using a theoretical value. Indeed, when flipping a coin it is reasonable to assume p = 1/2 is the probability of getting a head on one flip. Similarly, it is reasonable to assume p = 1/6 when you are looking for a particular side of a 6-sided die. However, many times you will want to deal with a problem in which it is not possible to determine exactly the precise value for the likelihood of success such as your true probability of making a free throw in basketball or knowing the true percentage of the electorate that will vote for your favorite candidate. 
</p>

<p>
In these later situations, we found in the previous section that relative frequency <m>\frac{Y}{n}</m> is generally a good way to estimate p. In this section, you will investigate how to measure the closeness--and thereby assure some confidence in that estimate--regarding how well the point estimate approximates the actual value of p.
</p>


<definition xml:id="DefnConfidenceIntervalForP"><title>Confidence Intervals for p</title>
<statement>
<p>Given a point estimate <m>\tilde{p}</m> for p, a confidence interval for p is a range of values which contains the actual value of p with high probability. In notation, a two-sided confidence interval for p is of the form
<me>\tilde{p} - E_1 \lt p \lt \tilde{p} + E_2</me>
with
<me>P(\tilde{p} - E_1 \lt p \lt \tilde{p} + E_2) = 1 - \alpha</me>
where <m>\alpha</m> is near 0 and <m>E_k \gt 0</m>.  One-sided confidence intervals for p can be similarly described 
<me>P( p \lt \tilde{p} + E_2) = 1 - \alpha</me>
or
<me>P(\tilde{p} - E_1 \lt p) = 1 - \alpha.</me>
</p>
</statement>
</definition>



<p>
Generally, symmmetry is presumed when using a two-sided confidence interval so that <m>E_1 = E_2 = E</m> and therefore the interval looks like
<me>P(\tilde{p} - E \lt p \lt \tilde{p} + E) = 1 - \alpha.</me>
In this case, E is known as the margin of error.
</p>

<p>
To determine E carefully, note that from the central limit theorem
<me>\frac{Y-np}{\sqrt{np(1-p}} = \frac{\tilde{p} - p}{\sqrt{p(1-p)/n}}</me>
is approximately standard normal for large n.  Presuming that <m>\tilde{p} \approx p</m> and replacing the unknown p terms on the bottom with <m>\tilde{p}</m> gives 
<me>z = \frac{\tilde{p} - p}{\sqrt{\tilde{p}(1-\tilde{p})/n}}</me>
where z is a standard normal distribution variable. So, using the central limit theorem and the standard normal distribution, you can find the value <m> z_{ \alpha/2}</m> where
<me>P( -z_{ \alpha/2} \lt z \lt z_{ \alpha/2}) = 1 - \alpha</me>
<me>P( -z_{ \alpha/2} \lt \frac{\tilde{p} - p}{\sqrt{\tilde{p}(1-\tilde{p})/n}} \lt z_{ \alpha/2}) = 1 - \alpha</me>
or by rearranging the inside inequality
<me>P( \tilde{p} - z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n} \lt  p \lt \tilde{p} + z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n}) = 1 - \alpha.</me>
Setting <m>E = z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n}</m> gives a way to determine a confidence interval centered on <m>\tilde{p} = \frac{Y}{n}</m> for p with "confidence level" <m>1-\alpha</m>.  
</p>

<p>
To complete the interval, one needs a specific value for <m>z_{ \alpha/2}</m>.
Generally, one chooses confidence levels on the order of 90%, 95%, or 99% with 95% being the usual choice.  Fortunately this value is easily computed using graphing calculators or other automatic methods although your ancient teacher might have been required to use tables. On a TI calculator, use
<me>z_{\alpha /2} = \text{InvNorm}( 1 - \frac{\alpha}{2} )</me>
</p>


<p>
For 90% confidence level, you need to find a z-value so that
<me>P( -z_{ \alpha/2} \lt z \lt z_{ \alpha/2}) = 0.9 = 1 - 0.1 .</me>
Using the symmetry of the normal distribution, this can be rewritten
<me>F(z_{ \frac{0.1}{2}}) = P( z \lt z_{ \frac{0.1}{2}}) = 0.95 = 1 - \frac{0.1}{2} .</me>
Using the inverse of the standard normal distribution (on the TI calculator this is InvNorm(0.95)) gives <m>z_{ 0.05} \approx 1.645</m>.
</p>

<p>
Similarly, for a 95 % confidence level, find where
<me>F(z_{ \frac{0.05}{2}}) = P( z \lt z_{ \frac{0.05}{2}}) = 0.975 = 1 - \frac{0.05}{2} .</me>
The calculators InvNorm(0.975) gives <m>z_{ 0.025} \approx 1.960</m>.
</p>

<p>
For a 99 % confidence level, find where
<me>F(z_{ \frac{0.01}{2}}) = P( z \lt z_{ \frac{0.01}{2}}) = 0.995 = 1 - \frac{0.01}{2} .</me>
The calculators InvNorm(0.995) gives <m>z_{ 0.005} \approx 2.576</m>.
</p>

<p>
Notice that when computing the confidence intervals above that we choose to just replace some of the p terms with <m>\tilde{p}</m> so that only one p term was left and could be isolated in the middle. There are other ways to deal with this. The easiest is to take the worst case scenario for the p terms in the denominator above. Indeed, the confidence interval is made wider (and therefore more likely to contain the actual p) if the square root term is as large as possible, using basic calculus it is easy to see that p(1-p) is maximized when p = 1/2. Therefore, a second alternative is to create your confidence interval using
<me>z = \frac{\tilde{p} - p}{\frac{1}{2\sqrt{n}}}</me>
and therefore <m>E = \frac{z_{ \alpha/2}}{2\sqrt{n}}</m>.
This method should be used only when trying to create the roughest and "safest" interval.
</p>

<p>
The methods for determining a confidence interval for p above depend upon a good approximation with the Central Limit Theorem. This approximation will be fine if n is relatively large. To consider a confidence interval for p when n is small, note that the binomial random variable is discrete and so expanding the interval by a factor of <m>\frac{1}{2n}</m> might be in order. Indeed, replace <m>z_{\alpha/2}</m> by <m>t_{\alpha/2}(n-1)</m> and continue otherwise.
</p>

<p>
Another more elaborate mechanism when n is relatively large is given by the Wilson Score. This confidence interval is more complicated than just taking <m>\tilde{p}</m> and adding and subtracting E. This approach notes that the possible extreme values for p must satisfy (before replacing some of the p terms with <m>\tilde{p}</m>)
</p>



<theorem xml:id="WilsonConfidenceInterval"><title>Wilson Score Confidence Interval for p</title>
<statement>
<p>
<me>\frac{\tilde{p} + \frac{z_{\alpha/2}^2}{2n} - z_{\alpha/2} \sqrt{\frac{\tilde{p}(1-\tilde{p}) + \frac{z_{\alpha/2}^2}{4n}}{n}}}{1 + \frac{z_{\alpha/2}^2}{n}} \lt p \lt \frac{\tilde{p} + \frac{z_{\alpha/2}^2}{2n} + z_{\alpha/2} \sqrt{\frac{\tilde{p}(1-\tilde{p}) + \frac{z_{\alpha/2}^2}{4n}}{n}}}{1 + \frac{z_{\alpha/2}^2}{n}}
</me>
</p>
</statement>
<proof>
<p>Again, noting that <m>\tilde{p} = \frac{Y}{n}</m>, the expression above
<me> \big | p - \tilde{p} \big | = z_{\alpha /2} \sqrt{\frac{p(1-p)}{n}}</me>
can be simplified by squaring both sides to get

<me> \big ( p - \tilde{p} \big )^2 = z_{\alpha /2}^2 \frac{p(1-p)}{n}.</me>

Replacing <m>\tilde{p}</m> with the relative frequency gives
<me> \big ( p - \frac{Y}{n} \big )^2 = z_{\alpha /2}^2 \frac{p(1-p)}{n}</me>

or by simplifying
<me>(n+z_{\alpha /2}^2 )p^2 - (2Y+z_{/alpha /2}^2) p + \frac{Y^2}{2} = 0.</me>

Solving for p using the quadratic formula and simplifying ultimately results in the described interval.
</p>
</proof>
</theorem>



<example><title>Comparison of the three Confidence Interval methods for p</title>
<p>
Presume that from a sample of size n = 400 you get Y = 144 successes.  Determine 95% two-sided confidence intervals for the actual p using all three of the methods above. Note that for each you will utilize <m>z_{\alpha/2} = z_{0.025} = 1.960</m> and <m>\tilde{p} = \frac{144}{400} = 0.36</m>.
</p>

<p>Normal Interval:
<me>P( 0.36 - 1.96 \sqrt{0.36 \cdot 0.64) / 400} \lt  p \lt 0.36 + 1.96 \sqrt{0.36 \cdot 0.64) / 400}) = 1 - \alpha.</me>
or
<me>P( 0.36 - 1.96 \cdot 0.6 \cdot 0.8) / 20 \lt  p \lt 0.36 + 1.96 \cdot 0.6 \cdot 0.8) / 20) = 0.95 </me>
or
<me>P( 0.36 - 0.04704 \lt  p \lt 0.36 + 0.04704) = 0.95 .</me>
or
<me>P( 0.31296 \lt  p \lt 0.40704) = 0.95 .</me>
So, there is a 95% chance that the actual value for p lies inside the interval
<m>(0.31296 , 0.40704).</m>
</p>

<p>Maximal Interval:
<me>P( 0.36 - 1.960 \frac{1}{2\sqrt{400}} \lt  p \lt 0.36 + 1.960 \frac{1}{2\sqrt{400}} ) = 1 - \alpha.</me>
or
<me>P( 0.36 - 1.960 \frac{1}{40} \lt  p \lt 0.36 + 1.960 \frac{1}{40} ) = 1 - \alpha.</me>
or
<me>P( 0.311 \lt  p \lt 0.409 ) = 1 - \alpha.</me>
Notice the interval is only slightly wider than when using <m>\tilde{p}</m> to estimate p in the first case.
</p>

<p>Wilson Score Interval:  Let's do this on in parts...
<me>z_{\alpha/2} \sqrt{\frac{\tilde{p}(1-\tilde{p}) + \frac{z_{\alpha/2}^2}{4n}}{n}} = 1.96 \sqrt{ \frac{0.36 \cdot 0.64 + \frac{1.96^2}{1600}}{400}} \approx 0.04728
</me>
Therefore, 
<me>\frac{0.36 + \frac{1.96^2}{800} - 0.04728}{1 + \frac{1.96^2}{400}} \lt p \lt \frac{0.36 + \frac{1.96^2}{800} + 0.04728}{1 + \frac{1.96^2}{400}}
</me>
or
<me>0.3145 \lt p \lt 0.4082</me>
which is slightly different than the first and slightly smaller than the second.
</p>

</example>




<exercise><title>WebWork - Confidence Intervals</title>
<webwork source="Library/ASU-topics/setStat/kolossa49.pg">
</webwork>
</exercise>



<theorem xml:id="SampleSizeForConfidenceIntervalForP"><title>Determining Sample Size for proportions with a preliminary estimate</title>
<statement>
<p>
Given a margin of error E and preliminary relative frequency estimate <m>\tilde{p_0}</m> the sample size needed to create the corresponding confidence interval is given by 
<me>n \gt \left ( \frac{z_{\alpha /2}}{E} \right )^2 \tilde{p_0}(1-\tilde{p_0}).</me>
</p>
</statement>
<proof>
<p>
From the confidence interval
<me>P( \tilde{p} - z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n} \lt  p \lt \tilde{p} + z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n}) = 1 - \alpha,</me>
note that 
<me>E = z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n}.</me>
Presuming E is given and n is unknown, simply solve for n (noting that n is an integer and therefore you will likely need to replace the equality with an appropriate inequality).
</p>
</proof>
</theorem>




<theorem xml:id="SampleSizeForConfidenceIntervalForPWithoutPreliminary"><title>Determining Sample Size for proportions with no preliminary estimate</title>
<statement>
<p>
Given only a margin of error E, the sample size needed to create the corresponding confidence interval is given by 
<me>n \gt \left ( \frac{z_{\alpha /2}}{2 E} \right )^2 .</me>
</p>
</statement>
<proof>
<p>
Note that the maximum for <m>y = x(1-x) </m> occurs at <m> x = 1/2, y = 1/4.</m> Therefore, replacing <m>\tilde{p_0}(1-\tilde{p_0} \le \frac{1}{4}</m> gives the result.
</p>
</proof>
</theorem>



<example><title>Determining Sample Size for one proportion</title>
<p>
Given a 99% confidence level, margin of error E=0.03, and preliminary estimate <m>\tilde{p_0} = 0.35</m>, notice that <m>z_{\alpha / 2} = 2.58</m> gives
<me>n \gt \left ( \frac{2.58}{0.03} \right )^2 0.35 \cdot 0.65 \approx 1682.59</me>
or a sample size of at least 1683.
</p>
</example>



<exercise><title>WebWork - Sample Size</title>
	<webwork source="Library/UBC/STAT/STAT200/hw07/hw07-q04b.pg">
	</webwork>
</exercise>

</section>

<section xml:id="ConfidenceIntervalForMean"><title>Interval Estimates - Confidence Interval for <m>\mu</m></title>
<p>
As with the confidence intervals above for proportions, the Central Limit Theorem also allows you to create an interval centered on a sample mean for estimating the population mean <m>\mu</m>.
</p>


<definition xml:id="DefnConfidenctIntervalForMean"><title>Confidence Interval for One Mean</title>
<statement>
<p>
Given a sample mean <m>\overline{x}</m>, a two-sided confidence interval for the mean with confidence level <m>1-\alpha</m> is an interval 
<me>\overline{x} - E_1 \lt \mu \lt \overline{x} + E_2</me>
such that
<me>P(\overline{x} - E_1 \lt \mu \lt \overline{x} + E_2) = 1-\alpha.</me>
Generally, the interval is symmetrical of the form <m>\overline{x} \pm E</m> with E again known as the margin of error.  One-sided confidence intervals can be determined in the same manner as in the previous section.
</p>
</statement>
</definition>


<p>
Once again, utilize the Central Limit Theorem.  Notice that the symmetrical confidence interval 
<me>P(\overline{x} - E \lt \mu \lt \overline{x} + E) = 1-\alpha.</me>
is equivalent to

<me>P \left ( \frac{-E}{\sigma / \sqrt{n}} \lt \frac{\overline{x} - \mu}{\sigma / \sqrt{n}} \lt \frac{E}{\sigma / \sqrt{n}} \right ) = 1 - \alpha</me>
in which the middle term can be approximated using a standard normal variable and therefore this statement is approximately

<me>P \left ( \frac{-E}{\sigma / \sqrt{n}} \lt Z \lt \frac{E}{\sigma / \sqrt{n}} \right ) = 1 - \alpha.</me>

Using the symmetry of the standard normal distribution about Z=0 gives
<me>\Phi (z_{\alpha/2} ) = \Phi \left ( \frac{E}{\sigma / \sqrt{n}} \right ) = P \left ( Z \lt \frac{E}{\sigma / \sqrt{n}} \right ) = 1 - \frac{\alpha}{2}</me>

and so to determine E again requires the inverse of the standard normal distribution function.  Using an appropriate <m>z_{\alpha /2}</m> (as determine in a manner described in the previous section) gives a confidence interval for the mean
<me>\overline{x} - z_{\alpha / 2} \frac{\sigma}{\sqrt{n}} \lt \mu \lt \overline{x} + z_{\alpha / 2} \frac{\sigma}{\sqrt{n}}</me>
with confidence level <m>1-\alpha</m> and margin of error 
<me>E = z_{\alpha /2} \frac{\sigma}{\sqrt{n}}.</me>
</p>


<exercise><title>WebWork - Confidence Interval for the Mean</title>
	<webwork source="Library/CollegeOfIdaho/setStatistics_Ch14InferenceIntro/14Stats_05_InferenceIntro.pg">
	</webwork>
</exercise>



<p>
It should be noted that the use of the Central Limit Theorem makes the use of  InvNorm an approximation. It can be shown that so long as n is larger than 30 then generally this approximation is reasonable. If not, then use replace the z-score with a corresponding value from the t-distribution.
</p>


<exercise><title>WebWork - Confidence Interval with t-scores</title>
	<webwork source="Library/Rochester/setStatistics3Estimates/ur_stt_3_6.pg">
	</webwork>
</exercise>



<p>
Additionally, this derivation assumes that <m>\mu</m> is not known...indeed the goal is to approximate that mean using <m>\overline{x}</m>...but that <m>\sigma</m> is known. This is often not the case. It can however be shown that if n is larger than 30, replacing <m>\sigma</m> with the sample standard deviation s gives an acceptable confidence interval.
</p>


<theorem xml:id="SampleSizeForConfidenceIntervalForMeanWithoutPreliminary"><title>Sample Size needed for <m>\mu</m> given Margin of Error</title>
<statement>
<p>
Given confidence level <m>1-\alpha</m> and margin of error E, the sample size needed to determine an appropriate confidence interval satisfies
<me> n \gt \left ( z_{\alpha /2} \frac{\sigma}{E} \right )^2</me>
</p>
</statement>
<proof>
<p>
Solve for n in the formula for E above. Notice that n must be an integer so you will need to round up. You will also need an estimate for the sample standard deviation s by using a preliminary sample. 
</p>
</proof>
</theorem>


<p>
Notice, in practice you might want to take n to be a little larger than the absolute minimum value prescribed above since you are dealing with approximations (Central Limit Theorem and the use of an estimate for s rather than the actual <m>\sigma</m>.)
</p>


<example><title>Determining Sample Size for one Mean</title>
<p>
Given a 95% confidence level, margin of error E=0.1, and preliminary sample with standard deviation s = 2, <m>z_{\alpha / 2} = 1.96</m> gives
<me>n \gt \left ( 1.96 \cdot \frac{2}{0.1} \right )^2 \approx 1536.64</me>
or a sample size of at least 1537.
</p>
</example>

</section>


<section xml:id="ConfidenceIntervalsForSigma2"><title>Interval Estimates - Confidence Interval for <m>\sigma^2</m></title>

<p>
Once again, you may need to approximate the population variance or standard deviation but only have the sample values available. One difference from the previous sections is that you are not dealing with an average of values (such as <m>\overline{x}</m> or <m>\tilde{p}</m>) but with the average of the squares of values. The Central Limit Theorem does not directly help you in this case but the following result (presented without proof) provides a solution.
</p>


<theorem><title>Relationship between Variance and <m>\chi ^2</m></title>
<statement>
<p>If <m>S^2</m> is a random variable of possible sample variance values from a sample of size n, then
<me>W = \frac{(n-1)S^2}{\sigma^2}</me>
is approximately <m>\chi ^2(n-1).</m>
</p>
</statement>
</theorem>

<p>
To create a confidence interval for <m>\sigma^2</m> first consider an interval of the form
<me>E_1 \lt \sigma^2 \lt E_2</me>
and determine values for the boundaries so that the likelihood of this being true is high. For this case, since the chi-square distribution only has a positive domain and is not symmetrical, you will not expect to determine a symmetrical confidence interval.  Therefore, consider
<me>P (E_1 \lt \sigma^2 \lt E_2 ) = 1 - \alpha </me> 

and by playing around with algebra you get
<me>P \left ( \frac{E_1}{(n-1)S^2} \lt \frac{\sigma^2}{(n-1)S^2} \lt \frac{E_2}{(n-1)S^2} \right ) = 1 - \alpha </me> 

or by inverting the inequality yields
<me>P \left ( \frac{(n-1)S^2}{E_2} \lt \frac{(n-1)S^2}{\sigma^2} \lt \frac{(n-1)S^2}{E_1} \right ) = 1 - \alpha .</me> 

Using the previous theorem, note that the inside variable can be replaced with a chi-square variable. If F is the distribution function for chi-square, then you get
<me>F \left ( \frac{(n-1)S^2}{E_1} \right ) - F \left ( \frac{(n-1)S^2}{E_2} \right ) = 1 - \alpha .</me> 

For a given value of <m>\alpha</m> there are many possible choices but often one often utilized is one in which 

<me> F(\chi^2_{1-\alpha/2} ) = F \left ( \frac{(n-1)S^2}{E_1} \right ) = 1 - \alpha / 2</me>
and
<me> F(\chi^2_{\alpha/2} ) = F \left ( \frac{(n-1)S^2}{E_2} \right ) = \alpha / 2.</me> 

Using the inverse chi-square gives values for the expression on the inside and algebra can be used to solve for each of <m>E_1, E_2</m>. Indeed,

<me> E_1 = \frac{(n-1)S^2}{\chi^2_{1-\alpha/2}} </me>
and
<me> E_2 = \frac{(n-1)S^2}{\chi^2_{\alpha/2}} </me>
</p>

<p>
To determine appropriate values for <m>\chi^2_{\alpha/2} </m> and <m>\chi^2_{1-\alpha/2} </m> with equal probabilities in each tail, consider using the interactive cell below:
</p>

<sage>
	<input>
# Chi-Square Calculator for confidence intervals with equal alpha/2 tails
var('t')
@interact(layout=dict(top=[['c'],['n']]))
def _(c=input_box(0.95,width=10,label='Confidence Level = '),n=input_box(20,width=8,label='n =')):
    alpha = 1-c
    T = RealDistribution('chisquared', n)
    a = T.cum_distribution_function_inv(alpha/2)
    a1 = T.cum_distribution_function(a)
    b = T.cum_distribution_function_inv(1-alpha/2)
    b1 = T.cum_distribution_function(b)
    
    print('From the Chi-Square distribution for X:')
    print('P(',a,'&lt; X &lt; ',(b),') = ',c)
    print('with')
    print('P( X &lt; ',a,') = ',a1)
    print('P( X &lt; ',b,') = ',b1)
    
    f = x^(n/2-1)*e^(-x/2)/(gamma(n/2)*2^(n/2))
    G = plot(f,x,0,b+(b-a)/2)+plot(f,x,a,b,thickness=5,color='green')
    G += line([(a,0),(a,f(x=a))],color='green',thickness=3)
    G += line([(b,0),(b,f(x=b))],color='green',thickness=3)
    G += text(str(c.n(digits=5)),((a+b)/2,f(x=(a+b)/2)/3),color='green')
    G.show()
	</input>
</sage>

<p>
You can simply type in your input values easily in the Sage cell below.
</p>

<sage>
	<input>
# Chi-Square Calculator specifics
var('t')
c=0.95
n=8
alpha = 1-c
T = RealDistribution('chisquared', n)
a = T.cum_distribution_function_inv(alpha/2)
a1 = T.cum_distribution_function(a)
b = T.cum_distribution_function_inv(1-alpha/2)
b1 = T.cum_distribution_function(b)

print('From the Chi-Square distribution for X:')
print('P(',a,'&lt; X &lt; ',(b),') = ',c)
print('with')
print('P( X &lt; ',a,') = ',a1)
print('P( X &lt; ',b,') = ',b1)

f = x^(n/2-1)*e^(-x/2)/(gamma(n/2)*2^(n/2))
G = plot(f,x,0,b+(b-a)/2)+plot(f,x,a,b,thickness=5,color='green')
G += line([(a,0),(a,f(x=a))],color='green',thickness=3)
G += line([(b,0),(b,f(x=b))],color='green',thickness=3)
G += text(str(c.n(digits=5)),((a+b)/2,f(x=(a+b)/2)/3),color='green')
G.show()
	</input>
</sage>


<example><title> - Two-sided Confidence interval for <m>\sigma^2</m> and <m>\sigma</m></title>
<p>Given the data 570, 561, 546, 540, 609, 580, 550, 577, 585, determine a 95% confidence interval for <m>\sigma^2</m>.
</p>
<p>
Using the computational forumaula (or your calculator) gives <m>s^2 \approx 479.5</m>. Also, notice for n=9, the resulting interval will use a Chi-square variable with 8 degrees of freedom. Using the symmetric option, gives
<m>\chi_{0.025}^2 = 2.18</m> and <m>\chi_{0.975}^2 = 17.53</m>.  Therefore
<me> E_1 = \frac{8 \cdot 479.5}{17.53} \approx 221.095</me>
and
<me> E_2 = \frac{8 \cdot 479.5}{2.18} \approx 1759.63.</me>
Hence, you are 95% certain that 
<me>221.095 \lt \sigma^2 \lt 1759.63.</me>
By taking square roots you get
<me>14.87 \lt \sigma \lt 41.95.</me>
Notice, this interval is relatively wide which is a result both of the number of data values being relatively small (n=9) and the actual data values being relatively large and spread out.
</p>
</example>



<sage>
	<input>
# Chi-Square Calculator specifics
var('t')
c=0.95
n=399
alpha = 1-c
T = RealDistribution('chisquared', n)
a = T.cum_distribution_function_inv(alpha/2)
a1 = T.cum_distribution_function(a)
b = T.cum_distribution_function_inv(1-alpha/2)
b1 = T.cum_distribution_function(b)

print('From the Chi-Square distribution for X:')
print('P(',a,'&lt; X &lt; ',(b),') = ',c)
print('with')
print('P( X &lt; ',a,') = ',a1)
print('P( X &lt; ',b,') = ',b1)
	</input>
</sage>




<exercise><title>WebWork - Two-sided Confidence Interval with large n</title>

<webwork source="Library/Rochester/setStatistics3Estimates/ur_stt_3_20.pg">
</webwork>
</exercise>



<p>Suppose  that you have n=400 data values and suppose you have computed from those a sample variance of <m>s^2 = 479.5</m>.  Then, the only change in the calculation is the two chi-square statistic values. For 95% but now with 399 degrees of freedom
<m>\chi_{0.025}^2 = 345.55</m> and <m>\chi_{0.975}^2 = 456.24</m>.  
</p>
<p>
	Therefore
	<me> E_1 = \frac{8 \cdot 479.5}{456.24} \approx 419.3</me>
	and
	<me> E_2 = \frac{8 \cdot 479.5}{345.55} \approx 553.7.</me>
	Hence, you are 95% certain that 
	<me>419.24 \lt \sigma^2 \lt 553.7.</me>
	By taking square roots you get
	<me>20.48 \lt \sigma \lt 23.53</me>
	which is a relatively tight confidence interval.  Notice, these are also completely contained in the confidence intervals from the previous small n example.
</p>


<p>
Similar to above, another choice to estimate <m>\sigma ^2</m> is to use a one sided confidence interval. If you want to find one of these, continue as described above but just leave one endpoint off.  Indeed, 
	<me>\sigma^2 \lt E_2</me>
can be determined using 
	<me> F(\chi^2_{\alpha} ) = F \left ( \frac{(n-1)S^2}{E_2} \right ) = \alpha</me>
and 
	<me>E_1 \lt \sigma^2 </me>
can be determined using
	<me> F(\chi^2_{1-\alpha} ) = F \left ( \frac{(n-1)S^2}{E_1} \right ) = 1 - \alpha.</me>
</p>


<example><title> - One-sided Confidence intervals for <m>\sigma^2</m></title>
<p>
TBA
</p>
</example>


<p>
You can, of course, use the formula to work out the sample size needed in order to have a sufficiently narrow confidence interval
</p>


<exercise><title>WebWork - One-sided Confidence Interval</title>
<webwork source="Library/Rochester/setStatistics3Estimates/ur_stt_3_22.pg">
</webwork>
</exercise>


<p>
Finally, to determine a confidence interval for <m>\sigma</m>, proceed using the protocols described above and simply take the square root on the resulting interval.
</p>


<example><title> - Confidence intervals for <m>\sigma</m></title>
<p> TBA
</p>
</example>


</section>

<section xml:id="IntervalEstimationExercises"><title>Exercises</title>



<exercise><title>Basic Confidence interval for p</title>
<p>
Given Y = 30 successes in n = 100 trials, determine a 90% confidence interval for the unknown value for p.  
</p>
<solution>
<p>
<m>\tilde{p} = 0.3</m> and <m>z_{0.05} = 1.645</m> gives
<me>0.3 - 1.645 \sqrt{\frac{0.3 \cdot 0.7}{100}} \lt p \lt 0.3 + 1.645 \sqrt{\frac{0.3 \cdot 0.7}{100}}</me>
or
<me>0.225 \lt p \lt 0.375.</me>
</p>
</solution>
</exercise>



<exercise><title> - Sample Size for confidence interval for p</title>
<p>
Given a preliminary estimate <m>\tilde{p_0} = 0.23</m>, determine the same size needed for determine a 95% confidence interval for p with margin of error 0.02.
</p>
<solution>
<p>
Using <m>z_{0.025} = 1.96</m>,
<me>n \gt \big ( \frac{1.96}{0.02} \big )^2 \cdot 0.23 \cdot 0.77 \approx 1700.87</me>
and so pick at least 1701 as the sample size.
</p>
</solution>
</exercise>



<exercise><title> - Voting projection</title>
<p>Randomly polling 3200 eligible voters for governor in a particular state resulted in finding that 1590 favored your candidate. Determine an appropriate 95% confidence interval for the true proportion p of voters who favor your candidate. Noting that <m>\tilde{p}</m> in this instance is smaller than 50%, write a short paragraph regarding what you might conclude from this confidence interval regarding your candidate's chances in winning the election.
</p>
<solution>
<p>
Note that although the point estimate is below 50%, the confidence interval includes the possibility that the actual value for p is greater than 50%. So, you cannot conclude that your candidate will either win or lose.
</p>
</solution>
</exercise>



<exercise><title> - Basic Confidence interval for the mean</title>
<p>
Given a sample mean of <m>\overline{x} = 25.3</m> with n = 121 and sample variance <m>s^2 = 12.1</m>, determine a 99% confidence interval for the true mean <m>\mu</m>.
</p>
<solution>
	<p>Using <m>z_{0.005} = 2.576</m> and <m>s = \sqrt{12.1} \approx 3.4786</m> gives a confidence interval 
	<me>25.3 - 2.576 \cdot \frac{3.4786}{11} \lt \mu \lt 25.3 + 2.576 \cdot \frac{3.4786}{11}</me>
or 
	<me>24.4854 \lt \mu \lt 26.1148.</me>
	</p>
</solution>
</exercise>



<exercise><title> - Confidence Interval Experiment</title>
<p>
TBA
</p>
</exercise>
>

<p>
Roll two regular pair of dice 35 times, recording the sum of the dots for each roll.  Using the data from your sample, determine the corresponding sample mean and sample variance.  Using this data, create a 95% confidence interval for the population mean and a 95% "centered" confidence interval for the standard deviation. Once complete, compare your results with what you know should be the correct population statistics an appropriate "hat" distribution. (You might want to use the chi-square calculator provided earlier in this text.)
</p>

<p>
Go back over your 35 rolls and count the number of 7's or 11's rolled. Determine a corresponding relative frequency for this outcome. Using this data, create a 95% confidence interval for the theoretical proportion of success p.  Compare your result with what you know should be the correct theoretical p. 
</p>
<p>
Repeat this exercise but this time roll 105 times.  Notice how these differ from the confidence intervals created with the smaller set.  Write a paragraph describing how these compare and whether one is better or not than the other.
</p>
</section>


</chapter>
