<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="IntervalEstimation" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Estimation</title>

<section><title>Introduction</title>
<p>
You should have noticed by now that repeatedly sampling from a given distribution will yield a variety of sample statistics such as <m>\overline{x}</m> as an estimate perhaps for the population mean <m>\mu</m> or <m>\frac{Y}{n}</m> as an estimate for the population likelihood of success p. In this section, you will see how these sample "point estimators" are actually the best possible choices.
</p>
<p>
In creating these point estimates repeatedly, you have noticed that the results will change somewhat over time. Indeed, flip a coin 20 times and you might expect 10 heads. However, in practice it is likely to 9 or 12 out of 20 and possible to get any of the other possible outcomes. This natural variation makes the point estimates noted above to almost certainly be in error. However one would expect that they should be close and the Central Limit Theorem does indicate that the distribution of sample means should be approximately normally distributed. Thus, instead of relying just on the value of the point estimate, you might want to investigate a way to determine a reasonable interval centered on the sample statistic in which you have some confidence the actual population statistic should belong. This leads to a discussion of interval estimates known as confidence intervals (using calculational tools) and statistical tolerance intervals (using order statistics).
</p>
</section>

<section><title>Point Estimates</title>
<p>
Equally likely point estimates.
</p>
<p>
For Binomial, Geometric, what is p? For exponential, what is lambda?  For normal, what is mu and sigma?
</p>
</section>

<section>	
	<title>Interval Estimates - Chebyshev</title>

	<p>An interval centered on the mean in which at least a certain proportion
	of the actual data must lie.
	</p>

	<theorem>
	<title>Chebyshev's Theorem</title>
	<statement>
	Given a random variable X with given mean μ and standard deviation σ, for k>1 at least
	2
	1 1
	k − of the observations lie within k standard deviations from the mean.
	</statement>
	</theorem>
</section>

<section><title>Interval Estimates - Confidence Intervals</title>
<p>
Confidence intervals centered on a single mean, on a single variance, and on p.
</p>
</section>


</chapter>