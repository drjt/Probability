<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="IntervalEstimation" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Estimation</title>

<section><title>Introduction</title>
<p>
You should have noticed by now that repeatedly sampling from a given distribution will yield a variety of sample statistics such as <m>\overline{x}</m> as an estimate perhaps for the population mean <m>\mu</m> or <m>\frac{Y}{n}</m> as an estimate for the population likelihood of success p. In this section, you will see how these sample "point estimators" are actually the best possible choices.
</p>
<p>
In creating these point estimates repeatedly, you have noticed that the results will change somewhat over time. Indeed, flip a coin 20 times and you might expect 10 heads. However, in practice it is likely to 9 or 12 out of 20 and possible to get any of the other possible outcomes. This natural variation makes the point estimates noted above to almost certainly be in error. However one would expect that they should be close and the Central Limit Theorem does indicate that the distribution of sample means should be approximately normally distributed. Thus, instead of relying just on the value of the point estimate, you might want to investigate a way to determine a reasonable interval centered on the sample statistic in which you have some confidence the actual population statistic should belong. This leads to a discussion of interval estimates known as confidence intervals (using calculational tools) and statistical tolerance intervals (using order statistics).
</p>
<p>
In this chapter we first discuss how to determine appropriate methods for estimating the needed population statistics (point estimates) and then quantify how good they are (confidence intervals).
</p>
</section>

<section><title>Point Estimates</title>

<p>
For Binomial, Geometric, what is p? For exponential, what is lambda?  For normal, what are mu and sigma? Each of these parameters are necessary before you can compute any probability values from their respective formulas. Since they might not be given in a particular instance, they will need to be estimated in some manner. Such an estimate however is very likely going to only be a close approximation to the exact (but unknown) value.
</p>

<p>What is an Estimator? A Maximum Likelihood Estimator?
</p>

<p>Maximum Likelihood estimators for p for Binomial, Geometric, Negative Binomial.
</p>

<p>Maximum Likelihood estimators for mu for Poisson, Exponential, Gamma.
</p>

<p>Maximum Likelihood estimators fro mu and sigma for Normal.
</p>

</section>

<section>	
	<title>Interval Estimates - Chebyshev</title>

	<p>An interval centered on the mean in which at least a certain proportion
	of the actual data must lie.
	</p>

	<theorem>
	<title>Chebyshev's Theorem</title>
	<statement>
	Given a random variable X with given mean μ and standard deviation σ, for k>1 at least
	2
	1 1
	k − of the observations lie within k standard deviations from the mean.
	</statement>
	</theorem>
</section>

<section><title>Interval Estimates - Confidence Interval for p</title>
<p>
Sometimes selecting a value for p for a Binomial, Geometric, or Negative Binomial distribution problem can be done by using a theoretical value. Indeed, when flipping a coin it is reasonable to assume p = 1/2 is the probability of getting a head on one flip. Similarly, it is reasonable to assume p = 1/6 when you are looking for a particular side of a 6-sided die. However, many times you will want to deal with a problem in which it is not possible to determine exactly the precise value for the likelihood of success such as your true probability of making a free throw in basketball or knowing the true percentage of the electorate that will vote for your favorite candidate. 
</p>
<p>
In these later situations, we found in the previous section that relative frequency <m>\frac{Y}{n}</m> is generally a good way to estimate p. In this section, you will investigate how to measure the closeness--and thereby assure some confidence in that estimate--regarding how well the point estimate approximates the actual value of p.
</p>
<definition><title>Confidence Intervals for p</title>
<statement>
<p>Given a point estimate <m>\tilde{p}</m> for p, a confidence interval is a range of values which contains the actual value of p with high probability. In notation, a two-sided confidence interval for p is of the form
<me>P(\tilde{p} - E_1 \lt p \lt \tilde{p} + E_2) = 1 - \alpha</me>
where <m>\alpha</m> is near 0 and <m>E_k \gt 0</m>.  One-sided confidence intervals for p can be similarly described 
<me>P( p \lt \tilde{p} + E_2) = 1 - \alpha</me>
or
<me>P(\tilde{p} - E_1 \lt p) = 1 - \alpha.</me>
</p>
</statement>
</definition>




<p>
Generally, symmmetry is presumed when using a two-sided confidence interval so that <m>E_1 = E_2 = E</m> and therefore the interval looks like
<me>P(\tilde{p} - E \lt p \lt \tilde{p} + E) = 1 - \alpha.</me>
In this case, E is known as the margin of error.
</p>
<p>
To determine E carefully, note that from the central limit theorem
<me>\frac{Y-np}{\sqrt{np(1-p}} = \frac{\tilde{p} - p}{\sqrt{p(1-p)/n}}</me>
is approximately standard normal for large n.  Presuming that <m>\tilde{p} \approx p</m> and replacing the unknown p terms on the bottom with <m>\tilde{p}</m> gives 
<me>z = \frac{\tilde{p} - p}{\sqrt{\tilde{p}(1-\tilde{p})/n}}</me>
where z is a standard normal distribution variable. So, using the central limit theorem and the standard normal distribution, you can find the value <m> z_{ \alpha/2}</m> where
<me>P( -z_{ \alpha/2} \lt z \lt z_{ \alpha/2}) = 1 - \alpha</me>
<me>P( -z_{ \alpha/2} \lt \frac{\tilde{p} - p}{\sqrt{\tilde{p}(1-\tilde{p})/n}} \lt z_{ \alpha/2}) = 1 - \alpha</me>
or by rearranging the inside inequality
<me>P( \tilde{p} - z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n} \lt  p \lt \tilde{p} + z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n}) = 1 - \alpha.</me>
Setting <m>E = z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n}</m> gives a way to determine a confidence interval centered on <m>\tilde{p} = \frac{Y}{n}</m> for p with "confidence level" <m>1-\alpha</m>.  
</p>
<p>
To complete the interval, one needs a specific value for <m>z_{ \alpha/2}</m>.
Generally, one chooses confidence levels on the order of 90%, 95%, or 99% with 95% being the usual choice.
</p>
<p>
For 90% confidence level, you need to find a z-value so that
<me>P( -z_{ \alpha/2} \lt z \lt z_{ \alpha/2}) = 0.9 = 1 - 0.1 .</me>
Using the symmetry of the normal distribution, this can be rewritten
<me>F(z_{ \frac{0.1}{2}}) = P( z \lt z_{ \frac{0.1}{2}}) = 0.95 = 1 - \frac{0.1}{2} .</me>
Using the inverse of the standard normal distribution (on the TI calculator this is InvNorm(0.95)) gives <m>z_{ 0.05} \approx 1.645</m>.
</p>
<p>
Similarly, for a 95 % confidence level, find where
<me>F(z_{ \frac{0.05}{2}}) = P( z \lt z_{ \frac{0.05}{2}}) = 0.975 = 1 - \frac{0.05}{2} .</me>
The calculators InvNorm(0.975) gives <m>z_{ 0.025} \approx 1.960</m>.
</p>
<p>
For a 99 % confidence level, find where
<me>F(z_{ \frac{0.01}{2}}) = P( z \lt z_{ \frac{0.01}{2}}) = 0.995 = 1 - \frac{0.01}{2} .</me>
The calculators InvNorm(0.995) gives <m>z_{ 0.005} \approx 2.576</m>.
</p>
<p>
Notice that when computing the confidence intervals above that we choose to just replace some of the p terms with <m>\tilde{p}</m> so that only one p term was left and could be isolated in the middle. There are other ways to deal with this. The easiest is to take the worst case scenario for the p terms in the denominator above. Indeed, the confidence interval is made wider (and therefore more likely to contain the actual p) if the square root term is as large as possible, using basic calculus it is easy to see that p(1-p) is maximized when p = 1/2. Therefore, a second alternative is to create your confidence interval using
<me>z = \frac{\tilde{p} - p}{\frac{1}{2\sqrt{	n}}}</me>
and therefore <m>E = \frac{z_{ \alpha/2}}{2\sqrt{n}}</m>.
This method should be used only when trying to create the roughest and "safest" interval.
</p>
<p>
The Wilson Score is another way to determine a confidence interval the in some ways is betters than either of the two methods described above. This confidence interval is more complicated than just taking <m>\tilde{p}</m> and adding and subtracting E.  
<theorem><title>Wilson Score Confidence Interval for p</title>
<statement>
<p>
<me>\frac{\tilde{p} + \frac{z_{\alpha/2}^2}{2n} - z_{\alpha/2} \sqrt{\frac{\tilde{p}(1-\tilde{p}) + \frac{z_{\alpha/2}^2}{4n}}{n}}}{1 + \frac{z_{\alpha/2}^2}{n}} \lt p \lt \frac{\tilde{p} + \frac{z_{\alpha/2}^2}{2n} + z_{\alpha/2} \sqrt{\frac{\tilde{p}(1-\tilde{p}) + \frac{z_{\alpha/2}^2}{4n}}{n}}}{1 + \frac{z_{\alpha/2}^2}{n}}
</me>
</p>
</statement>
</theorem>

<example><title>Comparison of the three Confidence Interval methods for p</title>
<p>
Presume that from a sample of size n = 400 you get Y = 144 successes.  Determine 95% two-sided confidence intervals for the actual p using all three of the methods above. Note that for each you will utilize <m>z_{\alpha/2} = z_{0.025} = 1.960</m> and <m>\tilde{p} = \frac{144}{400} = 0.36</m>.
</p>
<p>Normal Interval:
<me>P( 0.36 - 1.960 \sqrt{0.36 \cdot 0.64) / 400} \lt  p \lt 0.36 + 1.960 \sqrt{0.36 \cdot 0.64) / 400}) = 1 - \alpha.</me>
or
<me>P( 0.36 - 1.960 \cdot 0.6 \cdot 0.8) / 20 \lt  p \lt 0.36 + 1.960 \cdot 0.6 \cdot 0.8) / 20) = 0.95 </me>
or
<me>P( 0.36 - 0.04704 \lt  p \lt 0.36 + 0.04704) = 0.95 .</me>
or
<me>P( 0.31296 \lt  p \lt 0.40704) = 0.95 .</me>
So, there is a 95% chance that the actual value for p lies inside the interval
<m>(0.31296 , 0.40704).</m>
</p>
<p>Maximal Interval:
</p>
<p>Wilson Score Interval:
</p>
</example>

</p>
</section>

<section><title>Interval Estimates - Confidence Interval for <m>\mu</m></title>
<p>
Simple Interval
</p>
</section>


<section><title>Interval Estimates - Confidence Interval for <m>\sigma^2</m></title>
<p>
Simple Interval using Chi-Square
</p>
</section>




</chapter>