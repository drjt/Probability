<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="ProbabilityGeneralities" xmlns:xi="http://www.w3.org/2001/XInclude">
<title>Probability Theory</title>

<section xml:id="ProbabilityGeneralitiesIntro"><title>Introduction</title>

<p>
Mathematics generally focuses on providing precise answers with absolute certainty. For example, solving an equation generates specific (and non-varying) solutions. Statistics on the other hand deals with providing precise answers to questions when there is uncertainty. It might seem impossible to provide such precise answers but the focus of this text is to show how that can be done so long as the questions are properly posed and the answers properly interpreted.
</p>
<p>Indeed, people often make claims about being the biggest, best, most often recommended, etc. One sometimes even believes these claims based upon subjective metrics. In this chapter, we will start by looking at relative frequency and notice several properties regarding relative frequencies as the number of trials increases. We will use these examples to motivate a definition for probability and investigate the resulting consequences of that definition.</p>

</section>


<section xml:id="RelativeFrequency"><title>Relative Frequency</title>
	<p>When attempting to precisely measure uncertainty one often resorts to examples or experiments that model the theoretical question of interest. Before we investigate statistical experiments, we need to create some notation that we will utilize throughout the rest of this text.</p>
	<ul>
		<li>S = Universal Set or Sample Space Experiment or Outcome Space. 
		This is the collection of all possiblilities.</li>
		<li>Random Experiment. A random experiment is a repeatable activity that has more than one
		possible outcome all of which can be specified in advance but can not be known in advance with certainty.</li>
		<li>Trial. Performing a Random Experiment one time and measuring the result.</li> 
		<li>A = Event. A collection of outcomes.  Generally denoted by an upper case letter such as A, B, C, etc.</li>
		<li>Success/Failure. When recording the result of a trial, a success for event A occurs when the outcome
		lies in A. If not, then the trial was a failure. There is no qualitative meaning to this term.</li>
		<li>Mutually Exclusive Events. Two events that share no common outcomes. Also known as disjoint events.</li>
		<li>|A| = Frequency. In a sequence of n events, the frequency is the number of trials which resulted in 
		a success for event A.</li>
		<li>|A| / n = Relative Frequency. A proportion of successes to total number of trials.</li>
		<li>Histogram. A bar chart representation of data where area corresponds to the value being described.</li>
	</ul>

	<p>To investigate these terms and to motivate our discussion of probability, consider flipping coins using the interactive cell below. Notice in this case, the sample space S = \{ Heads, Tails \} and the random experiment consists of flipping a fair coin one time. Each trial results in either a Head or a Tail. Since we are measuring both Heads and Tails then we will not worry about which is a success or failure. Further, on each flip the outcomes of Heads or Tails are mutually exclusive events. We count the frequencies and compute the relative frequencies for a varying number of trials selected by you as you move the slider bar. Results are displayed using a histogram.</p>

<p>
<sage>
<input>
coin = ["Heads", "Tails"]
@interact
def _(num_rolls = slider([5..5000],label="Number of Flips")):
	rolls = [choice(coin) for roll in range(num_rolls)]
	show(rolls)   
	freq = [0,0]
	for outcome in rolls:
		if (outcome=='Tails'):
			freq[0] = freq[0]+1
		else:
			freq[1] = freq[1]+1
	print("The frequency of tails = %s"%str(freq[0])+" and heads = %s"%str(freq[1]))
	rel = [freq[0]/num_rolls,freq[1]/num_rolls]
	print("\nThe relative frequencies for Tails and Heads:"+str(rel))
	show(bar_chart(freq,axes=False,ymin=0,xmin=-1/2))     #  A histogram of the results
</input>
</sage>
</p>

<p>
	Question 1: What do you notice as the number of flips increases?
	</p>
	<p>
	Question 2: Why do you rarely (if ever) get exactly the same number of Heads and Tails? Would you not "expect"
	that to happen?
</p>
	



<p>You should have noticed that as the number of flips increases, the relative frequency of Heads (and Tails)
	stabilized around 0.5. This makes sense intuitively since there are two options for each 
	individual flip and 1/2 of those options are Heads while the other 1/2 is Tails.</p>
	<p>
	Let's try again
	by doing a random experiment consisting of rolling a single die one time. Note that the sample space 
	in this case will be the outcomes S = { 1, 2, 3, 4, 5, 6 \.
</p>

<p>
<sage>
<input>
@interact
def _(num_rolls = slider([20..5000],label='Number of rolls'),Number_of_Sides = [4,6,8,12,20]):
	die = list((1..Number_of_Sides))
	rolls = [choice(die) for roll in range(num_rolls)]
	show(rolls)   

	freq = [rolls.count(outcome) for outcome in set(die)]  # count the numbers for each outcome
	print('The frequencies of each outcome is %s'%str(freq))

	print('The relative frequencies of each outcome:')
	rel_freq = [freq[outcome-1]/num_rolls for outcome in set(die)]  # make frequencies relative
	print(rel_freq)
	fs = []
	for f in rel_freq:
		fs.append(f.n(digits=4))
	print(fs)
	show(bar_chart(freq,axes=False,ymin=0,xmin=-1/2))
</input>
</sage>
</p>

<p>
Notice for a single die there are a larger number of options (for example 6 on a regular die) but once again the relative frequencies of each  outcome was close to 1/n (i.e. 1/6 for the regular die) as the number of rolls increased.</p>

<p>
In general, this suggests a rule: if there are n outcomes and each one has the same
	chance of occurring on a given trial then on average on a large number of trials the relative
	frequency of that outcome is 1/n.
	In general, if a number of outcomes are "equally likely" then this is a good model for measuring
	the proportion of outcomes that would be expected to have any given outcome. However, it is not
	always true that outcomes are equally likely. Consider rolling two die and measuring their sum:
</p>

<p xml:id="TwoDiceSage">
<sage>
<title>Rolling Two Dice and Measuring their Sum</title>
<input>
@interact
def _(num_rolls = slider([20..5000],label='Number of rolls'),num_sides = slider(4,20,1,6,label='Number of sides')):
    die = list((1..num_sides))
    dice = list((2..num_sides*2))
    rolls = [(choice(die),choice(die)) for roll in range(num_rolls)]
    sums = [sum(rolls[roll]) for roll in range(num_rolls)]
    show(rolls)   

    freq = [sums.count(outcome) for outcome in set(dice)]  # count the numbers for each outcome
    print('The frequencies of each outcome is %s'%str(freq))
    
    print('The relative frequencies of each outcome:')
    rel_freq = [freq[outcome-2]/num_rolls for outcome in set(dice)]  # make frequencies relative
    print(rel_freq)
    show(bar_chart(freq,axes=False,ymin=0, xmin=-1))     #  A histogram of the results
    print("Relative Frequence of %s"%str(dice[0])+" is about %s"%str(rel_freq[0].n(digits=4)))
    print("Relative Frequence of %s"%str(dice[num_sides-1])+" is about %s"%str(rel_freq[num_sides-1].n(digits=4)))
</input>
</sage>
</p>


<!--
	<exercise>
		<introduction>
		<p>
		Let's see if you understand the relationship between frequency and relative frequency.
		</p>
		</introduction>
		<webwork source="local/relative_frequency1.pg">
		</webwork>
		<conclusion>
		<p>
		So, these are simple calculations.
		</p>
		</conclusion>
	</exercise>
-->


<p>
	Question 1: What do you notice as the number of rolls increases?
</p>

<p>
	Question 2: What do you expect for the relative frequencies and why are they not all exactly the same?
</p>

<p>Notice, not only are the answers not the same but they are not even close. To understand why this 
	is different from the examples before, consider the possible outcomes from each pair of die. Since we
	are measuring the sum of the dice then (for a pair of standard 6-sided dice) the possible sums are from 
	2 to 12. However, there is only one way to get a 2--namely from a (1,1) pair--while there are 6 ways to get
	a 7--namely from the pairs (1,6), (2,5), (3,4), (4,3), (5,2), and (6,1). So it might make some sense
	that the likelihood of getting a 7 is 6 times larger than that of getting a 2. Check to see if that
	is the case with your experiment above.
</p>
	

<p>Play with the following several times to investigate what you might expect to get when you repeatedly receive a "hand" of 5 standard playing cards. Can you imagine how you might possible enumerate the entire list of possible outcomes by hand? However, using this interactive cell, you can shuffle and deal 5-card hands over and over easily and then count the number of special poker outcomes. 
</p>

<p>
<sage>
<input>
var('A C D H J K Q S') 

suits = [S, D, C, H] 
values = [2, 3, 4, 5, 6, 7, 8, 9, 10, J, Q, K, A] 

full_deck = [(value, suit) for suit in suits for value in values]
@interact
def _(num_hands=slider[50..5000]):                  # Set up the number of hands to create
    hands= []                    # Start with a blank list. 
    for i in range(num_hands):   # This loops the following operation num_hands times. 
        deck = copy(full_deck)   # start over
        shuffle(deck)
        hands.append([deck.pop() for card in range(5)])
    freq_values = []
    one_pair = 0
    two_pair = 0
    three_kind = 0
    full_house = 0
    four_kind = 0
    for i in range(num_hands):
        hand = hands[i]
        hand_values = [hand[k][0] for k in range(5)]
        freq_values = [hand_values.count(value) for value in set(values)]
        freq_values.sort(reverse=True)
        if freq_values[0]==4:
            four_kind=four_kind+1
        if freq_values[0]==3:
            if freq_values[1]==2:
                full_house=full_house+1
            if freq_values[1]==1:
                three_kind=three_kind+1
        if freq_values[0]==2:
            if freq_values[1]==2:
                two_pair=two_pair+1
            if freq_values[1]==1:
                one_pair=one_pair+1
    print("       One Pair frequency = %s"%str(one_pair)+" with relative frequency %s"%str(one_pair/num_hands))
    print("       Two Pair frequency = %s"%str(two_pair)+" with relative frequency %s"%str(two_pair/num_hands))
    print("Three of a Kind frequency = %s"%str(three_kind)+" with relative frequency %s"%str(three_kind/num_hands))
    print("     Full House frequency = %s"%str(full_house)+" with relative frequency %s"%str(full_house/num_hands))
    print(" Four of a Kind frequency = %s"%str(four_kind)+" with relative frequency %s"%str(four_kind/num_hands))
</input>
</sage>
</p>

<p>
Sometimes you will find it useful to keep a running total of the relative frequencies. Such a cumulative approach is often called a distribution function.
</p>

<p>
<definition><title>Cumulative relative frequency</title> 
<statement>
<p>
For a collection of ordered events <m>x_1 \lt x_2 \lt ... \lt x_s</m> with corresponding frequencies <m>f_1, f_2, ..., f_s</m>, the cumulative relative frequency is the function
<me>F(x) = \sum_{x_k \le x} f_{x_k}</me>
</p>
</statement>
</definition>
</p>

<p>
Let's consider the cumulative relative frequency with the sum of dice example seen at the beginning of this chapter.

<p>
<sage>
<title>Two Dice Cumulative Relative Frequency</title>
<input>
@interact
def _(num_rolls = slider([20..5000],label='Number of rolls'),num_sides = slider(4,20,1,6,label='Number of sides')):
    die = list((1..num_sides))
    dice = list((2..num_sides*2))
    rolls = [(choice(die),choice(die)) for roll in range(num_rolls)]
    sums = [sum(rolls[roll]) for roll in range(num_rolls)]
    show(rolls)   

    freq = [sums.count(outcome) for outcome in set(dice)]  # count the numbers for each outcome
    n = len(freq)
    CF = freq
    for k in range(1,n):
        CF[k] = freq[k] + CF[k-1]
    
    print('The cumulative relative frequencies of each outcome:')
    Crel_freq = [CF[outcome-2]/num_rolls for outcome in set(dice)]  # make frequencies relative
    print(Crel_freq)        
    show(bar_chart(CF,axes=False,ymin=0, xmin= -1))     #  A histogram of the results
    print("Cumulative Relative Frequence of %s"%str(dice[0])+" is about %s"%str(Crel_freq[0].n(digits=4)))
    print("Cumulative Relative Frequence of %s"%str(dice[num_sides-1])+" is about %s"%str(Crel_freq[num_sides-1].n(digits=4)))
</input>
</sage>
</p>

</p>

</section>


<section xml:id="ProbabilityBasics"><title>Definition of Probability</title>

<p>	
<introduction>
<p>
Relative frequency gives a way to measure the proportion of "successful" outcomes when doing an experimental approach. From the interactive applications above, it appears that the relative frequency does jump around as the experiment is repeated but that the amount of variation decreases as the number of experiments increases. This is known to be true in general and is known as the "Law of Large Numbers". 
</p>
<p>
We would like to formalize what these relative frequencies are approaching and will call this theoretical limit the "probability" of the outcome. In doing so, we will do our best to model our definition so that it follow the behavior of relative frequency.
</p>
</introduction>
</p>

<p>
To generate a general definition for probability, we need to know what is is that we measuring. In general, we will be finding the probability of sets of possible outcomes...that is, a subset of the Sample Space S. Toward that end, it is important to briefly look at some properties of sets.
</p>

<p>
<definition xml:id="DefnMutuallyExclusive"><title>Pairwise Disjoint Sets</title>
	<statement>
	<p>
	<m> \{ A_1, A_2, ... , A_n \}</m> are pairwise disjoint provided <m>A_k \cap A_j = \emptyset</m> so long as <m>k \ne j</m>.
	Disjoint sets as also often called mutually exclusive.
	</p>
	</statement>
	</definition>
</p>

<p>Play around with the interactive cell below by adding and removing items in each of the three sets. Find elements so that the intersection of all three sets is empty but at least one of the paired sets are not disjoint.  See if you can make all of the paired sets not disjoint but the intersection of all three disjoint. This is why we need to consider "pairwise" disjoint sets.
</p>	

<p>
<sage><title>Playing around with intersections and unions of sets</title>
<input>
def f(s, braces=True): 
    t = ', '.join(sorted(list(s)))
    if braces: return '{' + t + '}'
    return t
def g(s): return set(str(s).replace(',',' ').split())

@interact
def _(X='1,2,3', Y='2,a,3,4,apple', Z='a,b,10,apple'):
    S = [g(X), g(Y), g(Z)]
    X,Y,Z = S
    XY = X &amp; Y
    XZ = X &amp; Z
    YZ = Y &amp; Z
    XYZ = XY &amp; Z

    Txy = " - NOT disjoint "
    if Set(XY).is_empty():
        Txy = ' - disjoint '
    pretty_print(html("$X \cap Y$ = %s"%f(XY)+"%s"%Txy))
    Txz = " - NOT disjoint "
    if Set(XZ).is_empty():
        Txz = ' - disjoint '
    pretty_print(html("$X \cap Z$ = %s"%f(XZ)+"%s"%Txz))
    Tyz = " - NOT disjoint "
    if Set(YZ).is_empty():
        Tyz = ' - disjoint ' 
    pretty_print(html("$Y \cap Z$ = %s"%f(YZ)+"%s"%Tyz))
    Txyz = " - NOT disjoint "
    if Set(XYZ).is_empty():
        Txyz = ' - disjoint ' 
    pretty_print(html("$X \cap Y \cap Z$ = %s"%f(XYZ)+"%s"%Txyz))
    centers = [(cos(n*2*pi/3), sin(n*2*pi/3)) for n in [0,1,2]]
    scale = 1.7
    clr = ['yellow', 'blue', 'green']
    G = Graphics()
    for i in range(len(S)):
        G += circle(centers[i], scale, rgbcolor=clr[i], 
             fill=True, alpha=0.3)
    for i in range(len(S)):
        G += circle(centers[i], scale, rgbcolor='black')

    # Plot what is in one but neither other
    for i in range(len(S)):
        Z = set(S[i])
        for j in range(1,len(S)):
            Z = Z.difference(S[(i+j)%3])
        G += text(f(Z,braces=False), (1.5*centers[i][0],1.7*centers[i][1]), rgbcolor='black')


    # Plot pairs of intersections
    for i in range(len(S)):
        Z = (set(S[i]) &amp; S[(i+1)%3]) - set(XYZ)
        C = (1.3*cos(i*2*pi/3 + pi/3), 1.3*sin(i*2*pi/3 + pi/3))
        G += text(f(Z,braces=False), C, rgbcolor='black')

    # Plot intersection of all three
    G += text(f(XYZ,braces=False), (0,0), rgbcolor='black')

    # Show it
    G.show(aspect_ratio=1, axes=False)
</input>
</sage>
</p>	

<p>
Consider how we might create a definition for the expectation of a given outcome. To do so, first consider a desired collection of outcomes A. If each outcome in A is chosen randomly then we might consider using a formula similar to relative frequency and set a measure of expectation to be |A|/|S|. For example, on a standard 6-sided die, the expectation of the outcome A={2} from the collection S = {1,2,3,4,5,6} could be
	|A|/|S| = 1/6.
</p>

<p>From our example where we take the sum of two die, the outcome A = { 4,5 } from the
	collection S = {2,3,4,...,12} would be
	<md>
		<mrow>|A| = | \{ (1,3),(2,2),(3,1),(1,4),(2,3),(3,2),(4,1) \}| = 7</mrow>
		<mrow>|S| = | \{ (1,1),...,(1,6),(2,1),...,(2,6),...,(6,1),...,(6,6) \}| = 36</mrow>
	</md>
	and so the expected relative frequency would be |A|/|S| = 7/36. Compare this theoretical value
	with the sum of the two outcomes from your experiment above.
</p>
			
<p>We are ready to now formally give a name to the theoretical measure of expectation for
	outcomes from an experiment. Taking our cue from our examples, let's 
	make our definition agree with the following relative frequency properties:
<ol>
		<li>Relative frequency cannot be negative, since cardinality cannot be negative</li>
		<li>Relative frequencies for disjoint events should sum to one</li>
		<li>Relative frequencies for collections of disjoint outcomes should equal the sum of the
	individual relative frequencies</li>
</ol>
</p>
	
<p>which leads us to the following formal definition...</p>

<p>
<definition xml:id="DefnProb"><title>Probability</title>
	<statement>
	<p>
	The probability P(A) of a given outcome A is a set function that satisfies:
	</p>
		<p>
		<ol>
			<li>(Nonnegativity) P(A) <m>\ge 0</m></li>
			<li>(Totality) P(S) = 1</li>
			<li>(Subadditivity) If A <m>\cap</m> B = <m>\emptyset</m>, then P(A <m>\cup</m> B) = P(A) + P(B).  
			In general, if {<m>A_k</m>} are pairwise disjoint then <m>P( \cup_k A_k) = \sum_k P(A_k)</m>.</li>
		</ol>
		</p>
		</statement>
	</definition>	
</p>	

<p>
	<exercise>
		<introduction>
		<p>
		Using the definition above, determine the following probabilities.
		</p>
		</introduction>
		<webwork source="Library/MC/PreAlgebra/setPreAlgebraC06S04/BasicProbability01.pg">
		</webwork>
		<conclusion>
		<p>
		Notice when you are given complete information regarding the entire data set then determining
		probabilities for events can be relatively easy to compute.
		</p>
		</conclusion>
	</exercise>
</p>


<p>
Based upon this definition we can immediately establish a number of results.
</p>

<p>
	<theorem xml:id="ProbabilityComplements"><title>Probability of Complements</title>
		<statement> For any event A, <m>P(A) + P(A^c) = 1</m>
		</statement>
		<proof>
			<p>Let A be any event and note that 
			<me>A \cap A^c = \emptyset.</me>  
			But <m>A \cup A^c = S</m>.
			So, by subadditivity 
			<me>1 = P(S) = P(A \cup A^c) = P(A) + P(A^c)</me> 
			as desired.</p>
		</proof>
	</theorem>
</p>

<p>
	<theorem xml:id="ProbabilityEmptySet">
		<statement>
		<p>
		<m>P(\emptyset) = 0</m>
		</p>
		</statement>
		<proof>
			<p>Note that <m>\emptyset^c = S</m>. So, by the theorem above, 
			<me>1 = P(S) + P(\emptyset) \Rightarrow 1 = 1 + P(\emptyset).</me>
			Cancelling the 1 on both sides gives <m>P(\emptyset) = 0</m>. </p>
		</proof>
	</theorem>
</p>

<p>
	<theorem xml:id="ProbabilityContainment">
		<statement>For events A and B with <m> A \subset B, P(A) \le P(B)</m>.
		</statement>
		<proof>
			<p>Assume sets A and B satisfy <m> A \subset B</m>. Then, notice that
			<me>A \cap (B-A) = \emptyset</me> 
			and  
			<me>B = A \cup (B-A).</me> 
			Therefore, by subadditivity and nonnegativity
			<md>
				<mrow>0 \le P(B-A)</mrow>
				<mrow>P(A) \le P(A) + P(B-A) </mrow>
				<mrow>P(A) \le P(B)</mrow>
			</md>
			</p>
		</proof>
	</theorem>
</p>

<p>
	<theorem xml:id="ProbabilityLessThanOne">
		<statement>For any event A, <m>P(A) \le 1</m> 
		</statement>
		<proof>
			<p>Notice <m>A \subset S</m>. By the theorem above <m> P(A) \le P(S) = 1</m></p>
		</proof>
	</theorem>
</p>

<p>
	<theorem xml:id="ProbabilityTwoUnions">
		<statement>For any sets A and B, <m>P(A \cup B) = P(A) + P(B) - P(A \cap B)</m>
		</statement>
		<proof>
			<p>Notice that we can write <m>A \cup B</m> as the disjoint union
			<me>A \cup B = (A-B) \cup (A \cap B) \cup (B-A).</me>
			We can also write disjointly
			<md>
				<mrow>A = (A-B) \cup (A \cap B)</mrow>
				<mrow>B = (A \cap B) \cup (B-A)</mrow>
			</md>
			Hence,
			<md>
				<mrow>P(A) &amp; + P(B) - P(A \cap B) </mrow>
				<mrow>&amp; = [P(A-B) + P(A \cap B)] </mrow>
				<mrow>&amp; + [P(A \cap B) + P(B-A)] - P(A \cap B)</mrow>
				<mrow>&amp; = P(A-B) + P(A \cap B) + P(B-A)</mrow>
				<mrow>&amp; = P(A \cup B)</mrow>
			</md>
			</p>
		</proof>
	</theorem>
</p>

<p>
This result can be extended to more that two sets using a property known as inclusion-exclusion. The following two theorems illustrate this property and are presented without proof.
</p>

<p>
<corollary xml:id="ProbabilityThreeUnions">
		<statement>
		<p>
			For any sets A, B and C, 
			<md>
				<mrow>P(A \cup B \cup C) &amp; = P(A) + P(B) + P(C)</mrow> 
				<mrow>&amp; - P(A \cap B) - P(A \cap C) - P(B \cap C) </mrow>
				<mrow>&amp; + P(A \cap B \cap C)</mrow>
			</md>
		</p>
		</statement>
	</corollary>
</p>

<p>
	<corollary xml:id="ProbabilityFourUnions">
		<statement>
		<p>
			For any sets A, B, C and D, 
			<md>
				<mrow>P(A \cup B \cup C \cup D) &amp; = P(A) + P(B) + P(C) + P(D)</mrow>
				<mrow>&amp; - P(A \cap B) - P(A \cap C) - P(A \cap D) </mrow>
				<mrow>&amp; - P(B \cap C) - P(B \cap D) - P(C \cap D)</mrow>
				<mrow>&amp; + P(A \cap B \cap C) + P(A \cap B \cap D) </mrow>
				<mrow>&amp; + P(A \cap C \cap D) + P(B \cap C \cap D)</mrow>
				<mrow>&amp; - P(A \cap B \cap C \cap D)</mrow>
			</md>
		</p>
		</statement>
	</corollary>
</p>
	
<p>
Many times, you will be dealing with making selections from a sample space where each item in the space has an equal chance of being selected. This may happen (for example) when items in the sample space are of equal size or when selecting a card from a completely shuffled deck or when coins are flipped or when a normal fair die is rolled. 
</p>

<p>
It is important to notice that not all outcomes are equally likely--even in times when there are only two of them. Indeed, it is generally not an equally likely situation when picking the winner of a football game which pits, say, the New Orleans Saints professional football team with the New Orleans Home School Saints. Even though there are only two options the probability of the professional team winning in most years ought to be much greater than the chances that the high school will prevail. 
</p>

<p>
When items are equally likely (sometimes also called "randomly selected") then each individual event has the same chance of being selected as any other. In this instance, determining the probability of a collection of outcomes is relatively simple.
</p>

<p>
<theorem><title>Probability of Equally Likely Events</title>
	<statement>
	<p>
	If outcomes in S are equally likely, then for <m>A \subset S,</m> 
	<me>P(A) = \frac{|A|}{|S|}.</me> 
	</p>
	</statement>
	<proof>
	<p>
	Enumerate S = {<m>x_1, x_2, ..., x_{|S|}</m>} and note <m>P( \{ x_k \} ) = c</m> for some constant c since each item is equally likely. However, using each outcome as a disjoint event and the definition of probability, 
	<md>
		<mrow>1 = P(S) &amp; = P( \{ x_1 \} \cup \{x_2 \} \cup ... \cup \{x_{|S|} \} )</mrow>
		<mrow> &amp; = P(\{ x_1 \}) + P(\{ x_2 \} ) + ... + P(\{ x_{|S|} \} )</mrow>
		<mrow> &amp; = c + c + ... + c = {|S|} \times c</mrow>
	</md>
	and so <m>c = \frac{1}{{|S|}}</m>. Therefore, <m>P( \{ x_k \} ) = \frac{1}{|S|}</m> .
	</p>
	<p>
	Hence, with A = {<m>a_1, a_2, ..., a_{|A|}</m>}, breaking up the disjoint probabilities as above gives
	<md>
		<mrow>P(A) &amp; = P( \{ a_1 \} \cup \{ a_2 \} \cup ... \cup \{ a_{|A|} \} )</mrow>
		<mrow> &amp; = P(\{ a_1 \}) + P(\{ a_2 \} ) + ... + P(\{ a_{|A|} \} )</mrow>
		<mrow> &amp; = \frac{1}{{|S|}} + \frac{1}{{|S|}} + ... + \frac{1}{{|S|}}</mrow>
		<mrow> &amp; = \frac{|A|}{{|S|}}</mrow>
	</md>
	as desired.
	</p>
	</proof>
</theorem>
</p>

<p>
<sage><title>Let's play around with cards</title>
<input>
var('A C D H J K Q S') 

def L(str):
    n = len(str)
    m = int(n/5)
    top = m+1
    if m == n/5:
        top = m
    for k in range(top):
        print str[5*k:5*k+5]
        
suits = [S, D, C, H] 
values = [2, 3, 4, 5, 6, 7, 8, 9, 10, J, Q, K, A] 

deck = [(value, suit) for suit in suits for value in values]
full_deck = copy(deck)  # to save a copy of the original deck for later use.

L(deck)
shuffle(deck)
L(deck)
deck1 = copy(full_deck)
shuffle(deck1)

@interact
def _(auto_update=False):
    global deck1    
    shuffle(deck1)            
    if (Set(deck1).cardinality()&lt;5):
        print 'Deck is too small...getting a new deck'
        deck1 = copy(full_deck)
    else:
        hand = [deck1.pop() for card in range(5)] 
        print "The cards dealt:"
        L(hand)
        print
        print " The remaining cards in the deck:"
        L(deck1)
        print
        print(html("\n The number of remaining cards in the deck = %s"%str(Set(deck1).cardinality())))</input>
</sage>
</p>

<p>	
	<exercise><title>WebWork</title>
		<introduction>
		<p>
		Let's see if you understand the relationship between frequency and relative frequency. In this exercise, presume "Probabiity" to be the expected fraction of outcomes you might logically expect.
		</p>
		</introduction>
		<webwork source="Library/MC/PreAlgebra/setPreAlgebraC06S04/BasicProbability03.pg">
		</webwork>
		<conclusion>
		<p>
		So, by counting actual "equally likely" outcomes these probabilities are easy to compute.
		</p>
		</conclusion>
	</exercise>
</p>

<p>
	<exercise><title>WebWork</title>
		<introduction>
		<p>This one is a little harder and uses the binomial coefficients from Combinatorics.
		</p>
		</introduction>
		<webwork source="Library/Rochester/setProbability5RandomSample/ur_pb_5_1a.pg">
		</webwork>
		<conclusion>
		<p>
		Notice how the probabilities look similar to relative frequencies. It's just the case 
		that you are counting ALL of the individual simple possibilities that lead to a success.
		</p>
		</conclusion>
	</exercise>
</p>

</section>


<section xml:id="ProbabilityBasicsExercises"><title>Exercises</title>

<p>
<exercise>
	<title>Poker</title>
	<statement><p>
	Determine the probabilities associated with the various 5-card hands. That is
	<ol>
		<li>P(one pair)</li>
		<li>P(two pair)</li>
		<li>P(three of a kind)</li>
		<li>P(full house)</li>
		<li>P(four of a kind</li>
		<li>P(straight)</li>
		<li>P(flush)</li>
		<li>P(royal flush)</li>
	</ol>
	</p>
	</statement>
</exercise>
</p>

<p>
<exercise><title>Dice</title>
	<statement>
	<p>
	Determine the 36 possible outcomes related to the rolling a pair of fair dice. Justify why each of these outcomes is equally likely. Determine the probabilities associated with each possible sum.
	</p>
	</statement>
	<solution>
	<p>
	Remember, when using equally likely outcomes |A|/|S| assumes that the items counted for A are also in the sample space S.  In this case, for example, to determine the Probability of getting a sum of (say) 4 includes the rolls (1,3), (2,2), and (3,1). These three "successes" from the 36 possible ordered pairs gives P(4) = 3/36.  Similarly, 
	<p>P(5) = |dice rolls with a sum of 5|/36 = |(1,4), (2,3), (3,2), (4,1)| / 36 = 4/36.</p>
Continue in this manner to determine the other possibilities and then compare to the experimental sage cell seen earlier for <xref ref="TwoDiceSage"> the sum of two dice </xref>.
	</p>
	</solution>
</exercise>
</p>

<p>
<exercise><title>Skew Dice</title>
	<statement>
	<p>
	Suppose you have one die which only has three possible sides labeled 1, 2, or 3. Suppose a second die has twelve equally likely sides with labels 1,2,3,4,4,5,5,6,6,7,8,9.  Justify that the probabilities associate with each possible sum is the same as the probabilities when using two normal 6-sided dice.
	</p>
	</statement>
	<solution>
	<p>
	Consider the outcome space 
	<me>S = {(1,1), (1,2), (1,3), (1,4), (1,4), (1,5), (1, 5), \\
     (1,6), (1,6), (1,7), (1,8), (1,9), (2,1) ... (3,9)}</me>
	Then P(5) = |(1,4), (1,4), (2,3), (3,2) |/36 = 4/36. Compare this to the exercise with regular dice performed above.  Similarly, compute the remaining probabilities.
	</p>
	</solution>
</exercise>
</p>

<p>
<exercise><title>Craps</title>
	<statement><p>
	Analyze the dice game known as "craps": Roll a pair of dice and consider the sum. If that sum is 7 or 11, the one who rolls wins and can roll again. If the sum is 2, 3, or 12 -- known as craps -- the one who rolls loses but keeps the dice. For any other outcome (called the "point"), the one who rolls continues hoping to roll the point value again before rolling a 7. If successful, then the roller wins and starts the game anew. If a 7 appears first, the roller loses and the next person gets to be the roller.
</p>
<p>
So, a win can be obtained in two ways: 7 or 11 on first roll or getting the point before the 7 thereafter. Therefore, determine the probability of a win and the probability of a loss.
</p>
</statement>
<solution>
    <p>
	<url href="http://mathworld.wolfram.com/Craps.html">"craps"</url>.
	</p>
</solution>
</exercise>
</p>

</section>

<section xml:id="ConditionalProbability">
<title>Conditional Probability</title>
	<introduction>
	<p>When finding the probability of an event, sometimes you may need to consider past history and how it might affect things. Indeed, you might think that when the local station forecasts rain then the probability of it actually raining should be greater than if they forecast fair skies. At least that is the hope. :)  In this section, you will develop a way to deal with the probability of some event that might change dependent upon the occurence or not of some other event.
	</p>
	</introduction>

<p>Indeed, consider what happens when you keep on dealing a hand of five cards from a shuffled deck but without replacement.  Notice how the probability of the same thing (such as P(getting a Heart on the next card)) oscillates based upon what cards came out of the deck on previous hands.
</p>

<p>
<sage>
<input>
print("Conditional Events - successively deal 5 cards w/o replacement" )

var('Ace Clubs Diamonds Hearts Jack King Queen Spades') 

suits = [Spades, Diamonds, Clubs, Hearts] 
values = [2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King, Ace] 

deck = [(value, suit) for suit in suits for value in values]
full_deck = copy(deck)  # to save a copy of the original deck for later use.

deck1 = copy(full_deck)
history1=[]
@interact
def _(choice=['Hearts','Spades','Diamonds','Clubs','New Deck'],again=['Repeat Same Suit']):
    global deck1, history1
    shuffle(deck1)
    if choice=='Hearts':
        suit = Hearts
    elif choice=='Spades':
        suit = Spades
    elif choice=='Diamonds':
        suit = Diamonds
    elif choice=='Clubs':
        suit = Clubs
    else:
        deck1 = copy(full_deck)
        shuffle(deck1)
        history1=[]
    if (Set(deck1).cardinality()&lt;5):
        print("Deck is too small...get a new deck")
    elif choice!='New Deck':
        hand = [deck1.pop() for card in range(5)] 
        print("Click on a desired suit above to deal out another 5 card hand.  The cards dealt:")
        print(hand)
        print("The remaining cards in the deck:")
        print(deck1)
        num = Set(deck1).cardinality()
        print("\nThe number of remaining cards in the deck = %s"%str(num))
        looking = []
        for card in deck1:
            if card[1]==suit:
                looking.append(card)
        prob = float(Set(looking).cardinality())/num
        history1.append(prob)
        
        print('So, the remaining probability of getting a card from '+choice+' from the remaining cards is %s'%str(prob))
    list_plot(history1).show(xmin=0,xmax=9,ymin=0,ymax=1,figsize=(5,2))</input>
</sage>
</p>
	
<p>Now, consider the case when you put the cards back in, reshuffle, and then get 5 new cards...</p>

<p>
<sage>
<input>
print('Independent Events - Successively deal 5 cards but WITH replacement')

var('Ace Clubs Diamonds Hearts Jack King Queen Spades') 

suits = [Spades, Diamonds, Clubs, Hearts] 
values = [2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King, Ace] 

deck = [(value, suit) for suit in suits for value in values]
full_deck = copy(deck)  # to save a copy of the original deck for later use.
deck1 = copy(full_deck)

h2=[]
@interact
def _(choice=['Heart','Spade','Diamond','Club'],again=['Repeat Same Suit']):

    if choice=='Hearts':
        suit = Hearts
    elif choice=='Spades':
        suit = Spades
    elif choice=='Diamonds':
        suit = Diamonds
    else:
        suit = Clubs

    deck1 = copy(full_deck)
    shuffle(deck1) 
    hand = [deck1.pop() for card in range(5)] 
    print("The cards dealt:")
    print(hand)
    print("Replacing this hand and reshuffling gives the remaining cards in the deck:")
    deck1 = copy(full_deck)
    shuffle(deck1)
    print(deck1)
    
    num = Set(deck1).cardinality()
    print("\nThe number of remaining cards in the deck = %s"%str(num))
    looking = []
    for card in deck1:
        if card[1]==suit:
            looking.append(card)
    prob = float(Set(looking).cardinality())/num
    h2.append(prob)
    
    print('So, the remaining probability of getting a '+choice+' from the remaining cards is %s'%str(prob))
    list_plot(h2).show(xmin=0,xmax=15,ymin=0,ymax=1,figsize=(5,2))
    
    print('Independent Events - Successively deal 5 cards but WITH replacement')</input>
</sage>
</p>

<p>
<example><title>Changing Sample Space - Balls</title>
<p>  
Consider a box with three balls: one Red, one White, and one Blue.  Using an equally likely assumption, the probability of randomly pulling out a Red ball should be 1/3.  That is P(Red) = 1/3.  
</p>
<p>However, suppose that for a first trial you pull out the White ball and set it aside. Attempting to pull out another ball leaves you with only two options and so the probability of randomly pulling out a Red ball is 1/2. Notice that the probability changed for the second trial dependent on the outcome of the first trial.
</p>
</example>
</p>

<p>
<example><title>Changing Sample Space - Cards</title>
<p>
Consider a deck of 52 standard playing cards and a success occurs when a Heart is selected from the deck. When extracting one card randomly, the probability	of that card being a Heart is P(Heart) = 13/52. 
</p>
<p>Now, assume that one card has already been extracted and set aside.  Next, prepare to extract another. If the first card drawn was a Heart, then there are only 12 Hearts left for the second draw. However, if the first card drawn was not a Heart, then there are 13 Hearts available for the second draw. To compute this probability correctly, one need to formulate the question so that subadditivity can be utilized.
</p>
<p>
Let <m>H_1</m> be the outcome Heart on 1st draw and <m>H_2</m> be the outcome Heart on 2nd draw. Then,
<md>
	<mrow>P(\text{Heart on 2nd draw}) &amp; = P( [ H_1 \cap H_2 ] \cup [ H_1^c \cap H_2 ] )</mrow>
	<mrow> &amp; = P( H_1 \cap H_2 ) + P( H_1^c \cap H_2 )</mrow>
	<mrow> &amp; = \frac{ | H_1 \cap H_2 |}{| P( \text{Number of ways to get two cards} | }</mrow>
	<mrow> &amp; + \frac{ | H_1^c \cap H_2 | }{ | \text{Number of ways to get two cards} | }</mrow>
	<mrow> &amp; = \frac{13}{52} \cdot \frac{12}{51} + \frac{39}{52} \cdot \frac{13}{51} = \frac{12}{4 \cdot 51} + \frac{3 \cdot 13}{4 \cdot 51}	</mrow>  
</md>
</p>
</example>
</p>
	
<p>
<definition xml:id="DefnConditionalProbability">
		<title>Conditional Probability</title>
		<statement>
		<p>For sets A and B,
			<me>P(B | A) = \frac{P(A \cap B)}{ P(A) },</me>
			 provided <m>P(A) \gt 0</m>.
		 </p>
		 </statement>
</definition>
</p>

<p>
You can read <m>P(B|A)</m> as "the probability of B given A".
</p>

<p>
<theorem>
	<statement>
	<p>
	<xref ref="DefnConditionalProbability">Conditional Probability</xref> satisfies all of the requirements of regular probability.
	</p>
	</statement>
	<proof>
		<p>
		By definition, for any event probability must be nonnegative. Therefore
		<me>P(A \cap B) \ge 0.</me> 
		So,
		<me>P(B | A) = \frac{\text{positive or zero}}{\text{positive}}\ge 0.</me>
		</p>
		<p>
		Further, 
		<me>P (S | A) = P(A \cap S)/P(A) = P(A)/P(A) = 1.</me>
		For the third part, we will only consider the case when there are two disjoint sets B and C.  Then,
		<md>
   			<mrow>P(B \cup C | A) &amp; = \frac{P(A \cap (B \cup C)}{P(A)} </mrow>
			<mrow> &amp; = \frac{P( (A \cap B) \cup (A \cap C) )}{P(A)}</mrow>
			<mrow> &amp; = \frac{P(A \cap B)}{P(A)} + \frac{P(A \cap C)}{P(A)}</mrow>
			<mrow> &amp; = P(B | A) + P(C | A).</mrow>
		</md>
		</p>
	</proof>
</theorem>
</p>

<p>	
<theorem xml:id="MultiplicationRule"><title>Multiplication Rule</title>
	<statement>
	<p> For any sets A and B,
		<me>P(A \cap B) = P(A) P(B | A) = P(B) P(A | B)</me>
	</p>
	</statement>
	<proof>
		<p>
		If <m>P(A)=0</m> or <m>P(B)=0</m>, then the result is trivial. Otherwise, unravel the definition of <xref ref="DefnConditionalProbability">conditional probability</xref> by taking the denominator to the other side. Also note that you can write <m>A \cap B = B \cap A</m>.
		</p>
	</proof>
</theorem>
</p>
	

<p>
<exercise><title>WebWork</title>
	<introduction>
		<p>Conditional Probability sometimes makes you have to think carefully about the ways to get 
		the desired outcome.
		</p>
		</introduction>
		<webwork source="Library/UMN/algebraKaufmannSchwitters/ks_15_5_42.pg">
		</webwork>
		<conclusion>
		<p>
		See how you had to break the given question up into two disjoint pieces.
		</p>
		</conclusion>
</exercise>	
</p>
</section>
	
	
<section xml:id="Bayes"><title>Bayes' Theorem</title>	
	<introduction>
	<p><xref ref="DefnConditionalProbability">Conditional probabilities</xref> can be computed using the methods developed above if the appropriate information is available. Some times you will however have some information available, such as <m>P(A | B)</m> but need <m>P(B | A)</m>. The ability to "play around with history" by switching what has been presumed to occur leads to an important result known as Bayes' Theorem.
	</p>
	</introduction>

	<theorem xml:id="BayesTheorem">
	<title>Bayes' Theorem</title>
	<statement>
	<p>
	Let <m>S = \{ S_1, S_2, ... , S_m \}</m> where the <m>S_k</m> are pairwise disjoint and <m>S_1 \cup S_2 \cup ... \cup S_m = S</m> (i.e. a partition of the space S).  Then for any <m>A \subset S</m>
	
	<me>P(S_j | A) = \frac{P(S_j)P(A | S_j)}{\sum_{k=1}^m P(S_k)P(A | S_k)}.</me>
	
	The conditional probability <m>P(S_j | A)</m> is called the posterior probability of <m>S_k</m>.
	</p>
	</statement>
	<proof>
	<p>
	Notice, by the definition of <xref ref="DefnConditionalProbability">conditional probability</xref> and the <xref ref="MultiplicationRule">multiplication rule</xref>
	<me>P(S_j | A) = \frac{P(S_j \cap A)}{P(A)} = \frac{P(S_j)P( A | S_j)}{P(A)}.</me>
	But using the disjointness of the partition 
	<md>
	<mrow>P(A) &amp; = P( (A \cap S_1) \cup (A \cup S_2) \cup ... \cup (A \cup S_m) )</mrow>
	<mrow>    &amp; = P(A \cap S_1) + P(A \cup S_2) + ... + P(A \cup S_m)</mrow>
	<mrow>    &amp; = P(S_1 \cap A) + P(S_2 \cup A) + ... + P(S_m \cup A)</mrow>
	<mrow>    &amp; = P(S_1) P(A | S_1) + P(S_2)P(A | S_2) + ... + P(S_m)P(A | S_m)</mrow>
	<mrow>    &amp; = \sum_{k=1}^m P(S_k)P(A | S_k)</mrow>
	</md>
	Put these two expansions together to obtain the desired result.
	</p>
	</proof>
	</theorem>
	
	<p>
	To illustrate this result, from the web site <url href="http://stattrek.com/probability/bayes-theorem.aspx"></url> consider the following problem:
	</p>
	<p>
	Marie is getting married tomorrow, at an outdoor ceremony in the desert. In recent years, it has rained only 5 days each year. Unfortunately, the weatherman has predicted rain for tomorrow. When it actually rains, the weatherman correctly forecasts rain 90% of the time. When it doesn't rain, he incorrectly forecasts rain 10% of the time. What is the probability that it will rain on the day of Marie's wedding?
	</p>
	<p>
	Notice, all days can be classified into one of two disjoint options:
	<ul>
		<li>Rainy, in which case we can deduce from the given info that P(Rain) = 5/365</li>
		<li>Not Rainy, and since this is the complement of above, P(Not Rain) = 360/365</li>
	</ul>
	In the notation of <xref ref="BayesTheorem">Bayes Theorem</xref>, let A represent a forecast of Rain and note you have 
	<me>P(\text{Rain}) = P(S_1) = \frac{5}{365}</me>
	 and 
	<me>P(\text{Not Rain}) = P(S_2) = \frac{360}{365}.</me> 
	Further, you are given the conditional probabilities
    <me>P(\text{ Forecast Rain | Rain}) = P( A | S_1) = 0.9</me>
    <me>P(\text{ Forecast Rain | Not Rain}) = P( A | S_2) = 0.1</me>
    
    Notice that the question provided requests that you find the probability of Rain given that the weatherman has forecasted rain. What is given on the other hand is the reverse of that conditional probability. Using Bayes' Theorem allows you to turn this around...
	<md>
		<mrow>P(\text{Rain}) &amp;  = P(S_1) P( A | S_1) + P(S_2) P(A | S_2)</mrow>	
		<mrow> &amp; = \frac{5}{365} \cdot 0.9 + \frac{360}{365} \cdot 0.1</mrow>
	</md>
	Hence, putting these together gives
	<md>
		<mrow>P(\text{Rain | Forecast Rain}) &amp; = \frac{\frac{5}{365} \cdot 0.9}{\frac{5}{365} \cdot 0.9 + \frac{360}{365} \cdot 0.1}</mrow>
		<mrow> &amp; = \frac{5 \cdot 0.9}{5 \cdot 0.9 + 360 \cdot 0.1}</mrow>
		<mrow> &amp; = \frac{45}{45+360} \approx 0.111</mrow>
	</md>
	So, normally there is only approximately a 1.369 percent chance of rain (5/365) on a given day but given that the weatherman has forecast rain, the chance of rain increases to a little more than 11 percent.
</p>	


<p>Here's a standard and yet surprising example:
</p>
<p>
Your neighbor has 2 children. You learn that he has a daughter Anna. What is the probability that Annaâ€™s sibling is a sister? Your first reaction might be that it is obviously 1/2 since (ok, roughly) it is equally likely that the other sibling was born female or male. But is that accurate?
</p>
<p>
If you were to list all of the possible female/male outcomes when having two children, then the sample space is S = {FF, FM, MF, MM} with FM meaning the first child is female and the second is male. Assuming again that girls and boys are equally likely to be born, these 4 outcomes have equal probability of <m>1/4</m>.
</p>
<p>
The question asks is whether the neighbor has another daughter in the set Fem2 = {FF} but since Anna is a girl then the possible outcomes now is only Fem = {FF, FM, MF}.
</p>
<p>
So, 
<md>
  <mrow>P(Fem2|Fem) &amp; = \frac{P(Fem2 \cap Fem)}{P(Fem)} </mrow>
  <mrow> &amp; = \frac{P(FF)}{P(FF or FM or MF)}</mrow>
  <mrow> &amp; = \frac{\frac{1}{4}}{\frac{3}{4}} = \frac{1}{3}</mrow>
</md>
</p>	

<p>
<xref ref="BayesTheorem">Bayes' Theorem</xref> also works well in analyzing "Let's Make A Deal" finale choice.
</p>

<p>
<exercise><title>WebWork</title>
	<introduction>
	<p>
	Let's try a Bayes' Theorem example...
	</p>
	</introduction>
	<webwork source="Library/Mizzou/Finite_Math/Probability_Bayes_Theorem/Bayes1.pg">
	</webwork>
	<conclusion>
	<p>
	You have to be careful to extract the conditional probabilities from the problem.
	</p>
	</conclusion>
</exercise>

<exercise><title>WebWork</title>
	<introduction>
	<p>
	Here is a more extensive Bayes' Theorem example...
	</p>
	</introduction>
	<webwork source="Library/UVA-Stat/setStat212-Homework04/stat212-HW04-16.pg">
	</webwork>
	<conclusion>
	<p>
	Notice that having the data expressed in tabular form sometimes makes it easier to deal with.
	</p>
	</conclusion>
</exercise>
</p>

<p>The interactive cell below can be used to easily compute all of the conditional probabilities associated with <xref ref="BayesTheorem">Bayes' Theorem</xref>. Notice how the relative size of the pie-shaped partition changes when you presume that an event in the space has already occurred.</p>

<p xml:id="BayesSage">
<sage>
<input>

#  This function is used to convert an input string into separate entries
def g(s): return str(s).replace(',',' ').replace('(',' ').replace(')',' ').split()

@interact
def _(Partition_Probabilities=input_box('0.35,0.25,0.40',label="$$ P(S_1),P(S_2),... $$"),
        Conditional_Probabilities=input_box('0.02,0.01,0.03',label='$$ P(A|S_1),P(A|S_2),... $$'),
        print_numbers=checkbox(True,label='Numerical Results on Graphs?'),
        auto_update=False):
            
    Partition_Probabilities = g(Partition_Probabilities)
    Conditional_Probabilities = g(Conditional_Probabilities)
    n = len(Partition_Probabilities)
    n0 = len(Conditional_Probabilities)
    
    # below needs to be n not equal to n0 but mathbook xml will not let me get the other
    if (n > n0):
        pretty_print("You must have the same number of partition probabilities and conditional probabilities.")
        
    else:                               # input data streams now are the same size!
        colors = rainbow(n)
        accum = float(0)                # to test whether partition probs sum to one
        ends = [0]                      # where the graphed partition sectors change in pie chart 
        mid = []                        # middle of each pie chart sector used for placement of text
        p_Sk_given_A = []               # P( S_k | A )
        pA = 0                          # P(A)
        PP=[]                           # array to hold the numerical Partition Probabilities 
        CP=[]                           # array to hold the numerical Conditional Probabilities     
        for k in range(n):
            PP.append(float(Partition_Probabilities[k]))
            CP.append(float(Conditional_Probabilities[k]))    
            p_Sk_given_A.append(PP[k]*CP[k] )
            pA += p_Sk_given_A[k]
            accum = accum + PP[k]
            ends.append(accum)
            mid.append((ends[k]+accum)/2)
#
#  Marching along from 0 to 1, saving angles for each partition sector boundary.
#  Later, we will multiple these by 2*pi to get actual sector boundary angles.
#
        if abs(accum-float(1))>0.0000001:     #  Due to roundoff issues, this should be close enough.                     
            pretty_print("Sum of probabilities should equal 1.")
        
        else:                           # probability data is sensible
 
#        
#  Draw the Venn diagram by drawing sectors from the angles determined above
#  First, create a circle of radius 1 to illustrate the the sample space S
#  Then draw each sector with varying colors and print out their names on the edge
#
            G = circle((0,0), 1, rgbcolor='black',fill=False, alpha=0.4,aspect_ratio=True,axes=False,thickness=5)
            for k in range(n):
                G += disk((0,0), 1, (ends[k]*2*pi, ends[k+1]*2*pi), color=colors[mod(k,10)],alpha = 0.2)
                G += text('$S_'+str(k+1)+'$',(1.1*cos(mid[k]*2*pi), 1.1*sin(mid[k]*2*pi)), rgbcolor='black')
                
            G += circle((0,0), 0.6, facecolor='yellow', fill = True, alpha = 0.1, thickness=5,edgecolor='black') 
    
#  Print the probabilities corresponding to each particular region as a list and on the graphs
            if print_numbers:               

                html("$P(A) = %s$"%(str(pA),))
                for k in range(n):
                    html("$P(S_{%s} | A)$"%(str(k+1))+"$ = %s$"%str(p_Sk_given_A[k]/pA))
                                        
                    G += text(str(p_Sk_given_A[k]),(0.4*cos(mid[k]*2*pi), 0.4*sin(mid[k]*2*pi)), rgbcolor='black')
                    G += text(str(PP[k] - p_Sk_given_A[k]),(0.8*cos(mid[k]*2*pi), 0.8*sin(mid[k]*2*pi)), rgbcolor='black')
        
#  This is essentially a repeat of some of the above code but focused only on creating the smaller inner circle dealing
#  with the set A so that the sectors now correspond in area to the Bayes Theorem probabilities


            accum = float(0)                        
            ends = [0]                     # where the graphed partition sectors change in pie chart 
            mid = []                       # middle of each pie chart sector used for placement of text
            for k in range(n): 
                accum += float(p_Sk_given_A[k]/pA) 
                ends.append(accum)
                mid.append((ends[k]+accum)/2)
            H = circle((0,0), 1, rgbcolor='black',fill=False, alpha=0,aspect_ratio=True,axes=False,thickness=0)
            H += circle((0,0), 0.6, facecolor='yellow',fill=True, alpha=0.1,aspect_ratio=True,axes=False,thickness=5,edgecolor='black')
            
            for k in range(n):
                H += disk((0,0), 0.6, (ends[k]*2*pi, ends[k+1]*2*pi), color=colors[mod(k,10)],alpha = 0.2)
                H += text('$S_'+str(k+1)+'|A$',(0.7*cos(mid[k]*2*pi), 0.7*sin(mid[k]*2*pi)), rgbcolor='black')
                    
        #  Now, print out the bayesian probabilities using the smaller set A only
    
            if print_numbers:
                for k in range(n):
                    H += text(str( N(p_Sk_given_A[k]/pA,digits=4) ),(0.4*cos(mid[k]*2*pi), 0.4*sin(mid[k]*2*pi)), rgbcolor='black')
                    
            G.show(title='Venn diagram of partition with A in middle')
            print
            H.show(title='Venn diagram presuming A has occured')
</input>
</sage>
</p>

<p>You can actually also use <xref ref="BayesTheorem">Bayes' Theorem</xref> to answer a easy question!  Indeed, suppose that you draw one card from a shuffled and standard 52 card deck.  Given that you know the card is an Ace, what is the probability that it is also a Heart.
</p>
<p>
Using the Bayes' approach, let's break up the world into Hearts (H) and non-Hearts (N).  Easily, 
<me>P(A|H) = 1/13</me>
<me>P(A|N) = 3/39</me>
and so by Bayes'
<me>P(H|A) = \frac{P(H) P(A|H)}{P(H) P(A|H) + P(N) P(A|N)} 
= \frac{\frac{13}{52} \cdot \frac{1}{13}}{\frac{13}{52} \cdot \frac{1}{13} + \frac{39}{52} \cdot \frac{3}{39}} = \frac{1}{4}</me>
as expected!
</p>



<p>
<exercise><title>Insured vs Accident</title>
	<statement>
	<p>
	Your automobile insurance company uses past history to determine how to set rates by measuring the number of accidents caused by clients in various age ranges. The following table summarizes the proportion of those insured and the corresponding probabilities by age range:

	<table halign="left">
		<caption>Age vs Accident Likelihood</caption>
      	<tabular halign="right">
      
<row><cell bottom="medium" right="medium">Age</cell><cell bottom="medium" right="medium">Proportion of Insured</cell><cell bottom="medium">Probability of Accident</cell></row>      
<row><cell right="medium">16-20</cell><cell right="medium">0.05</cell><cell>0.08</cell></row>
<row><cell right="medium">21-25</cell><cell right="medium">0.06</cell><cell>0.07</cell></row>
<row><cell right="medium">26-55</cell><cell right="medium">0.49</cell><cell>0.02</cell></row>
<row><cell right="medium">55-65</cell><cell right="medium">0.25</cell><cell>0.03</cell></row>
<row><cell right="medium">over 65</cell><cell right="medium">0.15</cell><cell>0.04</cell></row>
		</tabular>
	</table>
	
	One of your family friends insured by this company has an accident. 
	<ol>
		<li>Determine the conditional probability that the driver was in the 16-20 age range.</li>
		<li>Compare this to the probability that the driver was in the 18-20 age range. Discuss the difference.</li>
		<li>Determine how much more the company should charge for someone in the 16-20 age range compared to someone in the 26-55 age range.</li>
	</ol>
	</p>
	</statement>
	<solution>
	<p>
	  Plug the middle column into the first input box and the right column into the second input box of the  <xref ref="BayesSage">Bayes Sage Cell </xref>
	</p>
	</solution>
</exercise>
</p>

<p>	
<exercise><title>Spinal bifida odds</title>
	<statement>
	<p>
	Congratulations...your family is having a baby! As part of the prenatal care, some testing is part of the normal procedure including one for spinal bifida (which is a condition in which part of the spinal cord may be exposed.) Indeed, measurement of maternal serum AFP values is a standard tool used in obstetrical care to identify pregnancies that may have an increased risk for this disorder. You want to make plans for the new child's care and want to know how serious to take the test results. However, some times the test indicates that the child has the disorder when in actuality it does not (a false positive) and likewise may indicate that the child does not have the disorder when in fact it does (a false negative.) 
	</p>
	<p>The combined accuracy rate for the screen to detect the chromosomal abnormalities mentioned above is approximately 85% with a false positive rate of 5%. This means that (from <url href="http://americanpregnancy.org/prenatal-testing/first-trimester-screen/">americanpregnancy.org</url>)
	<ul>
		<li>Approximately 85 out of every 100 babies affected by the abnormalities addressed by the screen will be identified. (Positive Positive)</li>
		<li>Approximately 5% of all normal pregnancies will receive a positive result or an abnormal level. (False Positive)</li>
	</ul>
	<ol>
		<li>Given that your test came back negative, determine the likelihood that the child will actually have spinal bifida.</li>
		<li>Given that your test came back negative, determine the likelihood that the child will not have spina bifida</li>
		<li>Given that a positive test means you have a 1/100 to 1/300 chance of experiencing one of the abnormalities, determine the likelihood of spinal bifida in a randomly selected child.</li>
	</ol>
	You can get some help checking your arithmetic using the <xref ref="BayesSage">Bayes' Sage interact</xref>
	</p>
	</statement>
</exercise>
</p>

</section>



<section xml:id="Independence"><title>Independence</title>

	<introduction>
	<p>You have seen when repeatedly sampling without replacement leads to a change the the likelihood of some event in successive trials. Indeed, this is what conditional probabilities above illustrate. However, when sampling with replacement you may find a different situation arises. Indeed, you easily notice that when flipping a coin, P(Heads) = 1/2 regardless of the outcome of any previous flip.  In situations such as this where the probability of an event is not affected by the occurrence (or lack of occurrence) of some other event determining the probability of compound events can be greatly simplified.
	</p>
	</introduction>

<p>	
<definition xml:id="DefnIndependentEvents"><title>Independent Events</title>
		<statement>
		<p>Events A and B are independent provided 
		<me>P(A \cap B) = P(A) P(B)</me>
		</p>
		</statement>
</definition>
</p>

<p>	
<corollary><title>Independence and Conditional Probability</title>
	<statement>
	<p>Given <xref ref="DefnIndependentEvents">independent events</xref> A and B, 
			<me>P(B | A) = P(B)</me> and <me>P(A | B) = P(A).</me>
	</p>
	</statement>
	<proof>
			<p>By the multiplication rule and the definition of independence, for any events A and B
			<me>P(A) \cdot P(B) = P(A \cap B) = P(A) \cdot P(B | A) .</me>
			Therefore, if P(A) is non-zero, canceling yields the first result. Switching around notation provides the second.
			</p>
	</proof>
</corollary>
</p>



<!--
<p>
<exercise><title>WebWork</title>
	<introduction>
		<p>
		Independence makes combined probabilities VERY easy to compute.
		</p>
	</introduction>
	<webwork source="Library/Rochester/setProbability4Conditional/ur_pb_4_8.pg"></webwork>
	<conclusion>
		<p>
		Basically you just multiply individual probabilities together.  Independence is often
		assumed since it makes computations easier. That said, you should remember to consider
		each time whether independence should or should not be assumed.
		</p>
	</conclusion>
</exercise>
</p>
-->

<p>
<corollary xml:id="IndependenceVsDisjointness"><title>Independence and Mutual Exclusivity</title>
	<statement>
	<p>If events A and B are both <xref ref="DefnIndependentEvents">independent</xref> and <xref ref="DefnMutuallyExclusive">mutually exclusive</xref>, then at least one of them has zero probability.
	</p>
	</statement>
	<proof>
			<p>
			By independence, <m>P(A \cap B) = P(A) \cdot P(B)</m>. However, by mutually exclusivity, <m>A \cap B = \emptyset \Rightarrow P(A \cap B) = 0</m> gives
			<me>P(A) \cdot P(B) = 0.</me>
			Hence, one or the other (or both) must be zero.
			</p>
	</proof>
</corollary>
</p>

<p>
<corollary><title>Successive Independent Events</title>
	<statement>
	<p>Given a sequence of mutually independent events <m>A_1, A_2, A_3, ...</m>,
		<me>P(\cap_{k \in R} A_k) = \prod_{k \in R} P(A_k)</me>	
	</p>
	</statement>
</corollary>
</p>
</section>

<section xml:id="ProbabilityGeneralitiesSummary"><title>Summary</title>
<p>
TBA
</p>
</section>

<section xml:id="ProbabilityGeneralitiesExercises"><title>More Exercises</title>

<p>
<exercise><title>Conditional Basic computation</title>
	<statement><p>
	Given <m>P(A) = 0.43, P(B) = 0.72,</m> and <m>P(A \cap B) = 0.29</m>, determine
	<ol>
		<li><m>P(A \cup B)</m></li>
		<li><m>P(B | A)</m></li>
		<li><m>P(A | B)</m></li>
		<li><m>P(A^c \cap B^c)</m></li>
	</ol>
	</p>
	</statement>
	<solution>
	<p>Use the Theorems and Definitions provided to complete the first three.  For the fourth part, it might be useful to use DeMorgan's Laws from set theory to rewrite <m>A^c \cap B^c = (A \cup B)^c</m> and then use a theorem.
	</p>
	</solution>
</exercise>
</p>

<p>
<exercise><title>Gender vs University Major</title>
	<statement><p>
	The table below classifies students at your university according to gender and according to major.

	<table halign="left">
		<caption>Gender vs Major</caption>
      	<tabular halign="right">
      
<row><cell bottom="medium" right="medium">Enrollment</cell><cell bottom="medium">Male</cell><cell bottom="medium" right="medium">Female</cell><cell bottom="medium">Totals</cell></row>      
<row><cell right="medium">STEM</cell><cell>420</cell><cell right="medium">510</cell><cell>930</cell></row>
<row><cell right="medium">Business</cell><cell>320</cell><cell right="medium">270</cell><cell>590</cell></row>
<row><cell bottom="medium" right="medium">Other</cell><cell bottom="medium">610</cell><cell bottom="medium" right="medium">710</cell><cell bottom="medium">1320</cell></row>
<row><cell right="medium">Totals</cell><cell>1350</cell><cell right="medium">1490</cell><cell>2840</cell></row>
		</tabular>
	</table>

	Determine the following:
	<ol>
		<li><m>P( \text{STEM major} )</m></li>
		<li><m>P( \text{STEM | Female} )</m></li>
		<li><m>P( \text{Female | STEM} )</m></li>
		<li><m>P( \text{Female | Not STEM} )</m></li>
	</ol>
	</p>
	</statement>
</exercise>
</p>

<p>	
<exercise><title>Mean Tough Teacher</title>
	<statement>	
	<p>
	You are in a probability and statistics class with a teacher who has predetermined that only one student can make an A for the course. To be "fair", he places a number of slips of paper in a bowl equal to the number of students in the course with one of the slips having an A designation. Students in the course each can pick once randomly from the bowl and without replacement to see if they can get the lucky slip.  Determine the following:
	<ol>
		<li>If there are 15 students in your course, determine the probabilities of getting an A in the course if you pick first and if you pick last.</li>
		<li>Since the teacher likes you the most, she will give you the option of deciding whether to pick at any position. If so, determine the position that would give you the best likelihood of getting the A slop.</li>
		<li>Suppose again that the teacher was feeling more generous and decided instead to allow for two A's. Determine how that changes your likelihood of winning and on what position you would like to choose.</li>
		<li>Continue as above except that only one slip does not have an A on it.</li>
		<li>Discuss how your choice is affected by the number of students in the course or the number of A slips included.</li>
	</ol>
	</p>
	</statement>
	
	<solution>
	<p>
	Using the normal equally-likely definition, <m>P(\text{first}) = \frac{1}{15}</m>.
	</p>
	<p>
	To get the A on the last pick requires that all of the previous picks to be something else. You don't get the opportunity to pick the A if it has already been selected. So, if L stands for losing (not getting the A), then 
	<md>
	<mrow> P(\text{last}) &amp; = P(\text{LLLLLLLLLLLLLLA}) </mrow>
	<mrow> &amp; = \frac{14 \cdot 13 \cdot 12 \cdot 11 \cdot 10 \cdot 9  \cdot 8  \cdot 7  \cdot 6  \cdot 5  \cdot 4  \cdot 3 \cdot 2  \cdot 1}{15 \cdot 14 \cdot 13 \cdot 12 \cdot 11 \cdot 10 \cdot 9 \cdot 8 \cdot 7 \cdot 6 \cdot 5 \cdot 4 \cdot 3 \cdot 2} = \frac{1}{15}.</mrow>
	</md>
	Therefore, it is the same probability of getting the A whether you pick first or last.  In general, to win on the kth pick gives
	<md>
	<mrow> P(\text{kth}) &amp; = P(\text{LL...LA})</mrow>
	<mrow> &amp; = \frac{14 \cdot 13 \cdot ... \cdot (15-k) \cdot 1}{15 \cdot 14 ... \cdot (16-k) \cdot (15-k)} = \frac{1}{15}</mrow>
	</md>
	Hence, it is the same probability regardless of when you get to pick.
	</p>
	
	
	<p>
	If there are two A's possible, then the options for person k in include either receiving the first of the two slips or the second. The probability for determining the first of the two is computed in a manner similar to above except that there is one more A and one less other.
	<md>
	<mrow> P(\text{kth as first}) &amp; = P(\text{LL...LA})</mrow>
	<mrow> &amp; = \frac{13 \cdot 12 \cdot ... \cdot (15-k)  \cdot 2}{15 \cdot 14 ... \cdot (16-(k+1)) \cdot (16-k)} = \frac{2 \cdot (15-k)}{15 \cdot 14}</mrow>
	</md>

	The probability of getting the second A means exactly one of the previous k-1 selections also picked the other A. There are k-1 ways that this could happen. Computing for one of the options and multiplying by k-1 gives
	<md>
	<mrow> P(\text{kth as second}) &amp; = P(\text{LL...LAA})</mrow>
	<mrow> &amp; = (k-1) \cdot \frac{13 \cdot 12 \cdot ... \cdot (15-k) \cdot 2 \cdot 1}{15 \cdot 14 ... \cdot (16-k) \cdot (15-k)} = \frac{2 \cdot (k-1)}{15 \cdot 14}.</mrow>
	</md>	
	Adding these two together gives
	<md>
	<mrow>P(\text{getting an A when there are two}) = \frac{2 \cdot (15-k) + 2 \cdot (k-1)}{15 \cdot 14}</mrow>
	<mrow> = \frac{28}{15 \cdot 14} = \frac{2}{15}.</mrow>
	</md>

	For example, if k = 5,
	<md>
	<mrow> P(\text{5th as first}) = P(\text{LLLLA}) </mrow>
	<mrow> = \frac{13 \cdot 12 \cdot 11 \cdot 10  \cdot 2}{15 \cdot 14 \cdot 13 \cdot 12 \cdot 11} = \frac{20}{15 \cdot 14}</mrow>
	</md>
	<md>
	<mrow> P(\text{5th as second}) = P(\text{LL...LAA}) </mrow>
	<mrow> = 4 \cdot \frac{13 \cdot 12 \cdot \cdot 11 \cdot 2 \cdot 1}{15 \cdot 14 \cdot 13 \cdot 12 \cdot 11} = \frac{8}{15 \cdot 14}.</mrow>
	</md>	
	Adding these together yields the general result. So, once again, it doesn't matter which pick you use since the likelihood of getting an A is the same for all positions.
	</p>
	</solution>
</exercise>
</p>

<p>
<exercise><title>Shared Birthdays</title>
	<statement>
	<p>
	In this problem, you want to consider how many people are necessary in order to have an even chance of finding two or more who share a common birthday. Toward that end, assuming a year has exactly 365 equally likely days let r be the number of people in a sample and consider the following:
	<ol>
		<li>Determine the number of different outcomes of birthdays when order matters and birthdays are allowed to be repeated.</li>
		<li>Determine the number of different outcomes when birthdays are not allowed to be repeated.</li>
		<li>Determine the probability that two or more of your r students have the same birthday.</li>
		<li>Prepare a spreadsheet with the probabilities found above from r=2 to r=50. Determine the value of r for which this probability is closest to 0.5.</li>
		<li>As best as you can, sample two groups of the size found above and gather birthday information. For each group, determine if there is a shared birthday or not.  Compare your results with others in the class to check whether the sampling validates that about half of the samples should have a shared birthday group.</li>
	</ol>
	</p>
	</statement>
	
	<solution>
	<p>
	The correct sample size to get past a probability of 0.5 is 23 people. You should justify this numerically by justifying the following probabilities:
<pre>
#	P(Match)	
1	0
2	0.0027
3	0.0082
4	0.0164
5	0.0271
6	0.0405
7	0.0562
8	0.0743
9	0.0946
10	0.1169
11	0.1411
12	0.1670
13	0.1944
14	0.2231
15	0.2529
16	0.2836
17	0.3150
18	0.3469
19	0.3791
20	0.4114
21	0.4437
22	0.4757
23	0.5073
24	0.5383
25	0.5687
26	0.5982
27	0.6269
28	0.6545
29	0.6810
30	0.7063
</pre>
	</p>
	</solution>
</exercise>
</p>

<p>
<exercise><title>Internet meme solution</title>	
	<statement>	
	<p>
	This one is from an internet meme:  Two fair 6-sided dice are rolled together and you are told that at least one of the dice is a 6. Given that a 6 will be removed, determine the probability that the other die is a 6.
	</p>
	</statement>
	
	<solution>
	<p>
	In this case, you are presented with an outcome where the possible choices consist of 
	<me>(1,6), (2,6), (3,6), (4,6), (5,6), (6,6), (6,5), (6,4), (6,3), (6,2), (6,1).</me>
Each of these would satisfy the condition that at least one of the dice is a 6. From this group, the only success that satisfies being a 6, given that another 6 has already been removed, is the (6,6) outcome. Therefore, the conditional probability is 1/11.
	</p>
	<p>
	It is interesting to note that if the question instead was posed so that one of the dice was a 6 and it was removed, then the probability of the other dice showing a 6 would be 1/6.
	</p>
	
	</solution>
</exercise>
</p>

<p>
<exercise><title>100 people on an airplane with boarding pass issues</title>
	<statement>
	<p>
	This is a famous problem.  100 people are in line, boarding an airplane with 100 seats, one at a time. They are in no particular order. The first person has lost his boarding pass, so he sits in a random seat. The second person does the following:

	<ul>
	<li>Goes to his seat (the one it says to go to on the boarding pass). If unoccupied, sit in it.</li>
	<li>If occupied, find a random seat to sit in.</li>
	</ul>
	Everyone else behind him does the same. What is the probability that the last person sits in his correct seat?
	</p>
	</statement>

	<solution>
	<p>
	To get the idea, consider what happens with only 2 people, then only 3. Generalize. 
	</p>
	<p>
	The answer is 1/2. To obtain this, you can define recursively the probability that the kth person sits in their own set as f(k).  Consider the first traveler's and your seats. Then you get the following cases:
	<ul>
		<li>P(first guy sits in his own seat and you sit in yours) = <m>\frac{1}{k} \cdot 1</m> </li>
		<li>P(first guy sits in your seat and you do not sit in yours) = <m>\frac{1}{k} \cdot 0</m> </li>
		<li>P(other k-2 travelers make their choices) = <m>(k-2) \frac{1}{k} f(k-1)</m> </li>
	</ul>
	<me>f(k) = 1/k + 0 + (k-2)/k f(k-1)</me>
	with f(2) = 1/2.
	</p>
	<p>
	For example,
	<ul>
		<li>f(3) = 1/3 + f(2)/3 = 1/3 + 1/6 = 1/2. </li>
		<li>f(4) = 1/4 + 2/4 f(3) = 1/4 + 1/2 1/2 = 1/2. </li>
		<li>f(5) = 1/5 + 3/5 1/2 = 1/2. </li>
		<li>f(6) = 1/6 + 4/6 1/2 = 1/2. </li>
	</ul>
	Etc.
	</p>
	</solution>	
</exercise>
</p>

<p>
<exercise><title>Basic Independence Calculations</title>
	<statement>
	<p>
	Given <m>P(A) = 0.43, P(B) = 0.72, </m> and <m>P(A \cap B) = 0.31</m>, verify that A and B are not independent.
	</p>
	</statement>
	<solution>
	<p>
	A and B are <xref ref="DefnIndependentEvents">independent by definition</xref> provided 
	<me>P(A \cap B) = P(A) P(B).</me>
	Using the provided values, notice that
	<me>P(A \cap B) = 0.31</me>
	but
	<me>P(A)P(B) = 0.43 \cdot 0.72 = 0.3096.</me>
	Since these are not equal (regardless how close) then A and B are not independent.
	</p>
	</solution>
</exercise>
</p>

<p>
<exercise><title>Compound events and Independence</title>
	<statement>
	<p>
	Given A, B, and C are <xref ref="DefnIndependentEvents">independent events</xref>, with <m>P(A) = 2/5, P(B) = 3/4,</m> and <m>P(C) = 1/6</m>, determine:
	<ol>
		<li><m>P(A \cap B \cap C)</m></li>
		<li><m>P(A^c \cap B^c \cap C)</m></li>
		<li><m>P(A \cup B \cup C)</m></li>
	</ol>
	</p>
	</statement>
	<solution>
	<p>
	Extending the <xref ref="DefnIndependentEvents">definition of independent events</xref> gives
	<me>P(A \cap B \cap C) = \frac{2}{5} \frac{3}{4} \frac{1}{6}.</me>
	By the corollary for independent events, complements also maintain a similar independence. So
	<me>P(A^c \cap B^c \cap C) \frac{3}{5} \frac{1}{4} \frac{1}{6} .</me>
	To complete the third part, use the <xref ref="ProbabilityThreeUnions">inclusion/exclusion result</xref> for dealing with three sets.
	</p>
	</solution>
</exercise>
</p>

<p>	
<exercise><title>Rolling multiple dice</title>
	<statement>
	<p>
For a pair of dice you want to consider the events A = {rolling a 7 or 11} and B = {otherwise}...as in the first roll in the game of craps.  Further, for notation purposes let's take ABA (for example) to mean event A occurs on the first roll, event B occurs on the second roll, and event A occurs again on the third roll...in that order only. If you roll the dice 5 times, determine
	<ol>
		<li>P(AABBB)</li>
		<li>P(BBBAA)</li>
		<li>The probability of getting A on exactly two rolls of the dice.</li>
	</ol>
	</p>
	</statement>
	<solution>
	<p>
	Successive rollings of a pair of dice are <xref ref="DefnIndependentEvents">independent events</xref>. Therefore, 
	<me>P(AABBB) = P(A)P(A)P(B)P(B)P(B) = \frac{8}{36} \frac{8}{36} \frac{28}{36} \frac{28}{36} \frac{28}{36}</me>
	Similarly for the second part.
	</p>
	<p>
	For the third part, notice that there will be <m>\binom{5}{2}</m> ways to rearrange 2 A's and 3 B's but that each of these will have two 8/36's and three 28/36's but just in a different order. Therefore, you will get
	<me>10 \cdot \frac{8}{36} \frac{8}{36} \frac{28}{36} \frac{28}{36} \frac{28}{36}</me>
	</p>
	</solution>
</exercise>
</p>

<p>
<exercise><title>Redundancy</title>
	<statement>	
	<p>
	To help "insure" the success of a mission, you propose several redundant components so that the mission is a success if one or more succeed. Supposing that these separate components act independently of each other and that each component has a 75% chance of success, determine:
	<ol>
		<li>The probability of failure if you utilize 2 components.</li>
		<li>The probability of failure if you utilize 5 components.</li>
		<li>The number of components needed to insure that the probability of success is at least 99%.</li>
	</ol>
	</p>
	</statement>
</exercise>
</p>

<p>
<exercise><title>Internet Meme redux</title>
	<statement>	
	<p>
	Again, from an internet meme:  Two fair 6-sided dice are rolled together and you are told that at least one of the dice is a 6. A 6 is removed and you are presented with the other die.  Determine the probability that it is a 6.
	</p>
	</statement>
	<solution> 
	<p>
	For this setting, notice that the outcomes from each of the two dice are independent of each other. Removing one of the dice, regardless of it's value, does not affect the other. The question in this case does not ask for a conditional probability.
	</p>
	</solution>
</exercise>
</p>

<p>
<exercise><title>Single Elimination Tournament</title>
	<statement>
	<p>
	Consider a n=4 team single-elimination tournament where the teams are "seeded" from 1 (the best team) to 4 (the worst team).  For this tournament, team 1 plays team 4 and team 2 plays team 3. The winner of each play each other to determine the final winner. When teams j and k play, set P(j wins) = <m>\frac{k}{j+k}</m> and similarly for team k.  Assuming separate games are independent of each other, determine the probability that team 4 wins the tournament. What about with 8 teams? What about 64 teams?
	</p>
	</statement>
	<solution>
	<p>
	P(4 wins) = P(4 beats 1) P(4 beats the winner of the other bracket)
	</p>
	<p>
	P(4 wins) = (1/5) * P(4 beats 2 | 2 beats 3) + P(4 beats 3 | 3 beats 2)
	</p>
	<p>
	P(4 wins) = 1/5 [(3/5)(2/6) + (2/5)(3/7)] = 78/1050 = 0.0742
	</p>
	<p>
	For the other teams:
	</p>
	<p>
	P(1 wins) = 4/5 [(3/5)(2/3) + (2/5)(3/4) ] = 0.56
	</p>
	<p>
	P(2 wins) = 3/5 [(4/5)(1/3) + (1/5)(4/6) ] = 0.24
	</p>
	<p>
	P(3 wins) = 2/5 [(4/5)(1/4) + (1/5)(4/7) ] = 0.1257
	</p>
	</solution>
</exercise>
</p>	
</section>

</chapter>