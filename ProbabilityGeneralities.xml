<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="ProbabilityGeneralities" xmlns:xi="http://www.w3.org/2001/XInclude">
<title>Probability Theory</title>

<section><title>Introduction</title>
	<p>This chapter uses relative frequency to motivate the definition of probability and then delves into the resulting consequences.</p>
	<p>Mathematics generally focuses on providing precise answers with absolute certainty. For example, solving an equation generates specific (and non-varying) solutions. Statistics on the other hand deals with providing precise answers to questions when there is uncertainty. It might seem impossible to provide such precise answers but the focus of this text is to show how that can be done so long as the questions are properly posed and the answers properly interpreted.</p>
	<p>People often make claims about being the biggest, best, most often recommended, etc. One sometimes even believes these claims. In this class, we will attempt to determine if such claims are reasonable by first introducing probability from a semi rigorous mathematical viewpoint using concepts developed in Calculus. We will use this framework to carefully discuss making such statistical inferences as above and in general to obtain accurate knowledge even when the known data is not complete. </p>
</section>


<section xml:id="RelativeFrequency"><title>Relative Frequency</title>
	<p>When attempting to precisely measure uncertainty a few experiments are in order. When doing statistical experiments, a few terms and corresponding notation might be useful:</p>
	<ul>
		<li>S = Universal Set or Sample Space Experiment or Outcome Space. 
		This is the collection of all possible outcomes.</li>
		<li>Random Experiment. A random experiment is a repeatable activity which has more than one
		possible outcome all of which can be specified in advance but can not be known in advance with certainty.</li>
		<li>Trial. Performing a Random Experiment one time and measuring the result.</li> 
		<li>A = Event. A collection of outcomes.  Generally denoted by an upper case letter such as A, B, C, etc.</li>
		<li>Success/Failure. When recording the result of a trial, a success for event A occurs when the outcome
		lies in A. If not, then the trial was a failure. There is no qualitative meaning to this term.</li>
		<li>Mutually Exclusive Events. Two events which share no common outcomes. Also known as disjoint events.</li>
		<li>|A| = Frequency. In a sequence of n events, the frequency is the number of trials which resulted in 
		a success for event A.</li>
		<li>|A| / n = Relative Frequency. A proportion of successes to total number of trials.</li>
		<li>Histogram. A bar chart representation of data where area corresponds to the value being described.</li>
	</ul>

	<p>To investigate these terms and to motivate our discussion of probability, consider flipping coins using the interactive cell below. Notice in this case, the sample space S = {Heads, Tails} and the random experiment consists of flipping a fair coin one time. Each trial results in either a Head or a Tail. Since we are measuring both Heads and Tails then we will not worry about which is a success or failure. Further, on each flip the outcomes of Heads or Tails are mutually exclusive events. We count the frequencies and compute the relative frequencies for a varying number of trials selected by you as you move the slider bar. Results are displayed using a histogram.</p>

<p>
<sage>
<input>
coin = ["Heads", "Tails"]
@interact
def _(num_rolls = slider([5..5000],label="Number of Flips")):
	rolls = [choice(coin) for roll in range(num_rolls)]
	show(rolls)   
	freq = [0,0]
	for outcome in rolls:
		if (outcome=='Tails'):
			freq[0] = freq[0]+1
		else:
			freq[1] = freq[1]+1
	print("\nThe frequency of tails = "+ str(freq[0]))+" and heads = "+ str(freq[1])+"."
	rel = [freq[0]/num_rolls,freq[1]/num_rolls]
	print("\nThe relative frequencies for Tails and Heads:"+str(rel))
	show(bar_chart(freq,axes=False,ymin=0))     #  A histogram of the results
</input>
</sage>
</p>

	<p>
	Question 1: What do you notice as the number of flips increases?
	</p>
	<p>
	Question 2: Why do you rarely (if even) get exactly the same number of Heads and Tails? Would you not "expect"
	that to happen?
	</p>
	



	<p>You should have noticed that as the number of flips increases, the relative frequency of Heads (and Tails)
	stabilized around 0.5. This makes sense intuitively since there are two options for each 
	individual flip and 1/2 of those options are Heads while the other 1/2 is Tails.</p>
	<p>
	Let's try again
	by doing a random experiment consisting of rolling a single die one time. Note that the sample space 
	in this case will be the outcomes S = {1, 2, 3, 4, 5, 6}.
	</p>

<p>
<sage>
<input>
@interact
def _(num_rolls = slider([20..5000],label='Number of rolls'),Number_of_Sides = [4,6,8,12,20]):
	die = list((1..Number_of_Sides))
	rolls = [choice(die) for roll in range(num_rolls)]
	show(rolls)   

	freq = [rolls.count(outcome) for outcome in set(die)]  # count the numbers for each outcome
	print 'The frequencies of each outcome is '+str(freq)

	print 'The relative frequencies of each outcome:'
	rel_freq = [freq[outcome-1]/num_rolls for outcome in set(die)]  # make frequencies relative
	print rel_freq
	fs = []
	for f in rel_freq:
		fs.append(f.n(digits=4))
	print fs
	show(bar_chart(freq,axes=False,ymin=0)) 
</input>
</sage>
</p>

	<p>Notice for a single die there are a larger number of options (for example 6 on a regular
	die) but once again the relative frequencies of each  outcome was close to 1/n (i.e. 1/6 for the regular die)
	as the number of rolls increased.</p>
	<p>In general, this suggests a rule: if there are n outcomes and each one has the same
	chance of occurring on a given trial then on average on a large number of trials the relative
	frequency of that outcome is 1/n.
	In general, if a number of outcomes are "equally likely" then this is a good model for measuring
	the proportion of outcomes that would be expected to have any given outcome. However, it is not
	always true that outcomes are equally likely. Consider rolling two die and measuring their sum:</p>

<p>
<sage>
<title>Rolling Two Dice and Measuring their Sum</title>
<input>
@interact
def _(num_rolls = slider([20..5000],label='Number of rolls'),num_sides = slider(4,20,1,6,label='Number of sides')):
    die = list((1..num_sides))
    dice = list((2..num_sides*2))
    rolls = [(choice(die),choice(die)) for roll in range(num_rolls)]
    sums = [sum(rolls[roll]) for roll in range(num_rolls)]
    show(rolls)   

    freq = [sums.count(outcome) for outcome in set(dice)]  # count the numbers for each outcome
    print 'The frequencies of each outcome is '+str(freq)
    
    print 'The relative frequencies of each outcome:'
    rel_freq = [freq[outcome-2]/num_rolls for outcome in set(dice)]  # make frequencies relative
    print rel_freq        
    show(bar_chart(freq,axes=False,ymin=0))     #  A histogram of the results
    print "Relative Frequence of ",dice[0]," is about ",rel_freq[0].n(digits=4)
    print "Relative Frequence of ",dice[num_sides-1]," is about ",rel_freq[num_sides-1].n(digits=4)

</input>
</sage>
</p>


<!--
	<exercise>
		<introduction>
		<p>
		Let's see if you understand the relationship between frequency and relative frequency.
		</p>
		</introduction>
		<webwork source="local/relative_frequency1.pg">
		</webwork>
		<conclusion>
		<p>
		So, these are simple calculations.
		</p>
		</conclusion>
	</exercise>
-->


	<p>
	Question 1: What do you notice as the number of rolls increases?
	</p>
	<p>
	Question 2: What do you expect for the relative frequencies and why are they not all exactly the same?
	</p>

	<p>Notice, not only are the answers not the same but they are not even close. To understand why this 
	is different from the examples before, consider the possible outcomes from each pair of die. Since we
	are measuring the sum of the dice then (for a pair of standard 6-sided dice) the possible sums are from 
	2 to 12. However, there is only one way to get a 2--namely from a (1,1) pair--while there are 6 ways to get
	a 7--namely from the pairs (1,6), (2,5), (3,4), (4,3), (5,2), and (6,1). So it might make some sense
	that the likelihood of getting a 7 is 6 times larger than that of getting a 2. Check to see if that
	is the case with your experiment above.</p>
	
<definition><title>Cumulative relative frequency</title> 
<statement>For a collection of ordered events <m>x_1 \lt x_2 \lt ... \lt x_s</m> with corresponding frequencies <m>f_1, f_2, ..., f_s</m>, the cumulative relative frequency is the function
<me>F(x) = \sum_{x_k \le x} f_{x_k}</me>
</statement>
</definition>

<p>Add a sage interact here where a student can enter a set of data and display the relative frequency histogram and the cumulative relative frequency graph.
</p>

</section>








<section><title>Definition of Probability</title>
	<introduction>
		<p>
		Relative frequency gives a way to measure the proportion of "successful" outcomes when doing an experimental approach. From the interactive applications above, it appears that the relative frequency does jump around as the experiment is repeated but that the amount of variation decreases as the number of experiments increases. This is known to be true in general and leads to what is known as the "Law of Large Numbers". We would like to formalize what these relative frequencies seem to be approaching and will call this theoretical limit the "probability" of the outcome. In doing so, we will do our best to model our definition so that it follow the behavior of relative frequency.
		</p>
	</introduction>
	<p>Using the ideas from our examples above, consider how you might formally define a way
	to measure the expectation from similar experiments.  Before doing so, we need a little notation:</p>


	<definition><title>Pairwise Disjoint Sets</title>
	<statement>
	<p>
	<m> \{ A_1, A_2, ... , A_n \}</m> are pairwise disjoint provided <m>A_k \cap A_j = \emptyset</m> so long as <m>k \ne j</m>.
	Disjoint sets as also often called mutually exclusive.
	</p>
	</statement>
	</definition>
	
<sage><title>Playing around with intersections and unions of sets</title>
<input>
def f(s, braces=True): 
    t = ', '.join(sorted(list(s)))
    if braces: return '{' + t + '}'
    return t
def g(s): return set(str(s).replace(',',' ').split())

@interact
def _(X='1,2,3,a', Y='2,a,3,4,apple', Z='a,b,10,apple'):
    S = [g(X), g(Y), g(Z)]
    X,Y,Z = S
    XY = X &amp; Y
    XZ = X &amp; Z
    YZ = Y &amp; Z
    XYZ = XY &amp; Z
    pretty_print(html('<center>'))
    pretty_print(html("$X \cap Y$ = %s"%f(XY)))
    pretty_print(html("$X \cap Z$ = %s"%f(XZ)))
    pretty_print(html("$Y \cap Z$ = %s"%f(YZ)))
    pretty_print(html("$X \cap Y \cap Z$ = %s"%f(XYZ)))
    pretty_print(html('</center>'))
    centers = [(cos(n*2*pi/3), sin(n*2*pi/3)) for n in [0,1,2]]
    scale = 1.7
    clr = ['yellow', 'blue', 'green']
    G = Graphics()
    for i in range(len(S)):
        G += circle(centers[i], scale, rgbcolor=clr[i], 
             fill=True, alpha=0.3)
    for i in range(len(S)):
        G += circle(centers[i], scale, rgbcolor='black')

    # Plot what is in one but neither other
    for i in range(len(S)):
        Z = set(S[i])
        for j in range(1,len(S)):
            Z = Z.difference(S[(i+j)%3])
        G += text(f(Z,braces=False), (1.5*centers[i][0],1.7*centers[i][1]), rgbcolor='black')


    # Plot pairs of intersections
    for i in range(len(S)):
        Z = (set(S[i]) &amp; S[(i+1)%3]) - set(XYZ)
        C = (1.3*cos(i*2*pi/3 + pi/3), 1.3*sin(i*2*pi/3 + pi/3))
        G += text(f(Z,braces=False), C, rgbcolor='black')

    # Plot intersection of all three
    G += text(f(XYZ,braces=False), (0,0), rgbcolor='black')

    # Show it
    G.show(aspect_ratio=1, axes=False)
</input>
</sage>
	

	<p>
	To model the behavior above, consider how we might create a definition for our expectation
	of a given outcome by following the ideas uncovered above. To do so, first consider a desired collection
	of outcomes A. If each outcome in A is equally likely then we might follow the concept behind relative 
	frequency and consider a measure of expectation be |A|/|S|. Indeed, on a standard 
	6-sided die, the expectation of the outcome A={2} from the collection S = {1,2,3,4,5,6} should be
	|A|/|S| = 1/6.</p>
	<p>From the example where we take the sum of two die, the outcome A={4,5} from the
	collection S = {2,3,4,...,12} would be</p>
	<md>
		<mrow>|A| = | {(1,3),(2,2),(3,1),(1,4),(2,3),(3,2),(4,1)}| = 7</mrow>
		<mrow>|S| = | {(1,1),...,(1,6),(2,1),...,(2,6),...,(6,1),...,(6,6)}| = 36</mrow>
	</md>
	<p>and so the expected relative frequency would be |A|/|S| = 7/36. Compare this theoretical value
	with the sum of the two outcomes from your experiment above.</p>
			
	<p>We are ready to now formally give a name to the theoretical measure of expectation for
	outcomes from an experiment. Taking our cue from the ideas related to equally likely outcomes, we 
	make our definition have the following basic properties:</p>
	<ol>
		<li>Relative frequency cannot be negative, since cardinality cannot be negative</li>
		<li>Relative frequencies for disjoint events should sum to one</li>
		<li>Relative frequencies for collections of disjoint outcomes should equal the sum of the
	individual relative frequencies</li>
	</ol>
	
	<p>Based upon these we give the following:</p>
	<definition xml:id="DefnProb">
		<statement>The probability P(A) of a given outcome A is a set function which satisfies:
		<ol>
			<li>(Nonnegativity) P(A) <m>\ge 0</m></li>
			<li>(Totality) P(S) = 1</li>
			<li>(Subadditivity) If A <m>\cap</m> B = <m>\emptyset</m>, then P(A <m>\cup</m> B) = P(A) + P(B).  
			In general, if {<m>A_k</m>} are pairwise disjoint then <m>P( \cup_k A_k) = \sum_k P(A_k)</m>.</li>
		</ol>
		</statement>
	</definition>	
	

	<exercise>
		<introduction>
		<p>
		Using the definition above determine the following probabilities.  (This one not available
		in the PCC library for some reason.)
		</p>
		</introduction>
		<webwork source="Library/MC/PreAlgebra/setPreAlgebraC06S04/BasicProbability01.pg">
		</webwork>
		<conclusion>
		<p>
		Notice when you are given complete information regarding the entire data set then determining
		probabilities for events can be relatively easy to compute.
		</p>
		</conclusion>
	</exercise>

	<p>Based upon this definition we can immediately establish a number of results.</p>
	<theorem xml:id="ProbabilityComplemnts"><title>Probability of Complements</title>
		<statement> For any event A, <m>P(A) + P(A^c) = 1</m>
		</statement>
		<proof>
			<p>Let A be any event and note that <m>A \cap A^c = \emptyset</m>.  But <m>A \cup A^c = S</m>.
			So, by subadditivity <m>1 = P(S) = P(A \cup A^c) = P(A) + P(A^c)</m> as desired.</p>
		</proof>
	</theorem>
	<theorem xml:id="ProbabilityEmptySet">
		<statement><m>P(\emptyset) = 0</m>
		</statement>
		<proof>
			<p>Note that <m>\emptyset^c = S</m>. So, by the theorem above, 
			<m>1 = P(S) + P(\emptyset) \Rightarrow 1 = 1 + P(\emptyset)</m>.
			Cancelling the 1 on both sides gives <m>P(\emptyset) = 0</m>. </p>
		</proof>
	</theorem>
	<theorem xml:id="ProbabilityContainment">
		<statement>For events A and B with <m> A \subset B, P(A) \le P(B)</m>.
		</statement>
		<proof>
			<p>Assume sets A and B satisfy <m> A \subset B</m>. Then, notice that
			<m>A \cap (B-A) = \emptyset</m> and  <m>B = A \cup (B-A)</m>. Therefore, by 
			subadditivity and nonnegativity</p>
			<md>
				<mrow>0 \le P(B-A)</mrow>
				<mrow>P(A) \le P(A) + P(B-A) </mrow>
				<mrow>P(A) \le P(B)</mrow>
			</md>
		</proof>
	</theorem>
	<theorem xml:id="ProbabilityLessThanOne">
		<statement>For any event A, <m>P(A) \le 1</m> 
		</statement>
		<proof>
			<p>Notice <m>A \subset S</m>. By the theorem above <m> P(A) \le P(S) = 1</m></p>
		</proof>
	</theorem>
	<theorem xml:id="ProbabilityTwoUnions">
		<statement>For any sets A and B, <m>P(A \cup B) = P(A) + P(B) - P(A \cap B)</m>
		</statement>
		<proof>
			<p>Notice that we can write <m>A \cup B</m> as the disjoint union</p>
			<md>
				<mrow>A \cup B = (A-B) \cup (A \cap B) \cup (B-A).</mrow>
			</md> 
			<p>We can also write disjointly</p>
			<md>
				<mrow>A = (A-B) \cup (A \cap B)</mrow>
				<mrow>B = (A \cap B) \cup (B-A)</mrow>
			</md>
			<p>Hence, </p>
			<md>
				<mrow>P(A) &amp; + P(B) - P(A \cap B) </mrow>
				<mrow>&amp; = [P(A-B) + P(A \cap B)] + [P(A \cap B) + P(B-A)] - P(A \cap B)</mrow>
				<mrow>&amp; = P(A-B) + P(A \cap B) + P(B-A)</mrow>
				<mrow>&amp; = P(A \cup B)</mrow>
			</md>
		</proof>
	</theorem>
	<p>This result can be extended to more that two sets using a property known as inclusion-exclusion. The
	following two theorems illustrate this property and are presented without proof.
	</p>
	<corollary xml:id="ProbabilityThreeUnions">
		<statement>
			For any sets A, B and C, 
			<md>
				<mrow>P(A \cup B \cup C) &amp; = P(A) + P(B) + P(C)</mrow> 
				<mrow>&amp; - P(A \cap B) - P(A \cap C) - P(B \cap C) </mrow>
				<mrow>&amp; + P(A \cap B \cap C)</mrow>
			</md>
		</statement>
	</corollary>
	<corollary xml:id="ProbabilityFourUnions">
		<statement>
			For any sets A, B, C and D, 
			<md>
				<mrow>P(A \cup B \cup C \cup D) &amp; = P(A) + P(B) + P(C) + P(D)</mrow>
				<mrow>&amp; - P(A \cap B) - P(A \cap C) - P(A \cap D)  - P(B \cap C) - P(B \cap D) - P(C \cap D)</mrow>
				<mrow>&amp; + P(A \cap B \cap C) + P(A \cap B \cap D) + P(A \cap C \cap D) + P(B \cap C \cap D)</mrow>
				<mrow>&amp; - P(A \cap B \cap C \cap D)</mrow>
			</md>
		</statement>
	</corollary>
	
	<p>
	Many times, you will be dealing with making selections from a sample space where each item in the space has an equal chance of being selected. This may happen (for example) when items in the sample space are of equal size or when selecting a card from a completely shuffled deck or when coins are flipped or when a normal fair die is rolled. 
	</p>
	<p>It is important to notice that not all outcomes are equally likely--even in times when there are only two of them. Indeed, it is generally not an equally likely situation when picking the winner of a football game which pits, say, the New Orleans Saints professional football team with the New Orleans Home School Saints. Even though there are only two options the probability of the professional team winning is much greater than the chances that the high school will prevail. 
	</p>
	<p>
	When items are equally likely (sometimes also called "randomly selected") then each individual event has the same chance of being selected as any other. In this instance, determining the probability of a collection of outcomes is relatively simple.
	</p>
	<theorem><title>Probability of Equally Likely Events</title>
	<statement>If outcomes in S are equally likely, then for <m>A \subset S, P(A) = \frac{|A|}{|S|}</m> </statement>
	<proof>
	<p>
	Enumerate S = {<m>x_1, x_2, ..., x_{|S|}</m>} and note <m>P( \{ x_k \} ) = c</m> for some constant c since each item is equally likely. However, using each outcome as a disjoint event and the definition of probability, 
	<md>
		<mrow>1 = P(S) &amp; = P( \{ x_1 \} \cup \{x_2 \} \cup ... \cup \{x_{|S|} \} )</mrow>
		<mrow> &amp; = P(\{ x_1 \}) + P(\{ x_2 \} ) + ... + P(\{ x_{|S|} \} )</mrow>
		<mrow> &amp; = c + c + ... + c = {|S|} \times c</mrow>
	</md>
	and so <m>c = \frac{1}{{|S|}}</m>. Therefore, <m>P( \{ x_k \} ) = \frac{1}{|S|}</m> .
	</p>
	<p>
	Hence, with A = {<m>a_1, a_2, ..., a_{|A|}</m>}, breaking up the disjoint probabilities as above gives
	<md>
		<mrow>P(A) &amp; = P( \{ a_1 \} \cup \{ a_2 \} \cup ... \cup \{ a_{|A|} \} )</mrow>
		<mrow> &amp; = P(\{ a_1 \}) + P(\{ a_2 \} ) + ... + P(\{ a_{|A|} \} )</mrow>
		<mrow> &amp; = \frac{1}{{|S|}} + \frac{1}{{|S|}} + ... + \frac{1}{{|S|}}</mrow>
		<mrow> &amp; = \frac{|A|}{{|S|}}</mrow>
	</md>
	as desired.
	</p>
	</proof>
	</theorem>

<sage><title>Let's play around with cards</title>
<input>
var('A C D H J K Q S') 

suits = [S, D, C, H] 
values = [2, 3, 4, 5, 6, 7, 8, 9, 10, J, Q, K, A] 

deck = [(value, suit) for suit in suits for value in values]
full_deck = copy(deck)  # to save a copy of the original deck for later use.
print deck
shuffle(deck)
print deck
deck1 = copy(full_deck)
shuffle(deck1)

@interact
def _(auto_update=False):
    global deck1    
    shuffle(deck1)            
    if (Set(deck1).cardinality()&lt;5):
        print 'Deck is too small...getting a new deck'
        deck1 = copy(full_deck)
    else:
        hand = [deck1.pop() for card in range(5)] 
        print 'The cards dealt:'
        show(hand)
        print 'The remaining cards in the deck:'
        print(deck1)
        pretty_print(html("\nThe number of remaining cards in the deck = %s"%str(Set(deck1).cardinality())))
</input>
</sage>
	
	<exercise>
		<introduction>
		<p>
		Let's see if you understand the relationship between frequency and relative frequency. In this exercise, presume "Probabiity" to be the expected fraction of outcomes you might logically expect.
		</p>
		</introduction>
		<webwork source="Library/MC/PreAlgebra/setPreAlgebraC06S04/BasicProbability03.pg">
		</webwork>
		<conclusion>
		<p>
		So, these are simple calculations.
		</p>
		</conclusion>
	</exercise>

	<exercise>
		<introduction>
		<p>This one is a little harder and uses the binomial coefficients from Combinatorics.
		</p>
		</introduction>
		<webwork source="Library/Rochester/setProbability5RandomSample/ur_pb_5_1a.pg">
		</webwork>
		<conclusion>
		<p>
		Notice how the probabilities look similar to relative frequencies. It's just the case 
		that you are counting ALL of the individual simple possibilities that lead to a success.
		</p>
		</conclusion>
	</exercise>



<subsection><title>Exercises - Basic</title>

<exercise>
	<title>Poker</title>
	<statement><p>
	Determine the probabilities associated with the various 5-card hands.
	</p>
	</statement>
</exercise>

<exercise><title>Dice</title>
	<statement>
	<p>
	Determine the 36 possible outcomes related to the rolling a pair of fair dice. Justify why each of these outcomes is equally likely. Determine the probabilities associated with each possible sum.
	</p>
	</statement>
</exercise>

<exercise><title>Skew Dice</title>
	<statement>
	<p>
	Suppose you have one die which only has three possible sides labeled 1, 2, or 3. Suppose a second die has twelve equally likely sides with labels 1,2,3,4,4,5,5,6,6,7,8,9.  Justify that the probabilities associate with each possible sum is the same as the probabilities when using two normal 6-sided dice.
	</p>
	</statement>
</exercise>

<exercise><title>Craps</title>
	<statement><p>
	Analyze the game of
	<url href="http://mathworld.wolfram.com/Craps.html">"craps"</url>.
	</p>
	</statement>
</exercise>
</subsection>


</section>

<section>
<title>Conditional Probability</title>
	<introduction>
	<p>When finding the probability of an event, sometimes you may need to consider past history and how it might affect things. Indeed, you might think that when the local station forecasts rain than it the probability of it actually raining should be greater than if they forecast fair skies. At least that is the hope. :)  In this section, you will develop a way to deal with the probability of some event that might change dependent upon the occurence or not of some other event.
	</p>
	<p>Indeed, consider what happens when you keep on dealing a hand of five cards from a shuffled deck but without replacement...</p>

<sage>
<input>
print "Conditional Events - successively deal 5 cards w/o replacement" 

var('Ace Clubs Diamonds Hearts Jack King Queen Spades') 

suits = [Spades, Diamonds, Clubs, Hearts] 
values = [2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King, Ace] 

deck = [(value, suit) for suit in suits for value in values]
full_deck = copy(deck)  # to save a copy of the original deck for later use.

deck1 = copy(full_deck)
history1=[]
@interact
def _(choice=['Hearts','Spades','Diamonds','Clubs','New Deck'],again=['Repeat Same Suit']):
    global deck1, history1
    shuffle(deck1)
    if choice=='Hearts':
        suit = Hearts
    elif choice=='Spades':
        suit = Spades
    elif choice=='Diamonds':
        suit = Diamonds
    elif choice=='Clubs':
        suit = Clubs
    else:
        deck1 = copy(full_deck)
        shuffle(deck1)
        history1=[]
    if (Set(deck1).cardinality()&lt;5):
        print "Deck is too small...get a new deck"
    elif choice&lt;&gt;'New Deck':
        hand = [deck1.pop() for card in range(5)] 
        print "Click on a desired suit above to deal out another 5 card hand.  The cards dealt:"
        print hand
        print "The remaining cards in the deck:"
        print deck1
        num = Set(deck1).cardinality()
        print "\nThe number of remaining cards in the deck = %s"%str(num)
        looking = []
        for card in deck1:
            if card[1]==suit:
                looking.append(card)
        prob = float(Set(looking).cardinality())/num
        history1.append(prob)
        print 'So, the remaining probability of getting a card from '+choice+' from the remaining cards is %s'%str(prob)
    list_plot(history1).show(xmin=0,xmax=9,ymin=0,ymax=1,figsize=(5,2))

</input>
</sage>
	
<p>Now, what happens when you put the cards back in, reshuffle, and then get 5 cards.</p>
<sage>
<input>
print "Independent Events - Successively deal 5 cards but WITH replacement"

var('Ace Clubs Diamonds Hearts Jack King Queen Spades') 

suits = [Spades, Diamonds, Clubs, Hearts] 
values = [2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King, Ace] 

deck = [(value, suit) for suit in suits for value in values]
full_deck = copy(deck)  # to save a copy of the original deck for later use.
deck1 = copy(full_deck)

history2=[]
@interact
def _(choice=['Heart','Spade','Diamond','Club'],again=['Repeat Same Suit']):

    if choice=='Hearts':
        suit = Hearts
    elif choice=='Spades':
        suit = Spades
    elif choice=='Diamonds':
        suit = Diamonds
    else:
        suit = Clubs

    deck1 = copy(full_deck)
    shuffle(deck1) 
    hand = [deck1.pop() for card in range(5)] 
    print "The cards dealt:"
    show(hand)
    print "Replacing this hand and reshuffling gives the remaining cards in the deck:"
    deck1 = copy(full_deck)
    shuffle(deck1)
    print(deck1)
    
    num = Set(deck1).cardinality()
    pretty_print("\nThe number of remaining cards in the deck = %s"%str(num))
    looking = []
    for card in deck1:
        if card[1]==suit:
            looking.append(card)
    prob = float(Set(looking).cardinality())/num
    history2.append(prob)
    
    pretty_print('So, the remaining probability of getting a '+choice+' from the remaining cards is %s'%str(prob))
    list_plot(history2).show(xmin=0,xmax=9,ymin=0,ymax=1,figsize=(5,2))print 'Independent Events - Successively deal 5 cards but WITH replacement' 


</input>
</sage>
	</introduction>
	
	<p><title>Changing Sample Space - Balls:</title>  Consider a box with three balls: one Red, one White, and one Blue.  Using an equally likely assumption, the probability of randomly pulling out a Red ball should be 1/3.  That is P(Red) = 1/3.  However, suppose that for a first trial you pull out the White ball and set it aside. Attempting to pull out another ball leaves you with only two options and so the probability of randomly pulling out a Red ball is 1/2. Notice that the probability changed for the second trial dependent on the outcome of the first trial.</p>


	<p><title>Changing Sample Space - Cards: </title>
	Consider a deck of 52 standard playing cards and a success occurs when a Heart is selected from the deck. When extracting one card randomly, the probability	of that card being a Heart is then P(Heart) = 13/52. Now, assume that one card has already been extracted and setaside.  Now, prepare to extract another. If the first card drawn was a Heart, then there are only 12 Hearts left for the second draw. However, if the first card drawn was not a Heart, then there are 13 Hearts available for the second draw. To compute this probability correctly, one need to formulate the question so that subadditivity can  be utilized.</p>
	<p>
	To do this, consider 
	P(Heart on 2nd draw) 
	= P( [Heart on 1st draw <m>\cap</m> Heart on 2nd draw] <m>\cup</m> [Not Heart on 1st draw <m>\cap</m> Heart on 2nd draw] )
	= P(Heart on 1st draw <m>\cap</m> Heart on 2nd draw ) + P(Not Heart on 1st draw <m>\cap</m> Heart on 2nd draw )
	= | Heart on 1st draw <m>\cap</m> Heart on 2nd draw | / | Number of ways to get two cards |
	+ | Not Heart on 1st draw <m>\cap</m> Heart on 2nd draw / | Number of ways to get two cards |
	= (13 12) / (52 51) + (39 13) / (52 51) = 12 / (4 51) + (3 13) / ( 4 51) =  

	</p>
	
	<definition>
		<title>Conditional Probability</title>
			<statement>
			<me>P(B | A) = \frac{P(A \cap B)}{ P(A) },</me>
			 provided P(A)<m>\gt 0</m>.</statement>
		</definition>
		<theorem>
		<statement>Conditional Probability satisfies all of the requirements of regular probability.</statement>
		<proof>
		<p>
		By definition, for any event probability must be nonnegative. Therefore
		<m>P(A \cap B) \ge 0</m>.  Therefore, P(B | A) <m>\ge 0</m>.
		</p>
		<p>
		Further, P (S | A) = P(A <m>\cap</m> S)/P(A) = P(A)/P(A) = 1.
		</p>
		</proof>
		</theorem>
		
		<theorem><title>Multiplication Rule</title>
		<statement>
			<me>P(A \cap B) = P(A) P(B | A) = P(B) P(A | B)</me>
		</statement>
		<proof>
		<p>
		Unravel the definition of conditional probably by taking the denominator to the other side. Also note that you can write <m>A \cap B = B \cap A</m>.
		</p>
		</proof>
		</theorem>
	
	


	<exercise>
		<introduction>
		<p>Conditional Probability sometimes makes you have to think carefully about the ways to get 
		the desired outcome.
		</p>
		</introduction>
		<webwork source="Library/UMN/algebraKaufmannSchwitters/ks_15_5_42.pg">
		</webwork>
		<conclusion>
		<p>
		See how you had to break the given question up into two disjoint pieces.
		</p>
		</conclusion>
	</exercise>	


<subsection><title>Exercises - Conditional Probabilities</title>

<exercise><title>Conditional Basic computation</title>
	<statement><p>
	Given P(A) = 0.43, P(B) = 0.72, and <m>P(A \cap B) = 0.29</m>, determine
	<ol>
		<li><m>P(A \cup B)</m></li>
		<li><m>P(B | A)</m></li>
		<li><m>P(A | B)</m></li>
		<li><m>P(A^c \cap B^c)</m></li>
	</ol>
	</p>
	</statement>
</exercise>

<exercise><title>Gender vs University Major</title>
	<statement><p>
	The table below classifies students at your university according to gender and according to major.

	<table halign="left">
      	<tabular halign="right">
      
<row><cell bottom="medium" right="medium">Enrollment</cell><cell bottom="medium">Male</cell><cell bottom="medium" right="medium">Female</cell><cell bottom="medium">Totals</cell></row>      
<row><cell right="medium">STEM</cell><cell>420</cell><cell right="medium">510</cell><cell>930</cell></row>
<row><cell right="medium">Business</cell><cell>320</cell><cell right="medium">270</cell><cell>590</cell></row>
<row><cell bottom="medium" right="medium">Other</cell><cell bottom="medium">610</cell><cell bottom="medium" right="medium">710</cell><cell bottom="medium">1320</cell></row>
<row><cell right="medium">Totals</cell><cell>1350</cell><cell right="medium">1490</cell><cell>2840</cell></row>
		</tabular>
	</table>

	Determine the following:
	<ol>
		<li>P( STEM major )</li>
		<li>P( STEM | Female )</li>
		<li>P( Female | STEM )</li>
		<li>P( Female | Not STEM)</li>
	</ol>
	</p>
	</statement>
</exercise>
	
<exercise><title>Mean Tough Teacher</title>
	<statement>	
	<p>
	You are in a probability and statistics class with a teacher who has predetermined that only one student can make an A for the course. To be "fair", he places a number of slips of paper in a bowl equal to the number of students in the course with one of the slips having an A designation. Students in the course each can pick once randomly from the bowl and without replacement to see if they can get the lucky slip.  Determine the following:
	<ol>
		<li>If there are 15 students in your course, determine the probabilities of getting an A in the course if you pick first and if you pick last.</li>
		<li>Since the teacher likes you the most, she will give you the option of deciding whether to pick at any position. If so, determine the position that would give you the best likelihood of getting the A slop.</li>
		<li>Suppose again that the teacher was feeling more generous and decided instead to allow for two A's. Determine how that changes your likelihood of winning and on what position you would like to choose.</li>
		<li>Continue as above except that only one slip does not have an A on it.</li>
		<li>Discuss how your choice is affected by the number of students in the course or the number of A slips included.</li>
	</ol>
	</p>
	</statement>
	
	<solution>
	<p>
	Using the normal equally-likely definition, <m>P(\text{first}) = \frac{1}{15}</m>.
	</p>
	<p>
	To get the A on the last pick requires that all of the previous picks to be something else. You don't get the opportunity to pick the A if it has already been selected. So, if L stands for losing (not getting the A), then 
	<me> P(\text{last}) = P(\text{LLLLLLLLLLLLLLA}) \\ = \frac{14 \cdot 13 \cdot 12 \cdot 11 \cdot 10 \cdot 9  \cdot 8  \cdot 7  \cdot 6  \cdot 5  \cdot 4  \cdot 3  \cdot 2  \cdot 1}{15 \cdot 14 \cdot 13 \cdot 12 \cdot 11 \cdot 10 \cdot 9 \cdot 8 \cdot 7 \cdot 6 \cdot 5 \cdot 4 \cdot 3 \cdot 2} = \frac{1}{15}.</me>
	Therefore, it is the same probability of getting the A whether you pick first or last.  In general, to win on the kth pick gives
	<me> P(\text{kth}) = P(\text{LL...LA}) \\ = \frac{14 \cdot 13 \cdot ... \cdot (15-k)  \cdot 1}{15 \cdot 14 ... \cdot (16-k) \cdot (15-k)} = \frac{1}{15}</me>
	Hence, it is the same probability regardless of when you get to pick.
	</p>
	
	
	<p>
	If there are two A's possible, then the options for person k in include either receiving the first of the two slips or the second. The probability for determining the first of the two is computed in a manner similar to above except that there is one more A and one less other.
	<me> P(\text{kth as first}) = P(\text{LL...LA}) \\ = \frac{13 \cdot 12 \cdot ... \cdot (15-k)  \cdot 2}{15 \cdot 14 ... \cdot (16-(k+1)) \cdot (16-k)} = \frac{2 \cdot (15-k)}{15 \cdot 14}</me>

	The probability of getting the second A means exactly one of the previous k-1 selections also picked the other A. There are k-1 ways that this could happen. Computing for one of the options and multiplying by k-1 gives
	<me> P(\text{kth as second}) = P(\text{LL...LAA}) \\ = (k-1) \cdot \frac{13 \cdot 12 \cdot ... \cdot (15-k) \cdot 2 \cdot 1}{15 \cdot 14 ... \cdot (16-k) \cdot (15-k)} = \frac{2 \cdot (k-1)}{15 \cdot 14}.</me>	
	Adding these two together gives
	<me>P(\text{getting an A when there are two}) \\ = \frac{2 \cdot (15-k) + 2 \cdot (k-1)}{15 \cdot 14} = \frac{28}{15 \cdot 14} = \frac{2}{15}.</me>

	For example, if k = 5,
	<me> P(\text{5th as first}) = P(\text{LLLLA}) \\ = \frac{13 \cdot 12 \cdot 11 \cdot 10  \cdot 2}{15 \cdot 14 \cdot 13 \cdot 12 \cdot 11} = \frac{20}{15 \cdot 14}</me>
	<me> P(\text{5th as second}) = P(\text{LL...LAA}) \\ = 4 \cdot \frac{13 \cdot 12 \cdot \cdot 11 \cdot 2 \cdot 1}{15 \cdot 14 \cdot 13 \cdot 12 \cdot 11} \\ = \frac{8}{15 \cdot 14}.</me>	
	Adding these together yields the general result. So, once again, it doesn't matter which pick you use since the likelihood of getting an A is the same for all positions.
	</p>
	</solution>
</exercise>

<exercise><title>Shared Birthdays</title>
	<statement>
	<p>
	In this problem, you want to consider how many people are necessary in order to have an even chance of finding two or more who share a common birthday. Toward that end, assuming a year has exactly 365 equally likely days let r be the number of people in a sample and consider the following:
	<ol>
		<li>Determine the number of different outcomes of birthdays when order matters and birthdays are allowed to be repeated.</li>
		<li>Determine the number of different outcomes when birthdays are not allowed to be repeated.</li>
		<li>Determine the probability that two or more of your r students have the same birthday.</li>
		<li>Prepare a spreadsheet with the probabilities found above from r=2 to r=50. Determine the value of r for which this probability is closest to 0.5.</li>
		<li>As best as you can, sample two groups of the size found above and gather birthday information. For each group, determine if there is a shared birthday or not.  Compare your results with others in the class to check whether the sampling validates that about half of the samples should have a shared birthday group.</li>
	</ol>
	</p>
	</statement>
	
	<solution>
	<p>
	The correct sample size to get past a probability of 0.5 is 23 people. You should justify this numerically by justifying the following probabilities:
<pre>
#	P(Match)	
1	0
2	0.0027
3	0.0082
4	0.0164
5	0.0271
6	0.0405
7	0.0562
8	0.0743
9	0.0946
10	0.1169
11	0.1411
12	0.1670
13	0.1944
14	0.2231
15	0.2529
16	0.2836
17	0.3150
18	0.3469
19	0.3791
20	0.4114
21	0.4437
22	0.4757
23	0.5073
24	0.5383
25	0.5687
26	0.5982
27	0.6269
28	0.6545
29	0.6810
30	0.7063
</pre>
	</p>
	</solution>
</exercise>

<exercise><title>Internet meme solution</title>	
	<statement>	
	<p>
	This one is from an internet meme:  Two fair 6-sided dice are rolled together and you are told that at least one of the dice is a 6. Given that a 6 will be removed, determine the probability that the other die is a 6.
	</p>
	</statement>
	
	<solution>
	<p>
	In this case, you are presented with an outcome where the possible choices consist of (1,6), (2,6), (3,6), (4,6), (5,6), (6,6), (6,5), (6,4), (6,3), (6,2), (6,1).  Each of these would satisfy the condition that at least one of the dice is a 6. From this group, the only success that satisfies being a 6, given that another 6 has already been removed, is the (6,6) outcome. Therefore, the conditional probability is 1/11.
	</p>
	<p>
	It is interesting to note that if the question instead was posed so that one of the dice was a 6 and it was removed, then the probability of the other dice showing a 6 would be 1/6.
	</p>
	
	</solution>
</exercise>

<exercise><title>100 people on an airplane with boarding pass issues</title>
	<statement>
	<p>
	This is a famous problem.  100 people are in line, boarding an airplane with 100 seats, one at a time. They are in no particular order. The first person has lost his boarding pass, so he sits in a random seat. The second person does the following:

	<ul>
	<li>Goes to his seat (the one it says to go to on the boarding pass). If unoccupied, sit in it.</li>
	<li>If occupied, find a random seat to sit in.</li>
	</ul>
	Everyone else behind him does the same. What is the probability that the last person sits in his correct seat?
	</p>
	</statement>

	<solution>
	<p>
	To get the idea, consider what happens with only 2 people, then only 3. Generalize. 
	</p>
	<p>
	The answer is 1/2. To obtain this, you can define recursively the probability that the kth person sits in their own set as f(k).  Consider the first traveler's and your seats. Then you get the following cases:
	<ul>
		<li>P(first guy sits in his own seat and you sit in yours) = \frac{1}{k} \cdot 1</li>
		<li>P(first guy sits in your seat and you do not sit in yours) = \frac{1}{k} \cdot 0</li>
		<li>P(other k-2 travelers make their choices) = <m>(k-2) \frac{1}{k} f(k-1)</m></li>
	</ul>
	<me>f(k) = 1/k + 0 + (k-2)/k f(k-1)</me>
	with f(2) = 1/2.
	</p>
	<p>
	For example,
	f(3) = 1/3 + f(2)/3 = 1/3 + 1/6 = 1/2.   
	f(4) = 1/4 + 2/4 f(3) = 1/4 + 1/2 1/2 = 1/2.
	f(5) = 1/5 + 3/5 1/2 = 1/2.
	f(6) = 1/6 + 4/6 1/2 = 1/2. Etc.
	</p>
	</solution>	
</exercise>
</subsection>



</section>
	
<section><title>Bayes Theorem</title>	
	<introduction>
	<p>Conditional probabilities can be computed using the methods developed above if the appropriate information is available. Some times you will however have some information available, such as <m>P(A | B)</m> but need <m>P(B | A)</m>. The ability to "play around with history" by switching what has been presumed to occur leads to the following.
	</p>
	</introduction>

	<theorem>
	<title>Bayes Theorem</title>
	<statement>Let <m>S = \{ S_1, S_2, ... , S_m \}</m> where the <m>S_k</m> are pairwise disjoint and <m>S_1 \cup S_2 \cup ... \cup S_m = S</m> (i.e. a partition of the space S).  Then for any <m>A \subset S</m>
	<md>
		<mrow>P(S_j | A) = \frac{P(S_j)P(A | S_j)}{\sum_{k=1}^m P(S_k)P(A | S_k)}.</mrow>
	</md>
	The conditional probability <m>P(S_j | A)</m> is called the posterior probability of <m>S_k</m>.
	</statement>
	<proof>
	<p>
	Notice, by the definition of conditional probability and the multiplication rule
	<me>P(S_j | A) = \frac{P(S_j \cap A)}{P(A)} = \frac{P(S_j)P( A | S_j)}{P(A)}.</me>
	But using the disjointness of the partition 
	<md>
	<mrow>P(A) &amp; = P( (A \cap S_1) \cup (A \cup S_2) \cup ... \cup (A \cup S_m) )</mrow>
	<mrow>    &amp; = P(A \cap S_1) + P(A \cup S_2) + ... + P(A \cup S_m)</mrow>
	<mrow>    &amp; = P(S_1 \cap A) + P(S_2 \cup A) + ... + P(S_m \cup A)</mrow>
	<mrow>    &amp; = P(S_1) P(A | S_1) + P(S_2)P(A | S_2) + ... + P(S_m)P(A | S_m)</mrow>
	<mrow>    &amp; = \sum_{k=1}^m P(S_k)P(A | S_k)</mrow>
	</md>
	Put these two expansions together to obtain the desired result.
	</p>
	</proof>
	</theorem>
	
	<p>
	To illustrate this result, from the web site <url href="http://stattrek.com/probability/bayes-theorem.aspx"></url> consider the following problem:
	</p>
<exercise>
	<p>
	Marie is getting married tomorrow, at an outdoor ceremony in the desert. In recent years, it has rained only 5 days each year. Unfortunately, the weatherman has predicted rain for tomorrow. When it actually rains, the weatherman correctly forecasts rain 90% of the time. When it doesn't rain, he incorrectly forecasts rain 10% of the time. What is the probability that it will rain on the day of Marie's wedding?
	</p>
	<p>
	Notice, all days can be classified into one of two disjoint options:
	<ul>
		<li>Rainy, in which case we can deduce from the given info that P(Rain) = 5/365</li>
		<li>No Rainy, and since this is the complement of above, P(No Rain) = 360/365</li>
	</ul>
	In the notation of Bayes Theorem, let A represent a forecast of Rain and note you have <m>P(\text{Rain}) = P(S_1) = \frac{5}{365}</m> and <m>P(\text{No Rain}) = \frac{360}{365}</m>. Further, you are given the conditional probabilities
	<ul>
		<li><m>P(\text{Rain | Forecast Rain}) = P( A | S_1) = 0.9</m></li>
		<li><m>P(\text{Rain | Forecast Rain}) = P( A | S_2) = 0.1</m></li>
	</ul>
	Notice that the question provided requests that you find the probability of Rain given that the weatherman has forecasted rain. What is given on the other hand is the reverse of that conditional probability. Using Bayes Theorem allows you to turn this around...
	<md>
		<mrow>P(\text{Rain}) &amp;  = P(S_1) P( A | S_1) + P(S_2) P(A | S_2)</mrow>	
		<mrow> &amp; = \frac{5}{365} \cdot 0.9 + \frac{360}{365} \cdot 0.1</mrow>
		<mrow> &amp; = NUMBER </mrow>
	</md>
	Hence, putting these together gives
	<md>
		<mrow>P(\text{Rain | Forecast Rain}) &amp; = \frac{\frac{5}{365} \cdot 0.9}{\frac{5}{365} \cdot 0.9 + \frac{360}{365} \cdot 0.1}</mrow>
		<mrow> &amp; = \frac{5 \cdot 0.9}{5 \cdot 0.9 + 360 \cdot 0.1}</mrow>
		<mrow> &amp; = \frac{45}{45+360} \approx 0.111</mrow>
	</md>
	So, normally there is only a 5 percent chance of rain on a given day but given that the weatherman has forecast rain, the chance of rain has risen to a little more than 11 percent.
	</p>	
</exercise>	


<exercise>
	<introduction>
	<p>
	Let's try a Bayes Theorem example...
	</p>
	</introduction>
	<webwork source="Library/Mizzou/Finite_Math/Probability_Bayes_Theorem/Bayes1.pg">
	</webwork>
	<conclusion>
	<p>
	You have to be careful to extract the conditional probabilities from the problem.
	</p>
	</conclusion>
</exercise>

<exercise>
	<introduction>
	<p>
	Here is a more extensive Bayes Theorem example...
	</p>
	</introduction>
	<webwork source="Library/UVA-Stat/setStat212-Homework04/stat212-HW04-16.pg">
	</webwork>
	<conclusion>
	<p>
	Notice that having the data expressed in tabular form sometimes makes it easier to deal with.
	</p>
	</conclusion>
</exercise>


<p>The interactive cell below can be used to easily compute all of the conditional probabilities associated with Bayes's Theorem. Notice how the relative size of the pie-shaped partition changes when you presume that an event in the space has already occurred.</p>
<sage>
<input>

#  This function is used to convert an input string into separate entries
def g(s): return str(s).replace(',',' ').replace('(',' ').replace(')',' ').split()

@interact
def _(Partition_Probabilities=input_box('0.35,0.25,0.40',label="$P(B_1),P(B_2),...$"),
        Conditional_Probabilities=input_box('0.02,0.01,0.03',label='$P(A|B_1),P(A|B_2),...$'),
        print_numbers=checkbox(True,label='Numerical Results on Graphs?'),
        auto_update=False):
            
    Partition_Probabilities = g(Partition_Probabilities)
    Conditional_Probabilities = g(Conditional_Probabilities)
    n = len(Partition_Probabilities)
    n0 = len(Conditional_Probabilities)
    
    # below needs to be n not equal to n0 but mathbook xml will not let me get the other
    if (n > n0):
        pretty_print("You must have the same number of partition probabilities and conditional probabilities.")
        
    else:                               # input data streams now are the same size!
        colors = rainbow(n)
        accum = float(0)                # to test whether partition probs sum to one
        ends = [0]                      # where the graphed partition sectors change in pie chart 
        mid = []                        # middle of each pie chart sector used for placement of text
        p_Bk_given_A = []               # P( B_k | A )
        pA = 0                          # P(A)
        PP=[]                           # array to hold the numerical Partition Probabilities 
        CP=[]                           # array to hold the numerical Conditional Probabilities     
        for k in range(n):
            PP.append(float(Partition_Probabilities[k]))
            CP.append(float(Conditional_Probabilities[k]))    
            p_Bk_given_A.append(PP[k]*CP[k] )
            pA += p_Bk_given_A[k]
            accum = accum + PP[k]
            ends.append(accum)
            mid.append((ends[k]+accum)/2)
#
#  Marching along from 0 to 1, saving angles for each partition sector boundary.
#  Later, we will multiple these by 2*pi to get actual sector boundary angles.
#
        if abs(accum-float(1))>0.0000001:     #  Due to roundoff issues, this should be close enough.                     
            pretty_print("Sum of probabilities should equal 1.")
        
        else:                           # probability data is sensible
 
#        
#  Draw the Venn diagram by drawing sectors from the angles determined above
#  First, create a circle of radius 1 to illustrate the the sample space S
#  Then draw each sector with varying colors and print out their names on the edge
#
            G = circle((0,0), 1, rgbcolor='black',fill=False, alpha=0.4,aspect_ratio=True,axes=False,thickness=5)
            for k in range(n):
                G += disk((0,0), 1, (ends[k]*2*pi, ends[k+1]*2*pi), color=colors[mod(k,10)],alpha = 0.2)
                G += text('$B_'+str(k+1)+'$',(1.1*cos(mid[k]*2*pi), 1.1*sin(mid[k]*2*pi)), rgbcolor='black')
                
            G += circle((0,0), 0.6, facecolor='yellow', fill = True, alpha = 0.1, thickness=5,edgecolor='black') 
    
#  Print the probabilities corresponding to each particular region as a list and on the graphs
            if print_numbers:               

                html("$P(A) = %s$"%(str(pA),))
                for k in range(n):
                    html("$P(B_{%s} | A)$"%(str(k+1))+"$ = %s$"%str(p_Bk_given_A[k]/pA))
                                        
                    G += text(str(p_Bk_given_A[k]),(0.4*cos(mid[k]*2*pi), 0.4*sin(mid[k]*2*pi)), rgbcolor='black')
                    G += text(str(PP[k] - p_Bk_given_A[k]),(0.8*cos(mid[k]*2*pi), 0.8*sin(mid[k]*2*pi)), rgbcolor='black')
        
#  This is essentially a repeat of some of the above code but focused only on creating the smaller inner circle dealing
#  with the set A so that the sectors now correspond in area to the Bayes Theorem probabilities


            accum = float(0)                        
            ends = [0]                     # where the graphed partition sectors change in pie chart 
            mid = []                       # middle of each pie chart sector used for placement of text
            for k in range(n): 
                accum += float(p_Bk_given_A[k]/pA) 
                ends.append(accum)
                mid.append((ends[k]+accum)/2)
            H = circle((0,0), 1, rgbcolor='black',fill=False, alpha=0,aspect_ratio=True,axes=False,thickness=0)
            H += circle((0,0), 0.6, facecolor='yellow',fill=True, alpha=0.1,aspect_ratio=True,axes=False,thickness=5,edgecolor='black')
            
            for k in range(n):
                H += disk((0,0), 0.6, (ends[k]*2*pi, ends[k+1]*2*pi), color=colors[mod(k,10)],alpha = 0.2)
                H += text('$B_'+str(k+1)+'|A$',(0.7*cos(mid[k]*2*pi), 0.7*sin(mid[k]*2*pi)), rgbcolor='black')
                    
        #  Now, print out the bayesian probabilities using the smaller set A only
    
            if print_numbers:
                for k in range(n):
                    H += text(str( N(p_Bk_given_A[k]/pA,digits=4) ),(0.4*cos(mid[k]*2*pi), 0.4*sin(mid[k]*2*pi)), rgbcolor='black')
                    
            G.show(title='Venn diagram of partition with A in middle')
            print
            H.show(title='Venn diagram presuming A has occured')
</input>
</sage>




<subsection><title>Exercises - Bayes Theorem</title>

<p>Bayes Theorem</p>


<exercise><title>Insured vs Accident</title>
	<statement>
	<p>
	Your automobile insurance company uses past history to determine how to set rates by measuring the number of accidents caused by clients in various age ranges. The following table summarizes the proportion of those insured and the corresponding probabilities by age range:

	<table halign="left">
      	<tabular halign="right">
      
<row><cell bottom="medium" right="medium">Age</cell><cell bottom="medium" right="medium">Proportion of Insured</cell><cell bottom="medium">Probability of Accident</cell></row>      
<row><cell right="medium">16-20</cell><cell right="medium">0.05</cell><cell>0.08</cell></row>
<row><cell right="medium">21-25</cell><cell right="medium">0.06</cell><cell>0.07</cell></row>
<row><cell right="medium">26-55</cell><cell right="medium">0.49</cell><cell>0.02</cell></row>
<row><cell right="medium">55-65</cell><cell right="medium">0.25</cell><cell>0.03</cell></row>
<row><cell right="medium">over 65</cell><cell right="medium">0.15</cell><cell>0.04</cell></row>
		</tabular>
	</table>
	
	One of your family friends insured by this company has an accident. 
	<ol>
		<li>Determine the conditional probability that the driver was in the 16-20 age range.</li>
		<li>Compare this to the probability that the driver was in the 18-20 age range. Discuss the difference.</li>
		<li>Determine how much more the company should charge for someone in the 16-20 age range compared to someone in the 26-55 age range.</li>
	</ol>
	</p>
	</statement>
</exercise>
	
<exercise><title>Spinal bifida odds</title>
	<statement>
	<p>
	Congratulations...your family is having a baby! As part of the prenatal care, some testing is part of the normal procedure including one for spinal bifida (which is a condition in which part of the spinal cord may be exposed.) Indeed, measurement of maternal serum AFP values is a standard tool used in obstetrical care to identify pregnancies that may have an increased risk for this disorder. You want to make plans for the new child's care and want to know how serious to take the test results. However, some times the test indicates that the child has the disorder when in actuality it does not (a false positive) and likewise may indicate that the child does not have the disorder when in fact it does (a false negative.) 
	</p>
	<p>The combined accuracy rate for the screen to detect the chromosomal abnormalities mentioned above is approximately 85% with a false positive rate of 5%. This means that (from <url href="http://americanpregnancy.org/prenatal-testing/first-trimester-screen/">americanpregnancy.org</url>)
	<ul>
		<li>Approximately 85 out of every 100 babies affected by the abnormalities addressed by the screen will be identified. (Positive Positive)</li>
		<li>Approximately 5% of all normal pregnancies will receive a positive result or an abnormal level. (False Positive)</li>
	</ul>
	<ol>
		<li>Given that your test came back negative, determine the likelihood that the child will actually have spinal bifida.</li>
		<li>Given that your test came back negative, determine the likelihood that the child will not have spina bifida</li>
		<li>Given that a positive test means you have a 1/100 to 1/300 chance of experiencing one of the abnormalities, determine the likelihood of spinal bifida in a randomly selected child.</li>
	</ol>
	</p>
	</statement>
</exercise>

</subsection>


</section>







<section><title>Independence</title>

	<introduction>
	<p>You have seen when repeatedly sampling without replacement leads to a change the the likelihood of some event in successive trials. Indeed, this is what conditional probabilities above illustrate. However, when sampling with replacement you may find a different situation arises. Indeed, you easily notice that when flipping a coin, P(Heads) = 1/2 regardless of the outcome of any previous flip.  In situations such as this where the probability of an event is not affected by the occurrence (or lack of occurrence) of some other event determining the probability of compound events can be greatly simplified.
	</p>
	</introduction>
	
	<definition>
	<title>Independent Events</title>
		<statement>Events A and B are independent provided 
		<me>P(A \cap B) = P(A) P(B)</me>
		</statement>
	</definition>
	
	<corollary><title>Independence and Conditional Probability</title>
		<statement>
			Given independent events A and B, 
			<me>P(B | A) = P(B)</me> and <me>P(A | B) = P(A).</me>
		</statement>
		<proof>
			<p>By the multiplication rule and the definition of independence, for any events A and B
			<me>P(A) \cdot P(B) = P(A \cap B) = P(A) \cdot P(B | A) .</me>
			Therefore, if P(A) is non-zero, canceling yields the first result. Switching around notation provides the second.
			</p>
		</proof>
	</corollary>

	<exercise>
		<introduction>
		<p>
		Independence makes combined probabilities VERY easy to compute.
		</p>
		</introduction>
		<webwork source="Library/Rochester/setProbability4Conditional/ur_pb_4_8.pg">
		</webwork>
		<conclusion>
		<p>
		Basically you just multiply individual probabilities together.  Independence is often
		assumed since it makes computations easier. That said, you should remember to consider
		each time whether independence should or should not be assumed.
		</p>
		</conclusion>
	</exercise>

	<corollary><title>Independence and Mutual Exclusivity</title>
		<statement>
			If events A and B are both independent and mutually exclusive, then at least one of them has zero probability.
		</statement>
		<proof>
			<p>
			By independence, <m>P(A \cap B) = P(A) \cdot P(B)</m>. However, by mutually exclusivity, <m>A \cap B = \emptyset \Rightarrow P(A \cap B) = 0</m> gives
			<me>P(A) \cdot P(B) = 0.</me>
			Hence, one or the other (or both) must be zero.
			</p>
			</proof>
	</corollary>

	<corollary><title>Successive Independent Events</title>
		<statement>Given a sequence of mutually independent events <m>A_1, A_2, A_3, ...</m>,
		<me>P(\cap_{k \in R} A_k) = \prod_{k \in R} P(A_k)</me>
		</statement>
	</corollary>


<subsection><title>Exercises - Independence</title>
<exercise><title>Basic Independence Calculations</title>
	<statement>
	<p>
	Given P(A) = 0.43, P(B) = 0.72, and <m>P(A \cap B) = 0.29</m>, verify that A and B are not independent.
	</p>
	</statement>
</exercise>

<exercise><title>Compound events and Independence</title>
	<statement>
	<p>
	Given A, B, and C are independent events, with P(A) = 2/5, P(B) = 3/4, and P(C) = 1/6, determine:
	<ol>
		<li><m>P(A \cap B \cap C)</m></li>
		<li><m>P(A^c \cap B^c \cap C)</m></li>
		<li><m>P(A \cup B \cup C)</m></li>
	</ol>
	</p>
	</statement>
</exercise>
	
<exercise><title>Rolling multiple dice</title>
	<statement>
	<p>
	Suppose for a pair of dice you want to consider the events A = {rolling a 7 or 11} and B = {otherwise}.  Rolling the dice 5 times, determine
	<ol>
		<li>P(AABBB)</li>
		<li>P(BBBAA)</li>
		<li>The probability of getting A on exactly two rolls of the dice.</li>
	</ol>
	</p>
	</statement>
</exercise>

<exercise><title>Redundancy</title>
	<statement>	
	<p>
	To help "insure" the success of a mission, you propose several redundant components so that the mission is a success if one or more succeed. Supposing that these separate components act independently of each other and that each component has a 75% chance of success, determine:
	<ol>
		<li>The probability of failure if you utilize 2 components.</li>
		<li>The probability of failure if you utilize 5 components.</li>
		<li>The number of components needed to insure that the probability of success is at least 99%.</li>
	</ol>
	</p>
	</statement>
</exercise>

<exercise><title>Internet Meme redux</title>
	<statement>	
	<p>
	Again, from an internet meme:  Two fair 6-sided dice are rolled together and you are told that at least one of the dice is a 6. A 6 is removed and you are presented with the other die.  Determine the probability that it is a 6.
	</p>
	</statement>
	<solution> 
	<p>
	For this setting, notice that the outcomes from each of the two dice are independent of each other. Removing one of the dice, regardless of it's value, does not affect the other. The question in this case does not ask for a conditional probability.
	</p>
	</solution>
</exercise>

<exercise><title>Single Elimination Tounament</title>
	<statement>
	<p>
	Consider a n=4 team single-elimination tournament where the teams are "seeded" from 1 (the best team) to 4 (the worst team).  For this tournament, team 1 plays team 4 and team 2 plays team 3. The winner of each play each other to determine the final winner. When teams j and k play, set P(j wins) = <m>\frac{k}{j+k}</m> and similarly for team k.  Assuming separate games are independent of each other, determine the probability that team 4 wins the tournament. What about with 8 teams? What about 64 teams?
	</p>
	</statement>
	<solution>
	<p>
	P(4 wins) = P(4 beats 1) P(4 beats the winner of the other bracket)
	</p>
	<p>
	P(4 wins) = (1/5) * P(4 beats 2 | 2 beats 3) + P(4 beats 3 | 3 beats 2)
	</p>
	<p>
	P(4 wins) = 1/5 [(3/5)(2/6) + (2/5)(3/7)] = 78/1050 = 0.0742
	</p>
	<p>
	For the other teams:
	</p>
	<p>
	P(1 wins) = 4/5 [(3/5)(2/3) + (2/5)(3/4) ] = 0.56
	</p>
	<p>
	P(2 wins) = 3/5 [(4/5)(1/3) + (1/5)(4/6) ] = 0.24
	</p>
	<p>
	P(3 wins) = 2/5 [(4/5)(1/4) + (1/5)(4/7) ] = 0.1257
	</p>
	</solution>
</exercise>
</subsection>



		
</section>

</chapter>