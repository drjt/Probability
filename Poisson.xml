<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="PoissonExponentialGamma" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Distributions based upon Poisson Processes</title>

<section><title>Introduction</title>
<p>
In this chapter, you will investigate the relationship between number of successes over some interval.  For each, one of these quantities will be fixed and the other one variable. First, consider the following:
</p>

<p>
<definition><title>Poisson Process</title>
<statement>
<p>
A Poisson process is a course of action in which:
<ol>
	<li>Successes in non-overlapping subintervals are independent of each other.</li>
	<li>The probability of exactly one success in a sufficiently small interval of length h is proportional to h.  In notation, P(one success) = <m>\lambda h</m>.</li>
	<li>The probability of two or more successes in a sufficiently small interval is essentially 0.</li>
</ol>
</p>
</statement>
</definition>
</p>

<p>
You should presume these assumptions implicitly for the distributions discussed in this chapter.
</p>
<p>
In this chapter, you will investigate the following distributions:
<ol>
	<li>Poisson - the interval is fixed and X measures the variable number of successes.</li>
	<li>Exponential - the number of successes is fixed--at 1--and X measures the variable interval length needed to get that success.</li>
	<li>Gamma - the number of successes is fixed and X measures the variable interval needed to get the desired number of successes.</li>
</ol>
</p>

</section>

<section><title>Poisson Distribution</title>
<p>
Consider a Poisson Process where you start with an interval of fixed length T and where X measures the variable number of successes, or changes, within that interval. The resulting distribution of X will be called a Poisson distribution.
</p>

<p>
<theorem><title>Poisson Probability Function</title>
<statement>
<p xml:id="PoissonPF">Assume X measures the number of successes in an interval [0,T] within some Poisson process. Then, 
<me>f(x) = \frac{\mu^x e^{-\mu}}{x!}</me>
for <m>R = \{ 0, 1, 2, ... \}</m>.
</p>
</statement>
<proof>
<p>
For a sufficiently large natural number n, break up the given interval [0,T] into n uniform parts each of width h = T/n.  Using the properties of Poisson processes, n very large implies h will be very small and eventually small enough so that 
<me>P(\text{exactly one success on a given interval}) = p = \lambda \frac{T}{n}.</me> 

However, since there are a finite number of independent intervals each with probability p of containing a success then you can use a Binomial distribution to evaluate the corresponding probabilities so long as n is finite. Doing so yields and taking the limit as n approaches infinity gives:
<md>
	<mrow>f(x) &amp; = P(\text{x changes in [0,T]}) </mrow>
	<mrow> &amp; = \lim_{n \rightarrow \infty} \binom{n}{x} p^x (1-p)^{n-x}</mrow>
	<mrow> &amp; = \lim_{n \rightarrow \infty} \binom{n}{x} (\frac{\lambda T}{n})^x (1-\frac{\lambda T}{n})^{n-x}</mrow>
	<mrow> &amp; = \lim_{n \rightarrow \infty} \frac{n(n-1)...(n-x+1)}{x!} ( \frac{\lambda T}{n})^x (1- \frac{\lambda T}{n})^{n-x}</mrow>
	<mrow> &amp; = \frac{(\lambda T)^x}{x!} \lim_{n \rightarrow \infty} \frac{n(n-1)...(n-x+1)}{n \cdot n \cdot ... \cdot n} (1-\lambda \frac{T}{n})^{n}(1-\lambda \frac{T}{n})^{-x}</mrow>
	<mrow> &amp; = \frac{(\lambda T)^x}{x!} \lim_{n \rightarrow \infty} (1-\frac{1}{n})...(1-\frac{x-1}{n})  (1- \frac{\lambda T}{n})^{n}(1- \frac{\lambda T}{n})^{-x}</mrow>
	<mrow> &amp; = \frac{(\lambda T)^x}{x!} 
	\lim_{n \rightarrow \infty} (1- \frac{\lambda T}{n})^{n}
	\lim_{n \rightarrow \infty} (1- \frac{\lambda T}{n})^{-x}</mrow>
	<mrow> &amp; = \frac{(\lambda T)^x}{x!} 
	\lim_{n \rightarrow \infty} (1- \frac{\lambda T}{n})^{n} \cdot 1</mrow>
	<mrow> &amp; = \frac{(\lambda T)^x}{x!} 
	e^{-\lambda T}</mrow>
</md>
</p>
</proof>
</theorem>
</p>

<p>
<theorem><title>Verify Poisson Probability Function</title>
<statement>
<p>
	<me>\sum_{x=0}^{\infty} \frac{(\lambda T)^x}{x!} e^{-\lambda T} = 1</me>
</p>
</statement>
<proof>
<p>Using the Power Series expansion for the natural exponential,
<md>
	<mrow> \sum_{x=0}^{\infty} f(x) &amp; = \sum_{x=0}^{\infty} \frac{(\lambda T)^x}{x!} e^{-\lambda T} </mrow>		
	<mrow> &amp; = e^{-\lambda T} \sum_{x=0}^{\infty} \frac{(\lambda T)^x}{x!} </mrow>
	<mrow> &amp; = e^{-\lambda T} e^{\lambda T}  </mrow>
	<mrow> &amp; = 1</mrow>
</md>
</p>
</proof>
</theorem>
</p>

<p>
<example><title>Router Requests</title>
<p>
Everyone using the internet utilizes a series of "routers" who spend their time waiting for someone to show up and ask for something to be done. Let's consider one such router which, over time, has been shown to receive on average 1000 such requests in any given 10 minute period during regular working hours. In general, a Poisson process with mean 1000 would seem to fit and therefore the Poisson distribution would be a good model. We will find out below that <m>\lambda T = \mu = 1000</m> and will use that here to get 
<me> f(x) = \frac{1000^x}{x!} e^{-1000}.</me>
</p>
<p>
So, suppose we would like to know the likelihood of receiving exactly 1020 requests in a 10 minute time interval.  This means we need
<me>P(X = 1020) = f(1020) = \frac{1000^{1020}}{1020!} e^{-1000}</me>
which might be totally impossible to compute directly using a regular calculator. However, many graphing calculators have a built-in function where f(x) = poissonpdf(mu,x) and F(x) = poissoncdf(mu,x). To answer our question,
<me> f(1020) = \text{poissonpdf(1000,1020)} \approx 0.01024.</me>

On the other hand, suppose the question is to ask whether 1020 or fewer requests wil be made in the 10 minute interval. If so, then
<me> F(1020) = \text{poissoncdf(1000,1020)} \approx 0.74258.</me> 
</p>
</example>
</p>

<p>
<exercise><title>WebWork</title>
		<introduction>
		<p>
		This one uses the poisson distribution.
		</p>
		</introduction>
		<webwork source="Library/UBC/STAT/STAT302/HW06/HW06-03.pg">
		</webwork>
</exercise>
</p>


<p>
<theorem><title>Statistics for Poisson</title>
<statement>
<p>
<me>\mu = \lambda T</me>
<me>\sigma^2 = \mu</me>
<me>\gamma_1 = \frac{1}{\sqrt{\mu}}</me>
<me>\gamma_2 = \frac{1}{\mu}+3</me>
</p>
</statement>
<proof>
<p> Using the f(x) generated in the previous theorem
<md>
	<mrow>\mu &amp; = E[X] </mrow>
	<mrow> &amp; = \sum_{x=0}^{\infty} x \cdot \frac{(\lambda T)^x}{x!} e^{-\lambda T}</mrow>
	<mrow> &amp; = \lambda T e^{-\lambda T} \sum_{x=1}^{\infty} \frac{(\lambda T)^{x-1}}{(x-1)!} </mrow>
	<mrow> &amp; = \lambda T e^{-\lambda T} \sum_{k=0}^{\infty} \frac{(\lambda T)^k}{k!} </mrow>
	<mrow> &amp; = \lambda T e^{-\lambda T} e^{\lambda T} </mrow>
	<mrow> &amp; = \lambda T </mrow>
</md>
which confirms the use of <m>\mu</m> in the original probability formula.
</p>
<p>
Continuing with <m>\mu = \lambda T</m>, the variance is given by
<md>
	<mrow>\sigma^2 &amp; = E[X(X-1)] + \mu - \mu^2 </mrow>
	<mrow> &amp; = \sum_{x=0}^{\infty} x(x-1) \cdot \frac{\mu^x}{x!} e^{-\mu} + \mu - \mu^2</mrow>
	<mrow> &amp; = e^{-\mu} \mu^2 \sum_{x=2}^{\infty} \frac{\mu^{x-2}}{(x-2)!} + \mu - \mu^2</mrow>
	<mrow> &amp; = e^{-\mu} \mu^2 \sum_{k=0}^{\infty} \frac{\mu^k}{k!} + \mu - \mu^2</mrow>
	<mrow> &amp; = \mu^2 + \mu - \mu^2 </mrow>
	<mrow> &amp; = \mu</mrow>
</md>

To derive the skewness and kurtosis, you can depend upon Sage...see the live cell below.
</p>

</proof>
</theorem>
</p>

<p>
<sage>
	<input>
var('x,mu')
assume(x,'integer')

f(x) =e^(-mu)*mu^x/factorial(x)
mu = sum(x*f,x,0,oo).factor()
M2 = sum(x^2*f,x,0,oo).factor()
M3 = sum(x^3*f,x,0,oo).factor()
M4 = sum(x^4*f,x,0,oo).factor()

pretty_print('Mean = ',mu)

v = (M2-mu^2).factor()
pretty_print('Variance = ',v)
stand = sqrt(v)

sk = ((M3 - 3*M2*mu + 2*mu^3)).factor()/stand^3
pretty_print('Skewness = ',sk)

kurt = (M4 - 4*M3*mu + 6*M2*mu^2 -3*mu^4).factor()/stand^4
pretty_print('Kurtosis = ',(kurt-3).factor(),'+3')
	</input>
</sage>
</p>	

<p>
<sage language='r'>
<input>
mu = 3           # the mean must be given
sdev = sqrt(mu)  # the formula for the standard deviation

M = mu*6   # the space is infinite but we just go out 6 standard deviations
X = 0:M    # part of the space R of the random variable 
if(M &lt; 101){
dpois( X, mu, log = FALSE )   # let's print out a bunch of actual probs if M reasonable
}

Ppois = dpois(X, mu )  # create the probability function over X

Psample = rpois(10^6, mu)  # to create a histogram, sample a lot
Xtop=max(Psample)          # for scaling the x-axis. Shift by 1/2 below.
hist(Psample, prob=TRUE, br=(-1:Xtop)+0.5, col="skyblue2", xlab="X", 
  main="Poisson Probability Function vs Approximating 'Bell Curve'")

points(X, Ppois, pch=19, col="darkgreen")  # to create actual (x,f(x))

Pnormal &lt;- function(X){dnorm(X, mean=mu, sd=sdev)}   # to overlap a bell curve
curve(Pnormal, col="red", lwd=2, add=TRUE) 
</input>
</sage>
</p>
</section>



<section><title>Exponential Distribution</title>
	<p>
Once again, consider a Poisson Process where you start with an interval of variable length X so that X measures the interval needed in order to obtain a first success with <m>R = (0,\infty)</m>. The resulting distribution of X will be called an Exponential distribution.
	</p>


<p>
To derive the probability function for this distribution, consider finding f(x) by first considering F(x). This gives

<md>
	<mrow>F(x)&amp;  = P(X \le x)</mrow> 
	<mrow> &amp; = 1 - P(X \gt x)</mrow>
	<mrow> &amp; = 1 - P(\text{first change occurs after an interval of length x})</mrow>
	<mrow> &amp; = 1 - P(\text{no changes in the interval [0,x]})</mrow>
	<mrow> &amp; = 1 - \frac{(\lambda x)^0 e^{-\lambda x}}{0!}</mrow>
	<mrow> &amp; = 1 - e^{-\lambda x}</mrow>
</md>
where the discrete Poisson Probability Function is used to answer the probability of exactly no changes in the "fixed" interval [0,x]. 
</p>
<p>
Using this distribution function and taking the derivative yields
<me>f(x) = F'(x) = \lambda e^{-\lambda x}.</me>
</p>

<p>
We found that if <m>\lambda</m> is the parameter for the Poisson process, then the Poisson distribution had mean <m>\lambda T</m> or if one presumed that T=1, then for that distribution, the mean is just <m>\lambda</m>.  We will find out below that the mean for the exponential distribution will be <m>\frac{1}{\lambda}</m>. Therefore, we will eventually present this formula using the exponental mean <m>\mu = \frac{1}{\lambda}</m> rather than using <m>\lambda</m> in the actual formula. 
</p>
<p>
You will also often find that exercises in other textbooks and in online WeBWorK will just provide <m>\lambda</m> for the underlying Poisson process and that the time interval T will be presumed to be T=1.  For those problems, if the exercise asks for a Poisson probability, use <m>\mu = \lambda</m> while if the exercise asks for an Exponential probability, use <m>\mu = \frac{1}{\lambda}</m>.
</p>

<p>
<theorem><title>Verification of Exponential Probability Function</title>
	<statement>
	<p>
		<me>\int_0^{\infty} \lambda e^{-\lambda x} dx = 1</me>
	</p>
	</statement>
	<proof>
		<p>
		<md>
			<mrow> &amp; \int_0^{\infty} \lambda e^{-\lambda x} dx</mrow>
			<mrow> &amp; = \int_0^{\infty} e^{-u} du</mrow>
			<mrow> &amp; = -e^{-u} \big |_0^{\infty} = 1</mrow>
		</md>
		where we used the substitution <m>u = \lambda x</m>.
		</p>
	</proof>
</theorem>
</p>

<p>
<theorem><title>Derivation of Statistics for Exponential Distribution and Plotting</title>
<statement>
	<p>
	<me>\mu = \frac{1}{\lambda}</me>
	<me>\sigma^2 = \mu^2</me>
	<me>\gamma_1 = 2</me>
	<me>\gamma_2 = 9</me>
	</p>
</statement>
<proof>
	<p>
	For the mean, use integration by parts with <m>u = x</m> and <m>dv = \lambda e^{-\lambda \cdot x}</m> to get (eventually)
	<md>
<mrow>\text{Mean} = \mu &amp; = \int_0^{\infty} x \cdot \lambda  e^{-\lambda \mu} </mrow>
<mrow> &amp; = \left [ -(x+\frac{1}{\lambda}) e^{-\lambda \cdot x} \right ] \big |_0^{\infty} </mrow>
<mrow> &amp; = \frac{1}{\lambda}.</mrow>
	</md>
	and so the use of <m>\lambda = \frac{1}{\mu}</m> in f(x) is warranted.
	</p>
	<p>
	The remaining statistics are derived similarly using repeated integration by parts. The interactive Sage cell below calculates those for you automatically.
	</p>
</proof>
</theorem>
</p>


<p>
<definition><title>Alternate Form for the Exponential Distribution Probability Function</title>
	<statement>
	<p xml:id="ExponentialPF">Given a Poisson process and a constant <m>\mu = \frac{1}{\lambda}</m>, suppose X measures the variable interval length needed until you get a first success.  Then X has an exponential distribution with probability function
		<me>f(x) = \frac{1}{\mu} e^{-\frac{x}{\mu}}.</me>
	</p>
	</statement>
</definition>
</p>

<p>
<sage>
	<input>
# Exponential Distribution
var('x,mu')
assume(mu>0)

f(x) =e^(-x/mu)/mu
mu = integral(x*f,x,0,oo).factor()
M2 = integral(x^2*f,x,0,oo).factor()
M3 = integral(x^3*f,x,0,oo).factor()
M4 = integral(x^4*f,x,0,oo).factor()

pretty_print('Mean = ',mu)

v = (M2-mu^2).factor()
pretty_print('Variance = ',v)
stand = sqrt(v)

sk = (((M3 - 3*M2*mu + 2*mu^3))/stand^3).simplify()
pretty_print('Skewness = ',sk)

kurt = (M4 - 4*M3*mu + 6*M2*mu^2 -3*mu^4).factor()/stand^4
pretty_print('Kurtosis = ',(kurt-3).factor(),'+3')
@interact
def _(m = slider(1,12,1/2,2,label='mu')):
    plot(f(mu=m),x,0,30).show(ymax=1)
	</input>
</sage>
</p>


<p>
<theorem><title>Distribution function for Exponential Distribution</title>
	<statement>
	<p>
		<me>F(x) = 1 - e^{-\frac{x}{\mu}}</me>
	</p>
	</statement>
	<proof>
	<p>
	Using <m>f(x) = \frac{1}{\mu} e^{-\frac{x}{\mu}}</m>, note
	<md>
		<mrow>F(x) &amp; = \int_0^x \frac{1}{\mu} e^{-\frac{u}{\mu}} du</mrow>
		<mrow> &amp; =  - e^{-\frac{u}{\mu}} \big |_0^x</mrow>
		<mrow> &amp; = 1 - e^{-\frac{x}{\mu}}</mrow>
	</md>
	</p>
	</proof>
</theorem>
</p>


<p>
<example><title>Router Requests Revisited</title>
<p>
Once again, let's consider a router which, over time, has been shown to receive on average 1000 such requests in any given 10 minute period during regular working hours. This would mean that, on average, it would take <m>\mu = \frac{10}{1000} = \frac{1}{100} = 0.01</m> minutes (i.e., less than a second) to receive the first request. If X were to measure the time interval until the first actual request comes in, then the Exponential distribution would be a good model using 
<me> f(x) = \frac{1}{0.01} e^{-\frac{x}{0.01}}.</me>
</p>
<p>
Let's determine the probability that a first request arrives in the next two seconds. First, note that since X is a continuous variable that f(x) is NOT the probability of exactly X minutes but you must integrate to compute all probabilities. Also, the next 2 seconds is actually the next <m>\frac{2}{60} = \frac{1}{30}</m> of a minute. Therefore, F(x) is what you need in general and you find
<me>P(X \le \frac{1}{30}) = F(\frac{1}{30}) = 1 - e^{-\frac{\frac{1}{30}}{0.01}} \approx 0.96433.</me> 
</p>
</example>
</p>

<p>
<exercise><title>WebWork</title>
		<introduction>
		<p>
		This one uses the exponential distribution.
		</p>
		</introduction>
		<webwork source="Library/Rochester/setProbability14ExponentialDist/ur_pb_14_1.pg">
		</webwork>
</exercise>
</p>


<p>
<sage language = 'r'>
<input>
r=1                # the number of successes desired
            # the mean till first must be given
mu = 3
sdev = sqrt(mu)  # the formula for the standard deviation

M = mu*3   # the space is infinite but we just go out 3 standard deviations
X = 0:M    # quantiles for the space R of the random variable 

Pexp &lt;- function(x){dexp(x, mu )}  # create the probability function over X

curve(Pexp, from=0, to=M, xlab="X", col="blue", lwd=3,
 main="Gamma Sampling vs Gamma Curve vs Approximating 'Bell Curve'") 
Pnormal &lt;- function(X){dnorm(X, mean=mu, sd=sdev)}   # to overlap a bell curve
curve(Pnormal, col="red", lwd=2, add=TRUE) 

Psample = rexp(10^6, mu)  # to create a histogram, sample a lot
# Xtop=max(Psample)          # for scaling the x-axis. Shift by 1/2 below.
hist(Psample, prob=TRUE, add=TRUE)
</input>
</sage>
</p>

<p>
<theorem><title>The Exponential Distribution yields a continuous memoryless model.</title>
<statement>
	<p>
	If X has an exponential distribution and a and b are nonnegative integers, then
	<me>P( X > a + b | X > b ) = P( X > a)</me>
	</p>
</statement>
<proof>
	<p>Using the definition of conditional probability,
	<md>
		<mrow>P( X > a + b | X > b ) &amp; = P( X > a + b \cap X > b ) \ P( X > b)</mrow>
		<mrow> &amp; = P( X > a + b ) / P( X > b)</mrow>
		<mrow> &amp; = e^{-(a+b)/ \mu} / e^{-b / \mu}</mrow>
		<mrow> &amp; = e^{-a/ \mu}</mrow>
		<mrow> &amp; = P(X > a)</mrow>
	</md>
	</p>
</proof>
</theorem>
</p>

<!--
Adapted from "Library/UBC/STAT/STAT241_251/setAssignment-04/HW04-01.pg"
-->
<p>
<exercise>
<webwork>
<setup>
    <pg-code>
    </pg-code>
</setup>
<statement>
<p>
It can be presumed that the life span in years of a certain brand of lightbulbs follows the Poisson process assumptions.  Suppose that the mean life span till failure is known to be 0.7 years. A tester makes random observations of the life times of this particular brand of lightbulbs and records them one by one as either a success if the life time exceeds 1 year, or as a failure otherwise.
</p>
<p>
Determine the probability that a randomly tested bulb lasts more than 1 year:
<var name="0.2397" width="10" />
</p>

<p>
Determine probability that the first success occurs in the fifth observation: <var name="0.0801" width="10" />
</p>
<p>  
Determine the probability that the second success occurs in the 8th observation given that the first success occurred in the 3rd observation:  <var name="0.0801" width="10" />
</p>
<p>
Determine the probability that the first success occurs in an odd-numbered observation: <var name="0.5681" width="10" />
</p>
</statement>

<hint>
<p>
Use the exponential distribution to get the first answer. Use the geometric to get the remainder.
</p>
</hint>
<solution>
<p>Using the exponential distribution, 
<me>p = P(X \gt 1) = 1 - F(1) = 1 - (1-e^{-\frac{1}{0.7}}) \approx 0.2397.</me>
</p>
<p>
Now, use the geometric distribution with <m>p = 0.2397</m>...
<me>P(X = 5) = f(5) = (1-0.2397)^4 \cdot 0.2397 \approx 0.0801.</me>
<me>P(\text{2nd success on 8} | \text{1st success on 3}) = P(X = 5) \approx 0.0801</me>
<md>
    <mrow> P(\text{1st success on an odd trial}) &amp; = f(1) + f(3) + f(5) + ...</mrow> 
    <mrow> &amp; = p + (1-p)^2 \cdot p + (1-p)^4 \cdot p + (1-p)^6 \cdot p + ... </mrow>
    <mrow> &amp; = p \sum_{x=0}^{\infty} \left ( (1-p)^2 \right)^k </mrow>
    <mrow> &amp; = p \frac{1}{1-(1-p)^2} \approx 0.5681</mrow>
</md>
</p>
</solution>
</webwork>
</exercise>
</p>



</section>



<section><title>Gamma Distribution</title>
<p>
Extending the exponential distribution model developed above, consider a Poisson Process where you start with an interval of variable length X so that X measures the interval needed in order to obtain the rth success for some natural number r. Then <m>R = (0,\infty)</m> and the resulting distribution of X will be called a Gamma distribution.
</p>

<p>
<definition><title>Gamma Function</title>
<statement>
	<p>
	<me>\Gamma(t) = \int_0^{\infty} u^{t-1} e^{-u} du</me>
	</p>
</statement>
</definition>
</p>

<p>
<theorem><title>Gamma Function on the natural numbers</title>
<statement>
<p>
For <m>n \in \mathbb{N}</m>,
<me>\Gamma(n+1) = n!</me>
</p>
</statement>
<proof>
<p>
Letting n be a natural number and applying integration by parts one time gives
<md>
	<mrow>\Gamma(n+1) &amp; = \int_0^{\infty} u^n e^{-u} du</mrow>
	<mrow> &amp; = -u^n \cdot e^{-u} \big |_0^{\infty} + n \int_0^{\infty} u^{n-1} e^{-u} du </mrow>
	<mrow> &amp; = 0 - 0 + n \Gamma(n)</mrow>
</md>
Continuing using an inductive argument to obtain the final result.
</p>
</proof>
</theorem>
</p>


<p>To find the probability function for the gamma distribution, once again focus on the development of F(x). Assuming r is a natural number greater than 1 and noting that X measures the interval length needed in order to achieve the rth success
<md>
	<mrow>F(x) &amp; = P(X \le x)</mrow>
	<mrow> &amp; = 1 - P(X \gt x)</mrow>
	<mrow> &amp; = 1 - P(\text{fewer than r successes in [0,x]})</mrow>
	<mrow> &amp; = 1 - \big [ \frac{(\lambda x)^0 e^{-\lambda x}}{0!} + \frac{(\lambda x)^1 e^{-\lambda x}}{1!} + ... + \frac{(\lambda x)^{r-1} e^{-\lambda x}}{(r-1)!} \big ]</mrow>
	<mrow> &amp; = 1 - \sum_{k=0}^{r-1} \frac{(\lambda x)^k e^{-\lambda x}}{k!} </mrow>
</md>
where the discrete Poisson probability function is used on the interval [0,x]. The derivative of this function however is "telescoping" and terms cancel. Indeed,
<md>
	<mrow>F'(x) &amp; = \lambda e^{-\lambda x}/0!</mrow>
	<mrow> &amp; - \lambda e^{-\lambda x}/1! + \lambda x \cdot \lambda e^{-\lambda x}/1!</mrow>
	<mrow> &amp; - \lambda^2 2x e^{-\lambda x}/2! + \lambda^2 x^2 \cdot \lambda e^{-\lambda x}/2!</mrow>
	<mrow> &amp; - \lambda^3 3x^2 e^{-\lambda x}/3! + \lambda^3 x^3 \cdot \lambda e^{-\lambda x}/3!</mrow>
	<mrow> &amp; . . .</mrow>
	<mrow> &amp; - \lambda^{r-1} (r-1)x^{r-2} e^{-\lambda x}/(r-1)! + \lambda^{r-1} x^{r-1} \cdot \lambda e^{-\lambda x}/(r-1)!</mrow>
	<mrow> &amp; = \lambda^r x^{r-1} e^{-\lambda x}/(r-1)!</mrow>
</md>
where you can replace <m>(r-1)! = \Gamma(r)</m>.
</p>


<p>
Notice that for this random variable, <m>\mu = \lambda T</m> can be obtained for the exponential distribution. For the Gamma distribution, the following takes <m>\mu</m> to be the average interval till the first success and then modifies the corresponding Gamma parameters according to increasing values of r.
</p>

<p>
<definition><title>Gamma Distribution Probability Function</title>
<statement>
<p xml:id="GammaPF">
If X measures the interval until the rth success and <m>\mu</m> as the average interval until the 1st success, then X with probability function
<me>f(x) =  \frac{x^{r-1} \cdot e^{-\frac{x}{\mu}}}{\Gamma(r) \cdot \mu^r}</me>
has a Gamma Distribution.
</p>
</statement>
</definition>
</p>

<p>
<exercise>
<webwork>
<setup>
    <pg-code>
    </pg-code>
</setup>
<statement>
<p>
It can be presumed that the life span in years of a certain brand of lightbulbs follows the Poisson process assumptions.  Suppose that the mean life span till failure is known to be 0.7 years. To do long-term study, a series of light bulbs are arranged so that when the first one fails, the next one comes on, etc.
</p>
<p>
Determine the probability that it takes at most 3.5 years before the 4th bulb fails:
<var name="0.73497" width="10" />
</p>
</statement>
<solution>
<p>
This is a gamma distribution with r = 4 and
<me>F(x) = 1 - \sum_{k=0}^3 \frac{ (x/0.7)^k e^{-x/0.7}}{k!}</me>
So, 
<md>
<mrow>P(X \lt 3.5) &amp; = F(3.5)</mrow>
<mrow> &amp; = 1 - \frac{ (3.5/0.7)^0 e^{-3.5/0.7}}{0!} \\ - \frac{ (3.5/0.7)^1 e^{-3.5/0.7}}{1!} \\ - \frac{ (3.5/0.7)^2 e^{-3.5/0.7}}{2!} - \frac{ (3.5/0.7)^3 e^{-3.5/0.7}}{3!}</mrow>
<mrow> &amp; = 1 - e^{-5} - 5 \cdot e^{-5} - \frac{25}{2} \cdot e^{-5} - \frac{125}{6} \cdot e^{-5}</mrow>
<mrow> &amp; = 1 - e^{-5} \left ( 1 + 5 + 25/2 + 125/6 \right ) \approx 0.73497.</mrow>
</md>
</p>
</solution>
</webwork>
</exercise>
</p>

<p>
<example><title>Router Requests Revisited Again</title>
<p>
For the third time, let's consider a router which, over time, has been shown to receive on average 1000 requests in any given 10 minute period during regular working hours and you want to know the likelihood that it takes more than 4 seconds in order to receive the 5th request. As you have already seen, it takes on average <m>\frac{10}{1000} = \frac{1}{100} = 0.01</m> minutes to receive the first request so we use that again here. If X were to measure the time interval until the fifth actual request comes in, then the Gamma distribution would be a good model using 
<me>f(x) =  \frac{x^{5-1} \cdot e^{- \frac{x}{0.01}}}{\Gamma(5) \cdot 0.01^5}</me>
</p>
<p>
The question above asks for
<me>P(X \gt 4 \text{seconds}) = P(X \gt \frac{4}{60} ) = 1 - F(\frac{4}{60}).</me>

Again, since X is a continuous variable you must integrate to compute probabilities. This will require integration by parts or you can use the F(x) from the derivation above. Here, let's just let Sage do the integration for us noting that <m>\Gamma(5) = 4! = 24</m>. You can compute the needed integral using the interactive cell immediately below.
</p>
</example>
</p>
<p>
Therefore
<me>P(X \gt 4 \text{seconds}) = 1 - F(\frac{4}{60}) \approx 0.205627.</me>
</p>
<p>
<sage>
<input>
var('x')
f = x^4*e^(-x/0.01)/(24*0.01^5)
prob_complement = integrate(f,x,0,4/60)
print n(1-prob_complement)
</input>
</sage>
</p>


<p>
<theorem><title>Verify Gamma Probability function</title>
<statement>
<p>
<me>\int_0^{\infty} \frac{x^{r-1} e^{-x/ \mu}}{\Gamma(r) \mu^r} dx = 1</me>
</p>
</statement>
<proof>
<p>
Evaluate the sage code below.
</p>
</proof>
</theorem>
</p>

<p>
<sage>
	<input>
# Gamma Distribution
var('x,mu,r')
assume(mu>0)
assume(r,'integer')
assume(r>1)
f(x) =x^(r-1)*e^(-x/mu)/(gamma(r)*mu^r)
S = integral(f,x,0,oo).full_simplify()
F = '$ \int_0^{\infty} \\frac{x^{r-1} e^{-x/ \mu}}{\Gamma(r) \mu^r} dx = %s$'%str(S)
html(F) 
	</input>
</sage>
</p>

<p>
<sage>
<input>
# Gamma Distribution Graphing
var('x,mu,r')
assume(mu>0)
assume(r,'integer')
@interact
def _(r=[2,3,6,12,24],mu=slider(1,12,1,5,label='mu')):
    f(x) =x^(r-1)*e^(-x/mu)/(gamma(r)*mu^r)
    plot(f,x,0,200).show()
    
</input>
</sage>
</p>





<p>
Derivation of mean, variance, skewness, and kurtosis. Pick "alpha" for the general formulas.
<sage>
	<input>
# Gamma Distribution
var('x,mu,r,alpha')
assume(mu>0)
assume(alpha,'integer')
assume(alpha>1)
@interact
def _(r=[2,3,6,9,alpha]):
    f(x) =x^(r-1)*e^(-x/mu)/(gamma(r)*mu^r)
    mean = integral(x*f,x,0,oo).full_simplify()
    M2 = integral(x^2*f,x,0,oo).full_simplify()
    M3 = integral(x^3*f,x,0,oo).full_simplify()
    M4 = integral(x^4*f,x,0,oo).full_simplify()
    
    pretty_print('Mean = ',mean)
    
    v = (M2-mean^2).full_simplify().factor()
    pretty_print('Variance = ',v)
    stand = sqrt(v)
    
    sk = (((M3 - 3*M2*mean + 2*mean^3))/stand^3).full_simplify()
    pretty_print('Skewness = ',sk)
    
    kurt = (M4 - 4*M3*mean + 6*M2*mean^2 -3*mean^4).factor()/stand^4
    pretty_print('Kurtosis = ',(kurt-3).factor(),'+3')
	</input>
</sage>
</p>


<p>The interactive cell below can be used to compute the distribution function for the gamma distribution for various input values. If you desire to let r get bigger than the slider allows, feel free to edit the cell above and evaluate again.

<sage>
	<input>
# Gamma Distribution Calculator
var('x,mu,r')
pretty_print('Enter the number of successes desired, the given mean, and the value of X to get F(X)')
@interact
def _(r=slider(1,10,1,2),mu = input_box(2,label="$\mu = $",width=10),b=input_box(2,label="X = ",width=10)):
    f(x) =x^(r-1)*e^(-x/mu)/(gamma(r)*mu^r)
    p = integral(f,x,0,b)
    
    pretty_print('Probability = \t',p,' which is approximately \t',p.n(digits=5))
	</input>
</sage>
</p>

<p>
<sage language='r'>
<input>
r=3                # the number of successes desired
mu1 = 3            # the mean till first must be given
mu = mu1*r
sdev = sqrt(r)*mu1  # the formula for the standard deviation

M = mu*3   # the space is infinite but we just go out 3 standard deviations
X = 0:M    # quantiles for the space R of the random variable 

Ppois &lt;- function(x){dgamma(x, shape=r, scale=mu1 )}  # create the probability function over X

curve(Ppois, from=0, to=M, xlab="X", col="blue", lwd=3,
 main="Gamma Sampling vs Gamma Curve vs Approximating 'Bell Curve'") 
Pnormal &lt;- function(X){dnorm(X, mean=mu, sd=sdev)}   # to overlap a bell curve
curve(Pnormal, col="red", lwd=2, add=TRUE) 

Psample = rgamma(10^6, shape=r, scale=mu1)  # to create a histogram, sample a lot
# Xtop=max(Psample)          # for scaling the x-axis. Shift by 1/2 below.
hist(Psample, prob=TRUE, add=TRUE)



</input>
</sage>
</p>

</section>

<section><title>Generating Functions for Poisson Process Distributions</title>


<p>
<xref ref="MomentGeneratingFunctionDefn">Moment Generating Functions</xref> can be derived for each of the distributions in this chapter.
</p>


<p>

<theorem><title>Moment Generating Function for Poisson</title>
<statement>
<p xml:id="PoissonMGF">Presuming <m>t \gt 0</m> and
<me>M(t) = e^{\mu \left ( e^t - 1 \right )}</me>
</p>
</statement>
<proof>
<p>Using the <xref ref="PoissonPF">Poisson probability function</xref>,
<md>
<mrow>M(t) &amp; = \sum_{x=0}^{\infty} e^{tx} \frac{\mu^x e^{-\mu}}{x!}</mrow>
<mrow> &amp; = \sum_{x=0}^{\infty} \frac{\left (\mu e^t \right )^x e^{-\mu e^t} e^{\mu e^t} e^{-\mu} }{x!}</mrow>
<mrow> &amp; =  e^{\mu e^t} e^{-\mu} \sum_{x=0}^{\infty} \frac{\left (\mu e^t \right )^x e^{-\mu e^t} }{x!}</mrow>
<mrow> &amp; =  e^{\mu \left( e^t - 1 \right )}  \sum_{x=0}^{\infty} \frac{\left (\mu e^t \right )^x e^{-\mu e^t} }{x!}</mrow>
<mrow> &amp; =  e^{\mu \left( e^t - 1 \right )},</mrow>
</md>
where we used a new poisson distribution with new mean <m>\mu e^t</m> to convert the sum.
</p>
</proof>
</theorem>

<corollary><title>Poisson Properties via Moment Generating Function</title>
<statement>
<p>For the Poisson variable X,
<me> M(0) = 1</me>
<me> M'(0) = \mu</me>
<me> M''(0) = \mu + \mu^2 = \sigma^2 + \mu^2</me>
</p>
</statement>
<proof>
<p>
<me> M(0) = e^{\mu \left ( e^0 - 1 \right )} = e^0 = 1.</me>
Continuing,
<me> M'(t) = \mu e^{\left(\mu {\left(e^{t} - 1\right)} + t\right)}</me>
and therefore
<me> M'(0) = \mu e^{\left(\mu {\left(1 - 1\right)} + 0\right)} = \mu e^0 = \mu.</me>
Continuing with the second derivative,
<me> M''(t) = {\left(\mu e^{t} + 1\right)} \mu e^{\left(\mu {\left(e^{t} - 1\right)} + t\right)}</me>
and therefore
<me> M''(0) = {\left(\mu + 1\right)} \mu e^{\left(\mu {\left(1 - 1\right)} + 0\right)} = {\left(\mu + 1\right)} \mu e^0 = \mu + \mu^2 </me>
which is the squared mean plus the variance for the poisson distribution.
</p>
</proof>
</corollary>

</p>

<p>
<theorem><title>Moment Generating Function for Exponential</title>
<statement>
<p xml:id="ExponentialMGF">Presuming <m>t \lt \frac{1}{\mu}</m>, 
<me>M(t) = \frac{1}{1-\mu t}.</me>
</p>
</statement>
<proof>
<p>Using the <xref ref="ExponentialPF">Exponential probability function</xref>
<md>
<mrow>M(t) &amp; = \int_0^{\infty} e^{tx} \frac{1}{\mu} e^{-\frac{x}{\mu}} dx</mrow>
<mrow> &amp; = \frac{1}{\mu} \int_0^{\infty} e^{- x \left ( -t + \frac{1}{\mu} \right ) } dx</mrow>
<mrow> &amp; =  \frac{1}{\mu \left(-t + \frac{1}{\mu} \right )}  e^{- x \left ( -t + \frac{1}{\mu} \right ) } \big |_0^{\infty}</mrow>
<mrow> &amp; =  \frac{1}{\mu \left(-t + \frac{1}{\mu} \right )}  \left ( -0 + 1 \right )</mrow>
<mrow> &amp; =  \frac{1}{\left(-\mu t + 1 \right )}.</mrow>
</md>
</p>
</proof>
</theorem>

<corollary><title>Exponential Properties via Moment Generating Function</title>
<statement>
<p>For the Exponential variable X,
<me> M(0) = 1</me>
<me> M'(0) = \mu</me>
<me> M''(0) = \mu^2 + \mu^2 = \sigma^2 + \mu^2</me>
</p>
</statement>
<proof>
<p>
<me> M(0) = \frac{1}{1-\mu 0} = 1.</me>
Continuing,
<me> M'(t) = \frac{\mu}{ \left ( 1-\mu t \right )^2}</me>
and therefore
<me> M'(0) = \frac{\mu}{ \left ( 1-\mu 0 \right )^2} = \mu.</me>
Continuing with the second derivative,
<me> M''(t) = \frac{2 \mu^2}{ \left ( 1-\mu t \right )^3}</me>
and therefore
<me> M''(0) = \frac{2 \mu^2}{ \left ( 1-\mu 0 \right )^3}= 2 \mu^2 = \mu^2 + \mu^2. </me>
which is the squared mean plus the variance for the poisson distribution.
</p>
</proof>
</corollary>
</p>

<p>
<theorem><title>Moment Generating Function for Gamma</title>
<statement>
<p xml:id="GammaMGF">Presuming <m>t \lt \frac{1}{\mu}</m> where <m>\mu</m> is the mean waiting time till the first "change" and <m>r</m> is the number of changes desired, 
<me>M(t) = \frac{1}{ \left ( 1-\mu t \right )^{r}}</me>
</p>
</statement>
<proof>
<p>Using the <xref ref="GammaPF">Gamma probability function</xref>, 
<md>
<mrow>M(t) &amp; = \int_0^{\infty} e^{tx}  \frac{x^{r-1} \cdot e^{-\frac{x}{\mu}}}{\Gamma(r) \cdot \mu^r} dx</mrow>
<mrow>M(t) &amp; = \int_0^{\infty} \frac{x^{r-1} \cdot e^{-x \left ( \frac{1}{\mu} - t \right )}}{\Gamma(r) \mu^r \left ( \frac{1}{\mu} - t \right )} dx</mrow>
<mrow>M(t) &amp; = {  \frac{1}{\left ( 1-t \mu \right )^r} } \int_0^{\infty} \frac{x^{r-1} \cdot e^{-\frac{x}{ \left ( \frac{\mu}{1-t \mu} \right )}}}{\Gamma(r) \cdot { \left ( \frac{\mu}{1-t \mu} \right )}^r} dx</mrow>
<mrow> &amp; =  \frac{1}{\left(-\mu t + 1 \right )^{r}}.</mrow>
</md>
since the last integral is on the Gamma probability function but with an adjusted mean.
</p>
</proof>
</theorem>

<corollary><title>Gamma Properties via Moment Generating Function</title>
<statement>
<p>For the Gamma variable X,  
<me> M(0) = 1</me>
<me> M'(0) = r \mu</me>
<me> M''(0) = r \mu^2 + \left ( r \mu \right )^2 = \sigma^2 + \mu^2</me>
</p>
</statement>
<proof>
<p>
<me> M(0) = \frac{1}{ \left ( 1-\mu 0 \right )^{r}}  \frac{1}{1} = 1.</me>
Continuing,
<me> M'(t) = \frac{r \mu}{ \left ( 1-\mu t \right )^{r+1}}</me>
and therefore
<me> M'(0) = \frac{r \mu}{ \left ( 1-\mu 0 \right )^{r+1}} = r \mu.</me>
Continuing with the second derivative,
<me> M''(t) = \frac{r(r+1) \mu^2}{ \left ( 1-\mu t \right )^{r+2}}</me>
and therefore
<me> M''(0) = \frac{r(r+1) \mu^2}{ \left ( 1-\mu 0 \right )^{r+2}} = r(r+1) \mu^2 = r \mu^2 + r^2 \mu^2 </me>
which is the squared mean plus the variance for the poisson distribution.
</p>
</proof>
</corollary>
</p>
<p>Once again, Sage can obtain the final answers quickly.  For Poisson:
<sage>
<input>
var('t,mu')
M = e^(mu *  (e^t - 1))
Mt = derivative(M,t)
Mtt = derivative(Mt,t)
show(M)
show(Mt)
show(Mt(t=0))
show(Mtt)
show(Mtt(t=0))
</input>
</sage>


</p>
</section>


<section><title>Summary</title>
<introduction>
<p>
Here is a summary of the major points in this chapter:
</p>
</introduction>
<p>
TBA
</p>
</section>

<section><title>Exercises</title>

<p>
<exercise><title> - Home Sales</title>
<p>
A local realty office sells on average 10 houses a week.  Let X measure the number of houses they will sell in the next week.  Determine
<ol>
	<li>the probability the realty office sells 12 houses next week.</li>
	<li>the probability the realty office sells fewer than 10 houses next week.</li>
	<li>the interval <m>\mu - 2\sigma \le X \le \mu + 2\sigma</m>.</li>
	<li><m>P(\mu - 2\sigma \le X \le \mu + 2\sigma)</m>.</li>
	
</ol>
</p>
</exercise>
</p>

<p>
<exercise><title> - Customer arrivals - Total</title>
<p>
Customers arrive at your store on the average of 10 per hour.  Assuming that the arrival of customers satisfies the properties of a Poisson process, determine:
<ol>
	<li>the expected number of customers to arrive in a given 3 hour period.</li>
	<li>the probability that fewer than 10 customers arrive in a given hour.</li>
</ol>
<solution>
<p>
Using the given information, apply the Poisson distribution.  For one hour <m>\mu = 10</m> so that for three hours the expected number of customers would be triple with 30 expected customers.
</p>
<p>
With <m>\mu_1 = 10</m>, 
<md>
	<mrow>P(X \lt 10) &amp; = F(9)</mrow>
	<mrow> &amp; = \frac{10^0 e^{-10}}{0!} + \frac{10^1 e^{-10}}{1!} + \frac{10^2 e^{-10}}{2!} + ... + \frac{10^8 e^{-10}}{8!} + \frac{10^9 e^{-10}}{9!}</mrow>
	<mrow>&amp; = e^{-10} \cdot ( 1 + 10 + \frac{100}{2} + ... + \frac{10^8}{8!} + \frac{10^9}{9!} )</mrow>
</md>
</p>
</solution>
</p>
</exercise>
</p>

<p>
<exercise><title> - Customer arrivals - First</title>
<p>
Customers arrive at your store on the average of 10 per hour.  Assuming that the arrival of customers satisfies the properties of a Poisson process, determine:
<ol>
	<li>the number of minutes expected between the arrival of each customer</li>
	<li>the probability it takes more than 9 minutes before the next customer arrives.</li>
</ol>
<solution>
<p>
Using the given information, apply the exponential distribution.  Since 10 arrive on average in one hour then you would expect 1 to arrive in 6 minutes.
</p>
<p>
With <m>\mu = 6</m> minutes, 
<me>P(X \gt 9) = 1 - F(9) = e^{-9/6}.</me>
</p>
</solution>
</p>
</exercise>
</p>

<p>
<exercise><title> - Customer arrivals - 10th</title>
<p>
Customers arrive at your store on the average of 10 per hour.  Assuming that the arrival of customers satisfies the properties of a Poisson process, determine:
<ol>
	<li>the number of minutes expected for the arrival of three customers.</li>
	<li>the probability it takes less than 20 minutes before the third customer arrives.</li>
</ol>
<solution>
<p>
Using the given information, apply the gamma distribution.  Since 10 arrive on average in one hour then you would expect 1 to arrive in 6 minutes and therefore 3 to arrive on average in 18 minutes.
</p>
<p>
With <m>\mu = 18</m> minutes, 
<me>P(X \lt 20) = F(20).</me>
</p>
</solution>
</p>
</exercise>
</p>

<p>
<exercise><title>Approximating Binomial with Poisson</title>
In our derivation above, we used the limit of a Binomial probability function to create the Poisson's probability function. Therefore, for large n the two should be computationally very close.  Use this fact to approximate P(X &gt; 10000) for a Binomial distribution with n = 40000 and p = 1/4 by converting to a Poisson probability function. 
<hint>
<p>
First determine the mean <m>\mu</m> for the binomial. Use this mean as the input for the Poisson distribution function.
</p>
</hint>
<solution>
<p>
From the formula, <m>\mu = np = 40000 \cdot \frac{1}{4} = 10000</m>. So, using Poisson's probability function
<me>f_{\text{Poisson}} = \frac{10000^{x}}{x!}e^{-10000}</me>
would require you to compute 
<me>e^{-10000} \sum_{x=10000}^{\infty} \frac{10000^{x}}{x!}</me>
which is also a mess. However with a computational resource such as a graphing calculator, just compare 1 - binomcdf(40000,0.25,9999) to 1-poissoncdf(10000,9999) noting that the complement of the given question is from X from 0 to 9999. The two values should be relatively close
</p>
<p>
This approximation method is not completely satisfactory since both being discrete distributions with no nice distribution function formulas require summations to accumulate. We have seen however that both the Poisson and the Binomial probability functions start to have a bell-shape as <m>\mu</m> increases for the Poisson and as n increases (i.e. and therefore <m>\mu</m> increases) for the Binomial. Hence, we will eventually approximate with each of these using the (continuous) bell-shaped distribution--the normal distribution discussed later--for which instead of accumulating probability function values we integrate them.  
</p>
</solution>
</exercise>
</p>

<p>
<exercise><title> - Computer Network Data Traffic</title>
<p>
Consider the arrival of requests on a server. Presume that the requests are considered as coming from an anonymous and large collection of users independently of each other on an average of 50 requests per second. If X measures the number of requests per second, determine
<ol>
	<li>the probability that in any given second the server gets fewer than 50 requests</li>
	<li><m>P(\mu - 2\sigma \le X \le \mu + 2\sigma)</m></li>
	<li>the expected number of requests per hour.</li>
</ol>

<solution>
<p>
Given the average of 50 requests per second and X measuring the number of "successes" in a given second long time interval given a Poission distribution
<me>f(x) = \frac{50^x}{x!}e^{-50}.</me>
Then,
<me>P(X \lt 50) = F(49) = \sum_{x=0}^{49} \frac{50^x}{x!} e^{-50}</me>
and using the graphing calculator function poissoncdf(50,49) = 0.48119.
</p>
<p>
For a time interval of one second, the mean is given to be 50 requests. Using the formulas developed above, the standard deviation therefore is <m>\sqrt{50}</m>. Therefore
<md>
	<mrow>P(\mu - 2\sigma \le X \le \mu + 2\sigma) &amp; = P(50 - 2\sqrt{50} \le X \le 50 + 2\sqrt{50})</mrow>
	<mrow> &amp; = P(X \in \{ 36, 37, 38, ..., 62, 63, 64 \}).</mrow>
</md>
Using the distribution function,
	<me>F(64) - F(35) \approx 0.97640 - 0.01621 = 0.96019</me>
</p>
<p>
Finally, notice that the time interval has been adjusted. Since the mean formula is proportional to the interval over which X is measured, using <m>\mu = \lambda T</m> with <m>\lambda = 1</m> when the interval is 1 second, then when the interval is one hour, T = 3600 seconds. Hence, we would expect on average <m>50 \cdot 3600 = 180,000</m> requests in one hour.
</p>
</solution>
</p>
</exercise>
</p>

</section>


</chapter>