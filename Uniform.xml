<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="EquallyLikelyBasedDistributions" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Distributions based upon Equally likely Outcomes</title>

<section xml:id="EquallyLikelyBasedDistributionsIntroduction"><title>Introduction</title>
<p>
When motivating our definition of probability you may have noticed that we modeled our definition on the relative frequency of equally-likely outcomes. In this chapter you will develop the theoretical formulas which can be used to model equally-likely outcomes.
</p>
	
<p>
In this chapter, you will investigate the following distributions:
<ol>
	<li>Discrete Uniform - each of a finite collection of outcomes is equally likely and prescribed a "position" and <m>X</m> measures the position of an item selected randomly from the outcomes.</li>
	<li>Continuous Uniform - an interval of values is possible with sub-intervals of equal length having equal probabilities and <m>X</m> measures a location inside that interval.</li>
	<li>Hypergeometric - each of a finite collection of values are equally likely and grouped into two classes (successes vs failures) and a subset of that collection is extracted with <m>X</m> measuring the number of successes in the sample.</li>
</ol>
</p>
</section>	
	
<section xml:id="DiscreteUniform" text="title"><title>Discrete Uniform Distribution</title>
<p>
In this section, you will investigate distributions that begin with individual outcomes that are equally likely and expand into more general settings.
</p>

<p>
<theorem xml:id="DiscreteUniformPF"><title>Discrete Uniform Distribution</title>
<statement>
<p>
Assume outcomes in <m>R</m> = {1, 2, 3, ..., n} are equally likely.  Then, the probability function for the discrete uniform variable <m>X</m> is
<me>f(x) = \frac{1}{n}</me>
for <m>x \in R</m>.
</p>
</statement>
<proof>
	<p>
	Assume that you have a variable with space <m>R</m> = {1, 2, 3, ..., n} so that the likelihood of each value is equally likely. Then, the probability function satisfies <m>f(x) = c</m> for any <m>x \in R</m>.  As before, since <m>\sum_{x \in R} f(x) = 1</m>, then 
	<me>f(x) = \frac{1}{n}.</me> 
	</p>
</proof>
</theorem>
</p>	

<p>	
<sage>
<input>
# Uniform distribution over 1 .. n
pretty_print("Discrete Uniform Distribution over the set 1, 2, ..., n")
var('x')
@interact
def _(n=slider(2,10,1,2)):
    np1 = n+1
    R = range(1,np1)
    f(x) = 1/n
    pretty_print(html('Density Function: $f(x) =%s$'%str(latex(f(x)))+' over the space $R = %s$'%str(R)))
    points((k,f(x=k)) for k in R).show()
    for k in R:
        pretty_print(html('$f(%s'%k+') = %s'%f(x=k)+' \\approx %s$'%f(x=k).n(digits=5)))

</input>
</sage>
</p>

<p>
<theorem xml:id="DiscreteUniformProperties"><title>Properties of the Discrete Uniform Probability Function</title>
<statement>
	<p>
	<m>f(x) = \frac{1}{n}</m> over <m>R</m> = {1, 2, 3, ..., n} satisfies the properties of a discrete probability function and
	<ol>
		<li><m>\mu = \frac{1+n}{2}</m></li>
		<li><m>\sigma^2 = \frac{n^2-1}{12}</m></li>
		<li><m>\gamma_1 = 0</m></li>
		<li><m>\gamma_2 = 3 - \frac{6}{5}\frac{n^2+1}{n^2-1}</m></li>
		<li>Distribution function <m>F(x) = \frac{x}{n}</m> for <m>x \in R</m>.</li>
	</ol>
	</p>
</statement>
<proof>
	<p>
	Trivially, by construction you get by summing over <m>R = \{1, 2, ... , n \}</m>
	<me>\sum_{x=1}^n \frac{1}{n} = 1</me>
	Also, 1/n is positive for all x values.
	</p>
	
	<p>To determine the <xref ref="TheoreticalMean">mean</xref>,
		<md>
			<mrow>\mu &amp; = \sum_{x=1}^n x \cdot \frac{1}{n}</mrow>
			<mrow> &amp; = \frac{1}{n}\sum_{x=1}^n x </mrow>
			<mrow> &amp; = \frac{1}{n} \frac{n(n+1)}{2}</mrow>
			<mrow> &amp; = \frac{1+n}{2}</mrow>
		</md>
	</p>	
	
	<p>To determine the <xref ref="TheoreticalVariance">variance</xref>,
		<md>
			<mrow>\sigma^2 &amp; = \sum_{x=1}^n x^2 \cdot \frac{1}{n} - \mu^2</mrow>
			<mrow> &amp; = \frac{1}{n}\sum_{x=1}^n x^2 - \left ( \frac{1+n}{2}\right )^2 </mrow>
			<mrow> &amp; = \frac{1}{n} \frac{n(n+1)(2n+1)}{6} - \frac{1+2n+n^2}{4}</mrow>
			<mrow> &amp; = \frac{(2n^2+3n+1)}{6} - \frac{1+2n+n^2}{4}</mrow>
			<mrow> &amp; = \frac{(4n^2+6n+2)}{12} - \frac{3+6n+3n^2}{12}</mrow>
			<mrow> &amp; = \frac{(n^2-1)}{12}</mrow>
		</md>
	</p>

	<p>For <xref ref="TheoreticalSkewness">skewness</xref>,
		<md>
			<mrow>\gamma_1 = &amp; \sum_{x=1}^n x^3 \cdot \frac{1}{n} - 3 \mu \sum_{x=1}^n x^2 \cdot \frac{1}{n}  + 2\mu^3</mrow>
			<mrow> &amp; = \frac{n^2(n+1)^2}{4n} - 3\frac{(n(n+1)(2n+1))}{2n} \frac{1+n}{2} + 2 \left ( \frac{1+n}{2}\right )^3 </mrow>
			<mrow> &amp; = \frac{n^2(n+1)^2}{4n} - \frac{(n+1)^2 (n(2n+1)}{4n} + \frac{(n+1)^3}{4}</mrow>
			<mrow> &amp; = \frac{(n+1)^2}{4} \left [ n - 2n -1 + (n+1) \right ]</mrow>
			<mrow> &amp; = 0</mrow>
		</md>
	which should be obvious since the histogram for this distribution is constantly flat.
	</p>

	<p>For <xref ref="TheoreticalKurtosis">kurtosis</xref>, 
	use the fourth moment and simplify.  This this is tedious, the algebra is performed using Sage in the active cell below this proof. However, you might want to supply the remainder of this proof using the fact that
	<me>\sum_{x=1}^n x^4 = \frac{6n^5 + 15n^4 + 10n^3 - n}{30}.</me>
	</p>
</proof>
</theorem>
</p>

<p>
Sage can also do the algebra for you to determine each of these measures. Notice, as n increases the Kurtosis approaches <m>\frac{6}{5}</m> which indicates that there is (obviously) no tend toward central tendency over time.
</p>

<p>
<sage>
<input>
var('x,n')
f = 1/n
mu = sum(x*f,x,1,n).factor()
pretty_print('Mean = ',mu)
mu = (1+n)/2
v = sum((x-mu)^2*f, x, 1, n)
pretty_print('Variance = ',v.factor())
stand = sqrt(v)
pretty_print('Skewness =  ',(sum((x-mu)^3*f, x, 1, n)/stand^3))
kurt = sum((x-mu)^4*f, x, 1, n)/stand^4
pretty_print('Kurtosis = ',(kurt-3).factor(),' + 3')
</input>
</sage>
</p>

<p>
<example><title>Rolling one die</title>
<p>When you consider rolling a regular, fair, single 6-sided die, each side is equally likely. The sample space consists of the 6 sides, each with a unique number of physical dots. Let the random variable <m>X</m> correspond each side with the number corresponding to the number of dots. Then, <m>R</m> = {1, 2, 3, 4, 5, 6}.  Since each side is equally likely then <m>f(x) = 1/6</m>.
</p>
<p>
Further, the probability of getting an outcome in A = {2,3} would be <m>f(2)+f(3) = 1/6 + 1/6 = 2/6</m>.
</p>
</example>
</p>

</section>

<section xml:id="ContinuousUniform"><title>Continuous Uniform Distribution</title>
<p>
Modeling the idea of "equally-likely" in a continuous world requires a slightly different perspective since there are obviously infinitely many outcomes to consider. Instead, you should consider requiring that intervals in the domain which are of equal width should have the same probability regardless of where they are in that domain. This behaviour suggests 
   <me>P(u \lt X \lt v) = P(u + \Delta \lt X \lt v + \Delta)</me>
 for reasonable values of <m>\Delta</m> so that the interval remains inside <m>R</m>.
</p>
	
<p>
<theorem  xml:id="ContinuousUniformFunction"><title>Continuous Uniform Probability Function</title>
<statement>
<p>For <m>R</m> = [a,b], with a &lt; b, the continuous uniform probability function is given by
	<me>f(x) = \frac{1}{b-a}.</me>
</p>
</statement>
<proof>
	<p>From before, for X a continuous uniform variable, we get
	<md>
		<mrow>\int_u^v f(x) dx = \int_{u+\Delta}^{v+\Delta} f(x) dx</mrow>
		<mrow>F(v)-F(u) = F(v+\Delta)-F(u+\Delta)</mrow>
		<mrow>F(u+\Delta)-F(u) = F(v+\Delta)-F(v)</mrow>
		<mrow>\frac{F(u+\Delta)-F(u)}{\Delta} = \frac{F(v+\Delta)-F(v)}{\Delta}</mrow>
	</md>
	which is true regardless of \Delta so long as you stay in the domain of interest. Letting <m>\Delta \rightarrow 0</m> gives
	<me>F'(u) = F'(v)</me>
but since F is an antiderivative of the probability function,
	<me>f(u) = f(v)</me>
for all u and v in R. This only happens if f is constant...say, f(x)=c. If the space of X is a single interval with <m>R = [a,b]</m> then
	<me>1 = \int_a^b c dx = c(b-a)</me>
	which yields <m>c = \frac{1}{b-a}</m> as desired. 
	</p>
</proof>
</theorem>
</p>

<p>
<example><title>Basic Continuous Uniform</title>
<p>
On <m>R = [1,2 \pi]</m>,
<me>f(x) = \frac{1}{2 \pi - 1}.</me>
Then, if you want to compute something like <m>P(2 &lt; X &lt; 4.5)</m> integrate
<me>P(2 &lt; X &lt; 4.5) = \int_2^{4.5} \frac{1}{2 \pi -1} dx = \frac{2.5}{2 \pi - 1}</me>
</p>
</example>
</p>


<exercise><title>WebWork - Continuous Uniform</title>
		<webwork source="Library/Rochester/setProbability16JointDist/ur_pb_16_3.pg">
		</webwork>
</exercise>


<p>
<example><title>Continuous Uniform over two disjoint intervals</title>
<p>
Suppose <m>R = [0,2] \cup [5,7]</m>.  Then, as in the theorem proof
<me>1 = \int_R c \cdot dx = \int_0^2 c \cdot dx + \int_5^7 c \cdot dx = 4c.</me>
Thus, <m> f(x) = \frac{1}{4}</m>.
For computing probabilities, you will want to break up any resulting integrals in a similar manner.
</p>
</example>
</p>

<p>
<theorem xml:id="ContinuousUniformProperties"><title>Properties of the Continuous Uniform Probability Function</title>
<statement>
	<p>For the Continuous Uniform Distribution over <m>R = [a,b]</m>, with a &lt; b,
	<ol>
		<li><m>f(x) = \frac{1}{b-a}</m> satisfies the properties of a probability function over R = [a,b].</li>
		<li><m>\mu = \frac{a+b}{2}</m></li>
		<li><m>\sigma^2 = \frac{(b-a)^2}{12}</m></li>
		<li><m>\gamma_1 = 0</m></li>
		<li><m>\gamma_2 = \frac{9 \, {\left(a^{5} - 5 \, a^{4} b + 10 \, a^{3} b^{2} - 10 \, a^{2} b^{3} + 5 \, a b^{4} - b^{5}\right)} {\left(a - b\right)}}{5 \, {\left(a^{3} - 3 \, a^{2} b + 3 \, a b^{2} - b^{3}\right)}^{2}}</m>
		</li>
	</ol>
	</p>
</statement>
</theorem>
<proof>
<p>We can verify most of these here but you can also determine these using Sage below.
</p>
<p>For the <xref ref="TheoreticalMean">mean</xref>,
<md>
<mrow> \mu &amp; = E[X] = \int_a^b x \frac{1}{b-a} dx</mrow>
<mrow> &amp; = \frac{x^2}{2(b-a)} right |_a^b</mrow>
<mrow> &amp; = \frac{b^2-a^2}{2(b-a)} = \frac{b+a}{2}.</mrow>
</md>
</p>
<p>For the <xref ref="TheoreticalVariance">variance</xref>,
<md>
<mrow> \sigma^2 &amp; = E[X^2] - \mu^2 = \int_a^b x^2 \frac{1}{b-a} dx  - \mu^2</mrow>
<mrow> &amp; = \frac{x^3}{3(b-a)} right |_a^b - \left ( \frac{a+b}{2} \right )^2</mrow>
<mrow> &amp; = \frac{b^3-a^3}{3(b-a)} - \frac{a^2 + 2ab + b^2}{4}</mrow>
<mrow> &amp; = \frac{4 b^2 + 4 ab + 4 a^2 - 3a^2 - 6 ab - 3b^2}{12}</mrow>
<mrow> &amp; = \frac{b^2-2ab+a^2}{12} = \frac{(b-a)^2}{12}.</mrow>
</md>
</p>
<p>For the <xref ref="TheoreticalSkewness">skewness</xref>, 
<md>
<mrow> \gamma_0 &amp; = E[X^3] - 3 \mu E[X^2] + 2 \mu^3</mrow>
<mrow> &amp; = \int_a^b x^3 \frac{1}{b-a} dx - 3 \mu \frac{b^3-a^3}{3(b-a)} + 2 \left ( \frac{a+b}{2} \right )^3</mrow>
<mrow> &amp; = \frac{x^4}{4(b-a)} right |_a^b - 3 \frac{a+b}{2} \cdot \frac{b^3-a^3}{3(b-a)} + 2 \frac{a^3 + 3a^2 b + 3a b^2 + b^3}{8} </mrow>
<mrow> &amp; = \text{a miracle of algebra}</mrow>
<mrow> &amp; = 0.</mrow>
</md>
</p>
<p>The <xref ref="TheoreticalKurtosis">kurtosis</xref> is more algebra like above. We will just let Sage do that part for us below.
</p>
</proof>
</p>

<p>
<sage>
<input>
# Continous uniform distribution statistics derivation
reset()
var('x,a,b')
f = 1/(b-a)
mu = integrate(x*f,x,a,b).factor()
pretty_print('Mean = ',mu)

v = integrate((x-mu)^2*f, x, a, b)
pretty_print('Variance = ',v.factor())
stand = sqrt(v)

sk = (integrate((x-mu)^3*f, x, a, b)/stand^3)
pretty_print('Skewness =  ',sk)

kurt = (integrate((x-mu)^4*f, x, a, b)/stand^4)
pretty_print('Kurtosis = ',kurt)

pretty_print('Several Examples')
a1=0
for b1 in range(2,7):
    pretty_print('Using [',a1,',',b1,']:')
    pretty_print('    mean = ',mu(a=a1,b=b1))
    pretty_print('variance = ',v(a=a1,b=b1))
    pretty_print('skewness = ',sk(a=a1,b=b1))
    pretty_print('kurtosis = ',kurt(a=a1,b=b1))
</input>
</sage>
</p>

<p>
<example><title>Occurence of exactly one event randomly in a given interval</title>
<p>
Suppose you know that only one person showed up at the counter of a local business in a given 30 minute interval of time. Then, <m>R</m> = [0,30] given <m>f(x) = 1/30</m>.
</p>
<p>
Further, the probability that the person arrived within the first 6 minutes would be <m>\int_0^6 \frac{1}{30} dx = 0.2</m>.
</p>
</example>
</p>

<p>
<theorem><title>Distribution Function for Continuous Uniform</title>
<statement>
<p>
For <m>x \in [a,b], F(x) = \frac{x-a}{b-a}</m>
</p>
</statement>
<proof>
<p>
For x in this range, 
<me>F(x) = \int_a^x \frac{1}{b-a} du = \frac{u}{b-a} \big |_a^x = \frac{x-a}{b-a}.</me>
</p>
</proof>
</theorem>
</p>

</section>


<section xml:id="HypergeometricDistribution"><title>Hypergeometric Distribution</title>
<p>
For the discrete uniform distribution, the presumption is that you will be making a selection one time from the collection of items. However, if you want to take a larger sample without replacement from a distribution in which originally all are equally likely then you will end up with something which will not be uniform.
</p>

<p>
Indeed, consider a collection of n items from which you want to take a sample of size r without replacement. If <m>n_1</m> of the items are "desired" and the remaining <m>n_2 = n - n_1</m> are not, let the random variable X measure the number of items from the first group in your sample with <m>R = \{0, 1, ..., min {r,n_1} \}</m>. The resulting collection of probabilities is called a Hypergeometric Distribution.
</p>

<p>
<image source="images/HypergeometricBucket.png" width = "60%"/>
</p>	
	
<p>
<theorem xml:id="HypergeometricFunction"><title>Hypergeometric Probability Function</title>
<statement>
<p>For a Hypergeometric random variable with <m>R</m> = {0, 1, ..., r} and assuming <m>n_1 \ge r</m> and <m>n-n_1 \ge r</m>,
	<me>f(x) = \frac{\binom{n_1}{x} \binom{n-n_1}{r-x}}{\binom{n}{r}}</me>	
</p>
</statement>
<proof>	
<p>
For the following, we will presume thatSince you are sampling without replacement and trying only measure the number of items from your desired group in the sample, then the space of X will include R = {0, 1, ..., r} assuming <m>n_1 \ge r</m> and <m>n-n_1 \ge r</m>. In the case when r is too large for either of these, the formulas below will follow noting that binomial coefficients are zero if the top is smaller than the bottom or if the bottom is negative.
</p>
<p>
So f(x) = P(X = x) = P(x from the sample are from the target group and the remainder are not). Breaking these up gives
	<me>f(x) = \frac{\binom{n_1}{x} \binom{n-n_1}{r-x}}{\binom{n}{r}}</me>
</p>
</proof>
</theorem>
</p>

<p>
For example, suppose that you have a bag of assorted candies but you really prefer the little dark chocolate bars. Because you are obsessive, you first empty the whole bag onto your desk and discover that the bag contains 33 equally-sized candy bars of which 6 of them are your delightful dark chocolate bars. Putting the bars randomly back into the bag, you find that a friend's friend's friend walks into the room and grabs a handful of 5 candy bars from your bag. You are shocked and would like to know the probability that this person got 2 or more of your dark chocolate candy bars. 
</p>
<p>
As a good prob/stats student you recognize that this situation fits the requirements of the hypergeometric distribution with <m>n_1 = 6, n_2 = 27, r=5</m> and you want <m>P(X \ge 2)</m>.  You determine that it would be easier to compute the complement 
<me>1 - P(X \le 1) = 1 - f(0) - f(1).</me>  
Therefore, 
<me> P (X \ge 2) = 1 - \frac{\binom{6}{0} \binom{27}{5}}{\binom{33}{5}} - \frac{\binom{6}{1} \binom{27}{4}}{\binom{33}{5}}</me>
or after some simplification
<me>P (X \ge 2) = 1 - \frac{13455}{39556} - \frac{8775}{19778} \approx 0.21617</me>
Therefore, you have about 1 chance out of 5 that the friend got 2 or more of your bars. You can be somewhat confident that plenty of dark chocolate bars remain hoarded for yourself.
</p>

<p>	
	<theorem xml:id="HypergeometricProperties"><title>Properties of the Hypergeometric Distribution</title>
	<statement>
	<p>
	<ol>
		<li><m>f(x) = \frac{\binom{n_1}{x} \binom{n-n_1}{r-x}}{\binom{n}{r}}</m> satisfies the properties of a probability function.</li>
		<li><m>\mu = r \frac{n_1}{n}</m></li>
		<li><m>\sigma^2 = r \frac{n_1}{n} \frac{n_2}{n} \frac{n-r}{n-1}</m></li>
		<li><m>\gamma_1 = \frac{(n - 2 n_1)\sqrt{n-1}(n - 2r)}{r n_1 (n - n_1) \sqrt{n-r}(n-2)}</m></li>
		<li><m>\gamma_2 = \frac{n(n+1)-6n(n-r)}{n_1(n-n_1)} + \frac{3r(n-r)(n+6)}{n^2} - 6</m></li>
	</ol>
	</p>
	</statement>
	<proof>
	<p>
	<ol>
		<li>
			<p>		
			<md>
				<mrow>\sum_{x=0}^n \binom{n}{x} y^x &amp; = (1+y)^n, \text{ by the Binomial Theorem}</mrow>
				<mrow>&amp; = (1+y)^{n_1} \cdot (1+y)^{n_2} </mrow>
				<mrow>&amp; = \sum_{x=0}^{n_1} \binom{n_1}{x} y^x \cdot \sum_{x=0}^{n_2} \binom{n_2}{x} y^x </mrow>
				<mrow>&amp; = \sum_{x=0}^n \sum_{t=0}^r \binom{n_1}{r} \binom{n_2}{r-t} y^x</mrow>
			</md>		
			Equating like coefficients for the various powers of y gives
			<me>\binom{n}{r} = \sum_{t=0}^r \binom{n_1}{r} \binom{n_2}{r-t}.</me>
			Dividing gives
			<me>1 = \sum_{x=0}^r f(x).</me>
			</p>
		</li>
		<li>
		<p>For the mean
			<md>
				<mrow>\sum_{x=0}^n x \frac{\binom{n_1}{x} \binom{n-n_1}{r-x}}{\binom{n}{r}} &amp; = 
\frac{1}{\binom{n}{r}} \sum_{x=1}^n  \frac{n_1(n_1-1)!}{(x-1)!(n_1-x)!}  \binom{n-n_1}{r-x}				
				</mrow>
				<mrow> &amp; = \frac{n_1}{\binom{n}{r}} \sum_{x=1}^n  \frac{(n_1-1)!}{(x-1)!((n_1-1)-(x-1))!}  \binom{n-n_1}{r-x} </mrow>
				<mrow> &amp; = \frac{n_1}{\frac{n(n-1)!}{r!(n-r)!}} \sum_{x=1}^n  \binom{n_1-1}{x-1}  \binom{n-n_1}{r-x} </mrow>
			</md>
			Consider the following change of variables for the summation: 
			<md>
				<mrow>y = x-1</mrow>
				<mrow>n_3 = n_1-1</mrow>
				<mrow>s = r-1</mrow>
				<mrow>m = n-1</mrow>
			</md>
			Then, this becomes
			<md>
				<mrow> \mu = \sum_{x=0}^n x \frac{\binom{n_1}{x} \binom{n-n_1}{r-x}}{\binom{n}{r}} &amp; = r \frac{n_1}{n} \sum_{y=0}^m  \frac{\binom{n_3}{y} \binom{m-n_3}{s-y}}{\binom{m}{s}}
				</mrow>
				<mrow>&amp; = r \frac{n_1}{n} \cdot 1</mrow>
			</md>
			noting that the summation is in the same form as was show yields 1 above.

		</p>
		</li>
		<li>
		<p>
For variance, we will use an alternate form of the definition that is useful when looking for cancellation options with the numerous factorials in the <xref ref="HypergeometricDistribution">hypergeometric probability function</xref>. Indeed, you can easily notice that
<me>\sigma^2 = E[X^2] - \mu^2 = E[X^2-X]+E[X] -\mu^2 = E[X(X-1)] + \mu - \mu^2.</me>
Since we have <m>\mu = r \frac{n_1}{n}</m> from above then let's focus on the first term only and use the substitutions
			<md>
				<mrow>y = x-2</mrow>
				<mrow>n_3 = n_1-2</mrow>
				<mrow>s = r-2</mrow>
				<mrow>m = n-2</mrow>
			</md>
to get
<md>
<mrow>E[X(X-1)] &amp; = \sum_{x=0}^n x(x-1) \frac{\binom{n_1}{x} \binom{n-n_1}{r-x}}{\binom{n}{r}}</mrow>
<mrow> &amp; = \sum_{x=2}^n x(x-1) \frac{\frac{n_1!}{x(x-1)(x-2)!(n_1-x)! } \binom{n-n_1}{r-x}}{\binom{n}{r}}</mrow>
<mrow> &amp; = \sum_{x=2}^n \frac{\frac{n_1!}{(x-2)!(n_1-x)! } \frac{n_2!}{(r-x)!(n_2-r+x)!}}{\binom{n}{r}}</mrow>
<mrow> &amp; = n_1 \cdot (n_1-1) \cdot  
                      \\ \sum_{x=2}^n \frac{\frac{(n_3)!}{(x-2)!(n_3 -(x-2))! } \frac{n_2!}{((r-2)-(x-2))!(n_2-(r-2)+(x-2))!}}{\binom{n}{r}}</mrow>
<mrow> &amp; = n_1 \cdot (n_1-1) \sum_{y=0}^{m} \frac{\frac{(n_3)!}{y!(n_3 -y)! } \frac{n_2!}{(s-y)!(n_2-s+y)!}}{\binom{n}{r}}</mrow>
<mrow> &amp; = \frac{n_1 \cdot (n_1-1) \cdot r \cdot (r-1)}{n (n-1)} \sum_{y=0}^{m} \frac{\binom{(n_3)}{y} \binom{n_2}{s-y}}{\binom{m}{s}}</mrow>
<mrow> &amp; = \frac{n_1 \cdot (n_1-1) \cdot r \cdot (r-1)}{n (n-1)} </mrow>
</md>
where we have used the summation formula above that showed that f(x) was a probability function.  
</p>
<p>
Putting this together with the earlier formula gives
<me>\sigma^2 = \frac{n_1 \cdot (n_1-1) \cdot r \cdot (r-1)}{n (n-1)} + r \frac{n_1}{n} - \left ( r \frac{n_1}{n} \right )^2.</me> 
		</p>
		</li>
		<li>
		<p>The proof of kurtosis is even more messy and we wonâ€™t bother with proving it for this distribution!
		</p>
		</li>
	</ol>
	</p>
	</proof>
	</theorem>
</p>

	<p>
	Note, if r=1 then you are back at a regular discrete uniform model. Indeed, <me>P(\text{desired item}) = 1 \cdot \frac{n_1}{n} = \mu .</me>
	which is indeed what you might expect when selecting once.
	</p>

<p>
<sage language='r'>
<input>
N1 = 10
N2 = 16
n = N1+N2
r = 5

X = 0:r        # the space R of the random variable 
mu = r*N1/n      # the formula for mean of the Binomial Distributions
sdev = sqrt(mu*(N2/n)*(n-r)/(n-1))  # the formula for the standard deviation
dhyper( X, N1, N2, r )   # let's print out a bunch of actual probs

Phyper = dhyper(X, N1, N2, r )  # create the probability function over X

Psample = rhyper(10^6, N1, N2, r)  # to create a histogram, sample a lot
Xtop=max(Psample)          # for scaling the x-axis. Shift by 1/2 below.
hist(Psample, prob=TRUE, br=(-1:Xtop)+0.5, col="skyblue2", xlab="X", 
  main="Hypergeometric Probability Function vs Approximating 'Bell Curve'")

points(X, Phyper, pch=19, col="darkgreen")  # to create actual (x,f(x))

Pnormal &lt;- function(X){dnorm(X, mean=mu, sd=sdev)}   # to overlap a bell curve
curve(Pnormal, col="red", lwd=2, add=TRUE) 
</input>
</sage>
</p>

<!--

<exercise><title>WebWork - Hypergeometric</title>
<webwork source="Library/Misc/hypergeometric.pg"></webwork>
</exercise>

-->

<p>Consider the <xref ref="HypergeometricDistribution">Hypergeometric Distribution</xref> for various values of <m>n_1, n_2,</m> and r using the interactive cell above. Notice what happens when you start with relatively small values of <m>n_1, n_2,</m> and r (say, start with <m>n_1 = 5, n_2 = 8,</m> and r = 4 and then doubling then all again and again. Consider the likely skewness and kurtosis of the graph as the values get larger.
</p>	
<p>
In the interactive cell below, notice how many items are in each portion of the "Venn Bucket" when you change the size of the number extracted--denoted by what is inside the circle.
</p>
<sage>
	<input>
# Hypergeometric distribution over 0 .. N
# Size of classes N1 and N2 must be given as well as subset size r
var('x')
@interact(layout=dict(top=[['N1','N2','r']]))
def _(N1=input_box(10,width=5,label='$$ N_1 $$'),
    N2=input_box(10,width=5,label='$$ N_2 $$'),
    r=input_box(5,width=5,label='$$ r $$')):
    N = N1 + N2
    f = binomial(N1,x)*binomial(N2,r-x)/binomial(N,r)
    @interact
    def __(X = slider(0,min(r,N1),1)):
        Px = f(x=X)
        Pxapprox = Px.n(digits=8)
        formula = "\\frac{\\binom{%s}"%str(N1)+"{%s}"%str(X)+" \\binom{%s}"%str(N2)+"{%s}}"%str(r-X)+"{ \\binom{%s}"%str(N)+"{%s}}"%str(r)
        pretty_print(html("P(X = %s"%str(X)+") = $ %s $"%str(formula)+" = $ %s $"%str(latex(Px))+"$ \\approx %s $"%str(Pxapprox)))
        G = polygon([(0,0), (1,0), (1.2,1), (-.2,1)],color='lightblue')
        G += line([(0.5,0),(0.5,1)],thickness=2)
        G += circle((0.5,0.3),0.25,fill=True,facecolor='yellow',alpha=0.5,zorder=2)
        G += text("Things I want",(0.2,0.9))+text("Everything else",(0.8,0.9))
        G += text("%s"%X,(0.4,0.3),fontsize=30) + text("%s"%(r-X),(0.6,0.3),fontsize=30)
        G += text("%s"%(N1-X),(0.1,0.5),fontsize=30) + text("%s"%(N2-(r-X)),(0.9,0.5),fontsize=30)
        show(G, axes=0, figsize=(4,4) )
	</input>
</sage>
	
</section>


<section xml:id="EquallyLikelyGeneratingFunctions"><title>Generating Functions for Uniform-based Distributions</title>

<p>
<xref ref="DefnMomentGeneratingFunction">Moment Generating Functions</xref> can be derived for each of the distributions in this chapter.
</p>

<theorem xml:id="MGFDiscrete"><title>Moment Generating Function for Discrete Uniform</title>
<statement>
<p>For a discrete random variable X on the space <m>R</m> = {1, 2, ..., n},
<me>M(t) = \frac{1}{n} \cdot \left [ e^t + e^{2t} + ... e^{nt} \right ] </me>
</p>
</statement>
<proof>
<p>Presuming <m>R</m> = {1, 2, ..., n},
<me>M(t) = \sum_{x=1}^n e^{tx}/n = \frac{1}{n} \cdot \left [ e^t + e^{2t} + ... e^{nt} \right ] </me>
</p>
</proof>
</theorem>

<theorem xml:id="MGFContinuous"><title>Moment Generating Function for Continuous Uniform</title>
<statement>
<p>For a continuous random variable X on the space <m>R = [a,b]</m>,
<me>M(t) = \frac{e^{bx} - e^{ax}}{b-a}</me>
</p>
</statement>
<proof>
<p>Presuming <m>R</m> = [a,b],
<me>M(t) = \int_a^b e^{tx} \frac{1}{b-a} dx = \frac{1}{b-a} \frac{1}{t} e^{tx} \big |_a^b = \frac{e^{bx} - e^{ax}}{b-a}</me>
</p>
</proof>
</theorem>


<theorem  xml:id="MGFHypergeometric"><title>Moment Generating Function for Hypergeometric</title>
<statement>
<p>For a hypergeometric random variable over the space <m>R</m> = {0, 1, ..., min(<m>r,n_</m>)}.
<me>M(t) = \text{TBA}</me>
</p>
</statement>
<proof>
<p>
<md>
<mrow>M(t) &amp; = \sum_{x=0}^{n_1} e^{tx} \frac{\binom{n_1}{x} \binom{n-n_1}{r-x}}{\binom{n}{r}}</mrow>
<mrow> &amp; = \text{a mess...}</mrow>
</md>
</p>
</proof>
</theorem>




</section>

<section xml:id="EquallyLikelySummary"><title>Summary</title>
<p>

Here is a summary of the major formulas from this chapter:

</p>
<p>
<xref ref="DiscreteUniformPF">Discrete Uniform f(x)</xref>
</p>
<p>
<xref ref="DiscreteUniformProperties">Discrete Uniform statistics</xref>
</p>
<p>
<xref ref="ContinuousUniformFunction">Continuous Uniform f(x)</xref>
</p>
<p>
<xref ref="ContinuousUniformProperties">Continuous Uniform statistics</xref>
</p>
<p>
<xref ref="HypergeometricFunction">Hypergeometric f(x)</xref>
</p>
<p>
<xref ref="HypergeometricProperties">Hypergeometric statistics</xref>
</p>


</section>

<section xml:id="EquallyLikelyExercises"><title>Exercises</title>


<exercise><title> - The Proverbial Urn Problem</title>
<statement>
<p>
You have an urn with 10 marbles of which <m>n_1 = 6</m> are red and <m>N_2 = 4</m> are blue. You select randomly r = 3 of the marbles without replacement and let <m>X</m> represent the number of red marbles in your sample. With <m>R</m> = {0, 1, 2, 3}, determine:
<ul>
	<li>f(x)</li>
	<li>P(2 of the 3 are red) = f(2)</li>
	<li>P(at most 2 of the 3 are red) = f(0) + f(1) + f(2)</li>
</ul> 
</p>
</statement>
<hint>
<p>
This is hypergeometric using  
<me>f(x) = \frac{\binom{6}{x} \binom{4}{3-x}}{\binom{10}{3}}.</me>
</p>
</hint>
</exercise>



<exercise><title> - Playing Cards</title>
<statement>
<p>
You randomly select a hand of five cards without replacement from an ordinary deck of playing cards.  
<ul>
	<li>Determine the probability that four of the five are spades.</li>
	<li>Determine the probability that three of the five are face cards (ie, Jacks, Queens, Kings).</li>
</ul>
</p>
</statement>
<hint>
<p>
This exercise is actually two different hypergeometric distributions: the first is the 13 spades vs the 39 other cards and the second is the 12 face cards vs the 40 other cards.
</p>
</hint>
<solution>
<p>
<me>\frac{\binom{13}{4} \binom{39}{1}}{\binom{52}{5}}</me>
<me>\frac{\binom{12}{3} \binom{40}{2}}{\binom{52}{5}}</me>
</p>
</solution>
</exercise>



<exercise><title> - Starting Seniors</title>
<statement>
<p>
You are picking an eleven member football starting team by picking randomly from a group with 15 seniors and 35 others. Determine:
<ul>
	<li>P(all seniors)</li>
	<li>P(exactly 6 seniors)</li>
	<li>the expected number of seniors on the team</li>
	<li>If your team has all seniors, explain whether someone could suggest that your decision on members was unfair</li>
</ul>
</p>
</statement>
<hint>
<p>
This is a hypergeometric distribution with <m>n_1 = 15, n_2 = 35, r = 11</m>.
</p>
</hint>
</exercise>



<exercise><title> - Old Faithful</title>
<statement>
<p>
Ole Faithful geyser in Yellowstone National Park erupts every 91 minutes. You show up at some random time in the eruption cycle and your tour bus plans to stay for 25 minutes. Determine the likelihood that you will be able to see it erupt.  Express your answer by giving correct formulas for <m>f(x)</m> and <m>F(x)</m> and then determine the specific answer to this question.
</p>
</statement>
<hint>
<p>
This is the continuous uniform distribution over <m>R</m> = [0, 91].
</p>
</hint>
</exercise>



<exercise><title> - Uniform Scenarios</title>
<statement>
<p>
Explain how the following situations can be modeled using either a discrete or continuous uniform distribution by identifying the space <m>R</m> and the corresponding <m>f(x)</m> for each situation.
<ul>
	<li>The location on a prize wheel where the spun wheel will stop.</li>
	<li>Given a clock with only a minute hand, the current one second interval.</li>
	<li>The location on a automobile tire where the next puncture will occur.</li>
</ul>
</p>
</statement>
</exercise>



<exercise><title> - Continous Uniform on a different space</title>
<statement>
<p>
Determine an explicit formula for <m>f(x)</m> and the mean and variance for an continuous uniform distribution over <m>R = [-2,3] \cup [5,6] \cup [9,15]</m>.
</p>
</statement>
<solution>
<p>
Since you must have 
<me>\int_{x \in R} f(x) dx = 1</me>
and since <m>f(x)</m> must be constant than all you must do is measure the accumulated width of the intervals in <m>R</m>. This is 5 + 1 + 6 = 12 and so
<me>
f(x)=\left\{\begin{matrix}
 \frac{1}{12}, &amp; -2 \le x \le 3
\\ \frac{1}{12}, &amp; 5 \le x \le 6
\\ \frac{1}{12}, &amp; 9 \le x \le 15
\\ 0, &amp; \text{otherwise}
\end{matrix}\right.
</me>
For the mean, 
	<md>
		<mrow>\int_{x \in R} x \frac{1}{12} dx &amp; = \int_{-2}^3 \frac{x}{12} dx + \int_5^6 \frac{x}{12} dx + \int_9^{15} \frac{x}{12} dx</mrow>
		<mrow> &amp; = \frac{9-4}{24} + \frac{36-25}{24} + \frac{225-81}{24}</mrow>
		<mrow> &amp; = \frac{5+11+144}{24} = \frac{160}{24} = \frac{20}{3}.</mrow>
	</md>
For the variance,
	<md>
		<mrow>\int_{x \in R} x^2 \frac{1}{12} dx - \mu^2 &amp; = \int_{-2}^3 \frac{x^2}{12} dx + \int_5^6 \frac{x^2}{12} dx + \int_9^{15} \frac{x^2}{12} dx - \mu^2</mrow>
		<mrow> &amp; = \frac{81+8}{36} + \frac{216-125}{36} + \frac{3375-729}{36} - \big ( \frac{20}{3} \big )^2</mrow>
		<mrow> &amp; = \frac{89+91+2646}{36} - \frac{400}{9} = \frac{2826-1600}{36} </mrow>
		<mrow> &amp; = \frac{1226}{36} \approx	34.055.</mrow>
	</md>
</p>
</solution>
</exercise>



<exercise><title> - Louisiana Mega Millions Lottery</title>
<statement>
<p>
To play the Mega Millions Louisiana Lottery consists of picking five numbers from 1 to 75 and one yellow Mega Ball number from 1 through 15. (You can play up to five different sets of numbers on each playslip but we will just assume one play per ticket to keep things straight.) Each play costs $1 and you can pay an additional $1 to apply a "multiplier" which multiplies any non-Jackpot prize by the Multiplier number (2, 3, 4, or 5) randomly selected at the time of the drawing.</p>
<p> On October 10, 2016 the jackpots listed were
<ul>
	<li>Match 5 plus Mega ball = Jackpot of $49,000,000 with cash value of $32,600,000</li>
	<li>Match only 5 = $1,000,000</li>
	<li>Match 4 plus Mega ball = $5,000</li>
	<li>Match only 4 =$500</li>
	<li>Match 3 plus Mega ball = $50</li>
	<li>Match only 3 = $5</li>
	<li>Match 2 plus Mega ball = $5</li>
	<li>Match 1 plus Mega ball = $2</li>
	<li>Match only the Mega ball = $1</li>
</ul>
</p>
<p>
Verify the posted odds
<ul>
	<li>Match 5 plus Mega ball = 1 in 258,890,850</li>
	<li>Match only 5 = 1 in 18,492,204</li>
	<li>Match 4 plus Mega ball = 1 in 739,688</li>
	<li>Match only 4 = 1 in 52,835</li>
	<li>Match 3 plus Mega ball = 1 in 10,720</li>
	<li>Match only 3 = 1 in 766</li>
	<li>Match 2 plus Mega ball = 1 in 473</li>
	<li>Match 1 plus Mega ball = 1 in 56</li>
	<li>Match only the Mega ball = 1 in 21</li>
</ul>
</p>
<p>
Determine the expected payout for each ticket purchased. Also, determine what the Jackpot would need to be in order for the game to be considered "fair" with an expected value of zero.
</p>
</statement>
<solution>
<p>
Throughout these calculations, you can presume that the first five numbers are selected independently from the Mega Ball number. However, the first five numbers are selected without replacement so computing probabilities with those does not allow for independence. This part is hypergeometric with the <m>n_1 = 5</m> numbers you selected being the "desired" numbers and the Lottery Commission picking a subset of size r = 5 from the 75 possible numbers.  So, your likelihood of matching all five would be
<me>\frac{\binom{5}{5} \cdot \binom{70}{0}}{\binom{75}{5}} = \frac{1}{17259390}.</me>
Multiplying this by the 1 chance in 15 that you also match the Mega Ball gives
<me>P(\text{Match 5 plus Mega Ball}) = \frac{1}{17259390} \cdot \frac{1}{15} = \frac{1}{258,890,850}.</me>
To match only 5 means you also MUST miss the Mega Ball which has probability 14/15 to give
<me>\frac{1}{17259390} \cdot \frac{14}{15} = \frac{1}{17259390 \cdot \frac{15}{14}} \approx \frac{1}{18492204}.</me>
Continue in this manner to determine the other odds.
</p>
<p>
For the expected earnings, first determine a value function corresponding to each outcome and apply the discrete expected value process. This gives
<md>
	<mrow> &amp; \$32600000 \cdot \frac{1}{258,890,850} + \$1000000 \cdot \frac{1}{18,492,204} </mrow>
	<mrow> &amp; + \$5000 \cdot \frac{1}{739,688} + \$500 \cdot \frac{1}{52,835}</mrow>
	<mrow> &amp; + \$50 \cdot \frac{1}{10,720} + \$5 \cdot \frac{1}{766} </mrow>
	<mrow> &amp; + \$5 \cdot \frac{1}{473} + \$2 \cdot \frac{1}{56} + \$1 \cdot \frac{1}{21}</mrow>
	<mrow> &amp; \approx \$0.3013.</mrow>
</md>
So, the expected payout is approximately 30 cents. Subtracting the cost of playing ($1) indicates that the average winnings per play of the Louisiana Lottery would be -70 cents. So, you would be better off to take, say, 50 cents and just give it to the local school system every time you consider playing this game rather than actually playing.
</p>
<p>
To determine the Jackpot A needed to make this a fair game means to solve the equation
<md>
	<mrow> &amp; A \cdot \frac{1}{258,890,850} + \$1000000 \cdot \frac{1}{18,492,204} </mrow>
	<mrow> &amp; + \$5000 \cdot \frac{1}{739,688} + \$500 \cdot \frac{1}{52,835}</mrow>
	<mrow> &amp; + \$50 \cdot \frac{1}{10,720} + \$5 \cdot \frac{1}{766} </mrow>
	<mrow> &amp; + \$5 \cdot \frac{1}{473} + \$2 \cdot \frac{1}{56} + \$1 \cdot \frac{1}{21}</mrow>
	<mrow> &amp; = 1</mrow>
</md>
for A.
</p>
<p>
Finally, to deal with the multiplier, note that all but the Jackpot payouts would be increased by the multiplier m where <m>m \in \{1,2,3,4,5\}</m>.  For the cost of an extra $1 (total cost of $2 per bet) the expected payout increases as the multiplier increases but each of these decreases likelihood of winning that payout by a factor of 1/5.  In general, let x = 1, 2, ..., 9 indicate the various winning options in order listed above, f(x) the corresponding probabilities listed for each option, and  u(x) the listed payouts. Then the expected payout is given by
<me> \$32600000 \cdot \frac{1}{258,890,850} + \sum_{m=1}^5
\sum_{x=2}^9 m \cdot u(x) f(x)/5 </me>
or
	<md>
		<mrow> \$32600000 \cdot \frac{1}{258,890,850} &amp; + \sum_{m=1}^5 \frac{m}{5} \sum_{x=2}^9 u(x) f(x) </mrow>
		<mrow> &amp; = \frac{\$32600000}{258890850} + \sum_{m=1}^5 \frac{m}{5} 0.17539</mrow>
		<mrow> &amp; = 0.12592 + 3 \cdot 0.17539</mrow>
		<mrow> &amp; = 0.65209</mrow>
	</md>
Therefore, the expect value of spending another dollar to get the multiplier effect is about -$1.35.  Since this is slightly less than doubling the expected loss of 70 cents for playing without the multiplier with $1, it make more sense to bet $2 once rather than betting $1 twice.  Or, you can send the extra nickel to this author of this text and call it quits.
</p>

</solution>
</exercise>


</section>


</chapter>
