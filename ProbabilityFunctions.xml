<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="ProbabilityFunctions" xmlns:xi="http://www.w3.org/2001/XInclude">
<title>Probability Functions</title>

<section xml:id="ProbabilityFunctionsIntro"><title>Probability Niches</title>
	
<p>Each of the probability exercises thus far required you to utilize basic definitions and theorems to determine the answer. Starting a new problem meant starting over from scratch. This is burdensome.  However, you may have noticed that some of the ways you might have created solutions for some problems ending up looking very similar to the solutions for others. In this chapter, you will consider the framework needed for creating general solution techniques. These techniques will give a number of "distributions" which are general ways to solve a particular type of problem.
</p>

<p>Toward that end, in this chapter you will see how to create a random variable which takes items in the sample space and assigns corresponding numerical values. From that, you will see how to create "Probability Functions" on that variable that provide the desired probability by simple function evaluation. General properties these functions possess will also be developed.
</p>

</section>


<section xml:id="RandomVariables"><title>Random Variables</title>

<p>For a given set of events, we might have difficulty doing mathematics since the outcomes
are not numerical. In order to accomodate our desire to convert to numerical measures we want
to assign numerical values to all outcomes. The process of doing this creates what is known as a random
variable.
</p>

<p>
<definition xml:id="DefnRandomVariable"><title>Random Variable</title>
	<statement>
		<p>Given a random experiment with sample space <m>S</m>, a function <m>X</m> mapping each 
		element of <m>S</m> to a unique real number is called a random variable. 
		For each element s from the sample space <m>S</m>, denote this function by
		<me>X(s) = x</me>
		and let <m>R</m>  be the range of <m>X.</m></p>
		<p><m>R</m> will be called "the space of X" and in notation
		<me> R = { x : X(s)=x, \text{for some} s \in S } .</me>
		</p>
	</statement>
</definition>
</p>

<image source="images/randomvariable.png" />

<p>We will make various restrictions on the range of the <xref ref="DefnRandomVariable">random variable</xref> to fit different 
generalized problems. Then, we will be able to work on a problem (which may be 
inherently non-numerical) by using the random variable in subsequent calculations.
</p>

<p>
<example><title>Success vs Failure</title>
	<p>When dealing with only two outcomes, one might use 
	<me>S = \text{{ success, failure }}.</me>
	Choose 
	<md>
		<mrow>X( \text{success})=1</mrow>
		<mrow>X( \text{failure})=0.</mrow>
	</md>
	Then, <m>R</m> = {0,1}.
	</p>
</example>
</p>

<p>
<example>
	<title>Standard Dice Pairs</title>
	<p>When gambling with a pair of dice, one might use
	S=ordered pairs of all possible rolls.  Then 
	<me>S = \text{ {(a,b): a=die 1 outcome, b=die 2 outcome}}.</me>
	Choose 
	<me>X( (a,b) ) = a+b.</me> 
	Then, <m>R</m> = {2, 3, 4, 5, ..., 12}.
	</p>
</example>
</p>

<p>
<example>
	<title>Other Dice Options</title>
	<p>When rolling dice in a board game (like RISK), one might use
	<me>S= \text{{ (a,b) : a=die 1 outcome, b=die 2 outcome}}</me>
	Choose 
	<me>X( (a,b) ) = \text{max{a,b}}.</me> 
	Then, <m>R</m> = {1, 2, 3, 4, 5, 6}.
	</p>
</example>
</p>

<p>
<definition><title>Countable and Uncountable Sets</title>
	<statement>
		<p>
		<m>R</m> contains a countable number of points if either <m>R</m> is finite or there 
		is a one to one correspondence between R and the positive integers. 
		Such a set will be called discrete. We will see that often the set <m>R</m> is not countable. 
		If <m>R</m> consists of an interval of points (or a union of intervals), 
		then we call X a continuous random variable. 
		</p>
	</statement>
</definition>
</p>	

<p>
Notice that in each of the examples above, the space <m>R</m> was finite. Therefore, each of those random variables would be considered countable. One can however have random variables with a infinite number of values in <m>R</m> and yet still be countable.
</p>

<p>
<example>
<p>
Consider the random variable <m>X</m> representing the number of coin flips <em>until</em> you get a Heads.  Even though it is unlikely to have to keep on going, consider the fact that you <em>could</em> continually flip and get Tails every time for a long time.  So, the possible values for <m>X</m> would be <m>R</m> = {1, 2, 3, 4, ... } which is precisely the infinite set of positive integers.  Therefore, <m>X</m> is a countable random variable. 
</p>
</example>
</p>

<p>
<example>
<p>
Consider the random variable <m>X</m> representing the time until a light bulb fails to work after being successfully turned on.  Notice that the bulb <em>could</em> fail in a relatively short time (near 0 let's say) or <em>could</em> continue to last for almost eternity (or as long as you might want to measure).  However, time is measured on a continuum and therefore for this random variable <m>X</m> you would have <m>R</m> equal to the interval from 0 to infinity, i.e. <m>R = (0,\infty)</m> which is not a countable set.
</p>
</example>
</p>

</section>


<section xml:id="ProbabilityFunctionsDefined"><title>Probability Functions</title>

<p>In the formulas below, we will presume that we have a <xref ref="DefnRandomVariable">random variable</xref>
 <m>X</m> which maps the sample space S onto some range of real numbers <m>R</m>.  From this set, we then can define a probability function <m>f(x)</m> which acts on the numerical values in <m>R</m> and returns another real number.  We attempt to do so to obtain (for discrete values) P(sample space value s)<m> = f(X(s))</m>.  That is, the probability of a given outcome s is equal to the composition which takes s to a numerical value x which is then plugged into f to get the same final values.
</p>
<p>For example, consider a random variable which assigns a 1 when you roll a 1 on a six-sided die and 0 otherwise. Presuming each side is equally likely, <m>f(1) = \frac{1}{6}</m> and <m>f(0) = \frac{5}{6}</m>.
</p>

<p>
<definition xml:id="DefnPMF"><title>Probability "Mass" Function</title>
	<statement>
		<p>Given a discrete <xref ref="DefnRandomVariable">random variable</xref> <m>X</m> on a space <m>R</m>, a probability mass function on <m>X</m> is given by a function <m>f:R \rightarrow \mathbb{R}</m> such that:
		<md>
			<mrow>&amp; \forall x \in R , f(x) \gt 0</mrow>
			<mrow>&amp; \sum_{x \in R} f(x) = 1</mrow>
			<mrow>&amp; A \subset R \Rightarrow P(X \in A) = \sum_{x \in A}f(x)</mrow>
		</md>
		For <m>x \not\in R</m>, you can use the convention <m>f(x)=0</m>.
		</p>
	</statement>
</definition>
</p>

<p>	
<definition xml:id="DefnPDF"><title>Probability "Density" Function</title>
	<statement>
	<p>Given a continuous random variable <m>X</m> on a space <m>R</m>, a probability density function on <m>X</m> is given by a function <m>f:R \rightarrow \mathbb{R}</m> such that:
		<md>
			<mrow>&amp; \forall x \in R , f(x) \gt 0</mrow>
			<mrow>&amp; \int_{R} f(x) dx = 1</mrow>
			<mrow>&amp; A \subset R \Rightarrow P(X \in A) = \int_{A} f(x) dx</mrow>
		</md>
		For <m>x \not\in R</m>, you can use the convention <m>f(x)=0</m>.
	</p>
	</statement>
</definition>
</p>

<p>
For the purposes of this book, we will use the term "Probability Function" to refer to either of these options.
</p>
	
<p>
<example><title>Discrete Probability Function</title>
<p>
Consider <m>f(x) = x/10</m> over R = {1,2,3,4}.  Then, f(x) is obviously positive for each of the values in R and certainly 
<me>\sum_{x \in R} f(x) = f(1) + f(2) + f(3) + f(4) = 1/10 + 2/10 + 3/10 + 4/10 = 1.</me> 
Therefore, f(x) is a probability mass function over the space <m>R</m>.
</p>
</example>
</p>

<p>
<sage><title>Sampling Discrete Probability Function</title>
<input>
# Combining all of the above into one interactive cell
@interact
def _(D = input_box([1,2,3,5,6,8,9,11,12,14],label="Domain (in brackets):"), 
       Probs = input_box([1/20,1/20,1/20,3/20,1/20,4/20,4/20,1/20,1/20,3/20],label=" $$f(x)$$ (in brackets):"),
       n_samples=slider(100,10000,100,100,label="# if samples:")):
    n = len(D)
    R = range(n)
    one_huh = sum(Probs)

    if one_huh!=1:
        print("f(x) values do not sum to 1")
    else:
        G = Graphics()
        if len(D)==len(Probs):
            f = zip(D,Probs)
            meanf = 0
            variancef = 0
            for k in R:
                meanf += D[k]*Probs[k]
                variancef += D[k]^2*Probs[k]
                G += line([(D[k],0),(D[k],Probs[k])],color='green')
            variancef = variancef - meanf^2
            sd = sqrt(variancef)
            G += points(f,color='blue',size=50)
            G += point((meanf,0),color='yellow',size=60,zorder=3)
            G += line([(meanf-sd,0),(meanf+sd,0)],color='red',thickness=5)
    
            g = DiscreteProbabilitySpace(D,Probs)
            pretty_print("     mean = %s"%str(meanf))
            pretty_print(" variance = %s"%str(variancef))
    
        #  perhaps to add mean and variance for pmf here
        else:
            print("Domain D and Probabilities Probs must be lists of the same size")
    
    #  Now, let's sample from the distribution given above and see how a random sampling matches up

        counts = [0] * len(Probs)
        X = GeneralDiscreteDistribution(Probs)
        sample = []

        for _ in range(n_samples):
            elem = X.get_random_element()
            sample.append(D[elem])
            counts[elem] += 1
        Empirical = [1.0*x/n_samples for x in counts] # random
    
        samplemean = mean(sample)
        samplevariance = variance(sample)
        sampdev = sqrt(samplevariance)
    
        E = points(zip(D,Empirical),color='orange',size=40)
        E += point((samplemean,0.005),color='brown',size=60,zorder=3)
        E += line([(samplemean-sampdev,0.005),(samplemean+sampdev,0.005)],color='orange',thickness=5)    
        (G+E).show(ymin=0,figsize=(8,5))	
</input>
</sage>
</p>

<p>
<example><title>Continuous Probability Function</title>
<p>
	Consider <m>f(x) = x^2/c</m> for some positive real number c and presume <m>R</m> = [-1,2]. Then f(x) is nonnegative (and only equals zero at one point). To make <m>f(x)</m> a <xref ref="DefnPDF">probability density function</xref>, we must have
	<me>\int_{x \in R} f(x) = 1.</me>
	In this instance you get
	<me>1 = \int_{-1}^2 x^2/c = x^3/(3c) |_{-1}^2 = \frac{8}{3c} - \frac{-1}{3c} = \frac{3}{c}</me>
	Therefore, <m>f(x)</m> is a probability density function over <m>R</m> provided  <m>c = 3</m>.
</p>
</example>
</p>

<p>	
<definition xml:id="DefnDistributionFunction"><title>Distribution Function</title>
	<statement>
		<p>Given a random variable <m>X</m> on a space <m>R</m>, a probability distribution function on <m>X</m> is given by a function 
		<me>F:\mathbb{R} \rightarrow \mathbb{R} \ni \displaystyle F(x)=P(X \le x).</me>
		</p>
	</statement>
</definition>
</p>
	
<p>
<example><title>Discrete Distribution Function</title>
	<p>Using <m>f(x) = x/10</m> over <m>R</m> = {1,2,3,4} again, note that <m>F(x)</m> will only change at these four domain values. We get
	
	<table halign="left">
		<title>Discrete Distribution Function Example</title>
      	<tabular halign="right">
      
<row><cell bottom="medium" right="medium">X</cell><cell bottom="medium">F(x)</cell></row>      
<row><cell right="medium"><m>x \lt 1</m></cell><cell>0</cell></row>
<row><cell right="medium"><m>1 \le x \lt 2</m></cell><cell>1/10</cell></row>
<row><cell right="medium"><m>2 \le x \lt 3</m></cell><cell>3/10</cell></row>
<row><cell right="medium"><m>3 \le x \lt 4</m></cell><cell>6/10</cell></row>
<row><cell right="medium"><m>4 \le x </m></cell><cell>1</cell></row>
		</tabular>
	</table>
	</p>
</example>
</p>

<p>	
<example><title>Continuous Distribution Function</title>
	<p>
	Consider <m>f(x) = x^2/3</m> over <m>R</m> = [-1,2].  Then, for <m>-1 \le x \le 2</m>,
	<me>F(x) = \int_{-1}^x u^2/3 du = x^3/9 + 1/9.</me>
	Notice, <m>F(-1) = 0</m> since nothing has yet been accumulated over values smaller than -1 and <m>F(2) = 1</m> since by that time everything has been accumulated. In summary:
	
	<table halign="left">
		<title>Continuous Distribution Function Example</title>
      	<tabular halign="right">
      
<row><cell bottom="medium" right="medium">X</cell><cell bottom="medium">F(x)</cell></row>      
<row><cell right="medium"><m>x \lt -1</m></cell><cell>0</cell></row>
<row><cell right="medium"><m>-1 \le x \lt 2</m></cell><cell><m>x^3/9 + 1/9</m></cell></row>
<row><cell right="medium"><m>2 \le x</m></cell><cell>1</cell></row>
	
		</tabular>
	</table>
		
	</p>
</example>
</p>

<!--

<exercise><title>WebWork</title>

<webwork source="Library/UBC/STAT/STAT241_251/setAssignment-02/HW02-01/HW02-01a.pg">
		</webwork>
</exercise>

-->

<p>		
<theorem xml:id="theorem-Fmin">
	<statement>
		<p><m>F(x)=0, \forall x \lt \inf(R)</m> where inf is the infimum...the "minimum" but in a limit sense.
		</p>
	</statement>
	<proof>
		<p>
		Let a = inf(<m>R</m>). Then, for <m>x \lt a,</m>
		<me>F(x) = P(X \le x) \le P(X \lt a) = 0</me> 
		since none of the x-values in this range are in <m>R</m>.
		</p>
	</proof>
</theorem>
</p>

<p>	
<theorem xml:id="theorem-Fmax">
	<statement>
	<p>
	<m>F(x)=1, \forall x \ge \sup(R)</m> where sup is the supremum...the "maximum" but in a limit sense.
	</p>
	</statement>
	<proof>
		<p>
		Let b = sup(<m>R</m>). Then, for <m>x \ge b,</m> 
		<me>F(x) = P(X \le x)  = P(X \le b) + P( b \lt X \le x) = P(X \le b) = 1</me> 
		since all of the x-values in this range are in R and therefore will either sum over or integrate over all of <m>R</m>.
		</p>
		
	</proof>
</theorem>
</p>

<p>	
<theorem>
	<statement xml:id="theorem-F-non-decreasing">
	<p>
	<m>F</m> is non-decreasing
	</p>
	</statement>
	<proof>
		<p>Case 1: <m>R</m> discrete
			<md>
				<mrow>\forall x_1,x_2 \in \mathbb{Z} \ni x_1 \lt x_2</mrow>
				<mrow>F(x_2) &amp; = \sum_{x \le x_2} f(x) </mrow>
				<mrow>&amp; = \sum_{x \le x_1} f(x) + \sum_{x_1 \lt x \le x_2} f(x)</mrow>
				<mrow>&amp; \ge \sum_{x \le x_1} f(x) = F(x_1)</mrow>
			</md>
		</p>
		<p>Case 2: <m>R</m> continuous
			<md>
				<mrow>\forall x_1,x_2 \in \mathbb{R} \ni x_1 \lt x_2</mrow>
				<mrow>F(x_2) &amp; = \int_{-\infty}^{x_2} f(x) dx </mrow>
				<mrow> &amp; = \int_{-\infty}^{x_1} f(x) dx + \int_{x_1}^{x_2} f(x) dx</mrow>
				<mrow> &amp; \ge \int_{-\infty}^{x_1} f(x) dx</mrow>
				<mrow> &amp; = F(x_1)</mrow>
			</md>
		</p>
		</proof>
	</theorem>
</p>
	
<p>
<theorem xml:id="theorem-Fvsf-discrete">
<title>Using Discrete Distribution Function to compute probabilities</title>
	<statement>
	<p>For <m>x \in R, f(x) = F(x) - F(x-1)</m>
	</p>
	</statement>
	<proof>
		<p>Assume <m>x \in R</m> for some discrete <m>R</m>. Then,
		<me>F(x) - F(x-1) = \sum_{u \le x} f(u) - \sum_{u \lt x} f(u) = f(x)</me>
		</p>
	</proof>
</theorem>
</p>

<p>	
<theorem xml:id="theorem-Fvsf-continuous">
<title>Using Continuous Distribution function to compute probabilities</title>
	<statement>
	<p>For <m>a \lt b, (a,b) \in R, P(a \lt X \le b) = F(b) - F(a)</m>
	</p>
	</statement>
	<proof>
		<p>
		For a and b as noted, consider 
		<md>
		<mrow>F(b) - F(a) &amp; = \int_{-\infty}^b f(x) dx - \int_{-\infty}^a f(x) dx</mrow>
		<mrow> &amp; = \int_a^b f(x) dx </mrow>
		<mrow> &amp; = P(a \lt x \le b)</mrow>
		</md>
		</p>
	</proof>
</theorem>
</p>
	
<p>
<corollary xml:id="corollary-ProbPointZero-continuous">
	<statement>
	<p>For continuous distributions, <m>P(X = a) = 0</m>.
	</p>
	</statement>
	<proof>
		<p>
		We will assume that <m>F(x)</m> is a continuous function. With that assumption, note
		<me>P(a-\epsilon \lt  x \le a)  = \int_{a-\epsilon}^a f(x) dx = F(a) - F(a-\epsilon)</me>
		Take the limit as <m> \epsilon \rightarrow 0^+</m> to get the result noting that
		</p>
	</proof>
</corollary>
</p>	

<p>
<theorem><title><m>F(x)</m> vs <m>f(x)</m>, for continuous distributions</title>
	<statement>
		<p>If <m>X</m> is a continuous random variable, <m>f</m> the corresponding probability function, and <m>F</m> the associated distribution function, then
		<me>f(x) = F'(x)</me>
		</p>
	</statement>
	<proof>
	<p>Assume <m>X</m> is continuous and <m>f</m> and <m>F</m> as above. Notice, by the definition of <m>f</m>, <m>\lim_{x \rightarrow \pm \infty} f(x) = 0</m> since otherwise the integral over the entire space could not be finite.
	</p>
	<p>
	Now, let <m>A(x)</m> be any antiderivative of <m>f(x)</m>. Then, by the Fundamental Theorem of Calculus,
	<md>
		<mrow>F(x) &amp; = \int_{-\infty}^x f(u) du</mrow>
		<mrow> &amp; = A(x) - \lim_{u \rightarrow -\infty} A(u)</mrow>
	</md>
	Hence, <m>F'(x) = A'(x) - \lim_{u \rightarrow -\infty} A'(u) = f(x)</m> as desired.
	</p>
	</proof>
</theorem>
</p>

<p>
<definition xml:id="DefnPercentileRandomVariable"><title>Percentiles for Random Variables</title>
<statement>
<p>
For <m>0 \lt p \lt 1</m>, the <m>100p^{th}</m> percentile is the largest random variable value c that satisfies
<me>F(c) = p.</me>
For continuous random variables over an interval <m>R = [a,b]</m>, you will solve for c in the equation
<me>\int_a^c f(x) dx.</me>
For discrete random variables, it is unlikely that a particular percentile will land exactly on one of the elements of <m>R</m> but you will want to take the smallest value in <m>R</m> so that <m>F(c) \ge p</m>.
</p>
<p>
The 50th percentile (as before) is also known as the median.
</p>
</statement>
</definition>
</p>

<p>
<example><title>Continuous Percentile</title>
<p>
For our earlier example with <m>f(x) = x^2/3</m> on R = [-1,2], the 50th percentile (i.e. the median) is found by starting with p = 0.5 and then solving 
<me>F(c) = 0.5</me>
or
<me>c^3/9 + 1/9 = 1/2</me>
or 
<me>c^3 + 1 = 9/2.</me>
After solving for c, you find
<me>\text{median} = \sqrt[3]{7/2} \approx 1.518.</me> 
</p>
</example>
</p>

<p>
<example><title>Discrete Percentile</title>
<p>
TBA, using one of the table examples from above.
</p>
</example>
</p>
		
</section> 

<section xml:id="ExpectedValue"><title>Expected Value</title>
	<p>Blaise Pascal was a 	17th century mathematician and philosopher who was accomplished in many areas but may likely be best known to you for his creation of what is now known as Pascal's Triangle. As part of his philosophical pursuits, he proposed what is known as "Pascal's wager". It suggests two  mutually exclusive outcomes: that God exists or that he does not. His argument is that a rational person should live as though God exists and seek to believe in God. If God does not actually exist, such a person will have only a finite loss (some pleasures, luxury, etc.), whereas they stand to receive infinite gains as represented by eternity in Heaven and avoid an infinite losses of eternity in Hell. This type of reasoning is part of what is known as "decision theory".
	</p>
	<p>You may not confront such dire payouts when making your daily decisions but we need a formal method for making these determinations precise. The procedure for doing so is what we call expected value.
	</p>
	
	<p>
	<definition xml:id="DefnExpectedValue"><title>Expected Value</title>
	<p>Given a random variable <m>X</m> over space <m>R</m>, corresponding probability function <m>f(x)</m> and "value function" <m>v(x)</m>, the expected value of <m>v(x)</m> is given by
	<me>E = E[v(X)] = \sum_{x \in R} v(x) f(x)</me>
	provided <m>X</m> is discrete, or
	<me>E = E[v(X)] = \int_R v(x)f(x) dx</me>
	provided <m>X</m> is continuous.
	</p>
	</definition>
	</p>
	
<p>
<theorem><title>Expected Value is a Linear Operator</title>
	<statement>
	<p>
	<ol>
		<li><m>E[c] = c</m></li>
		<li><m>E[c v(X)] = c E[v(X)]</m></li>
		<li><m>E[v(X) + w(X)] = E[v(X)] + E[w(X)]</m></li>
	</ol>
	</p>
	</statement>
	<proof>
	<p>Each of these follows by utilizing the corresponding linearity properties of the summation and integration operations. For example, to verify part three in the continuous case:
	<md>
		<mrow>E[v(X) + w(X)] &amp; = \int_{x \in R} [v(x)+w(x)]f(x) dx</mrow>
		<mrow> &amp; = \int_{x \in R} v(x)f(x) dx + \int_{x \in R} w(x)f(x) dx</mrow>
		<mrow> &amp; = E[v(X)] + E[w(X)].</mrow>
	</md>
	</p>
	</proof>
</theorem>
</p>
	



<exercise><title>WebWork - Expected Value</title>
		<webwork source="Library/Rochester/setProbability17Expectation/ur_pb_17_4.pg">
		</webwork>
</exercise>



	<p>
	<example><title>Discrete Expected Value</title>
	<p>Consider <m>f(x) = x/10</m> over <m>R</m> = {1,2,3,4} where the payout is 10 euros if x=1, 5 euros if x=2, 2 euros if x=3 and -7 euros if x = 4.  Then your value function would be 
	<me>v(1)=10, v(2) = 5, v(3)=2, v(4) = -7.</me> 
	Computing the expect payout gives
	<me>E = 10 \times 1/10 + 5 \times 2/10 + 2 \times 3/10 - 7 \times 4/10 = -2/10</me>
	Therefore, the expected payout is actually negative due to a relatively large negative payout associated with the largest likelihood outcome and the larger positive payout only associated with the least likely outcome.
	</p>
	</example>
	</p>
	
	<p>
	<example xml:id="ContinuousEV-example"><title>Continuous Expected Value</title>
	<p>
	Consider <m>f(x) = x^2/3</m> over <m>R</m> = [-1,2] with value function given by <m>v(x) = e^x - 1</m>. Then, the expected value for <m>v(x)</m> is given by
	<me>E = \int_{-1}^2 (e^x-1) \cdot x^2/3 = -1/9 \cdot (e + 15) \cdot e^{-1} + 2/3 \cdot e^2 - 8/9 \approx 3.3129</me>
	</p>
	</example>
	</p>
	
	<p>
	<definition xml:id="TheoreticalMeasures"><title>Theoretical Measures</title>
	<statement>
	<p>
	Given a random variable with probability function f(x) over space R
	<ol>
		<li xml:id="TheoreticalMean">The mean of <m>X = \mu = E[x]</m></li>
		<li xml:id="TheoreticalVariance">The variance of <m>X = \sigma^2 = E[(x-\mu)^2]</m></li>
		<li xml:id="TheoreticalSkewness">The skewness of <m>X = \gamma_1 = \frac{E[(x-\mu)^3]}{\sigma^3}</m></li>
		<li xml:id="TheoreticalKurtosis">The kurtosis of <m>X = \gamma_2 = \frac{E[(x-\mu)^4]}{\sigma^4}</m></li>
	</ol>
	</p>
	</statement>
	</definition>
	</p>
	
	<p>
	<theorem xml:id="TheoreticalMeasuresAlternates"><title>Alternate Formulas for Theoretical Measures</title>
	<statement>
	<p>
	<ol>
		<li xml:id="TheoreticalMeasuresAlternatesVar"><m>\sigma^2 = E[x^2] - \mu^2 = E[X(x-1)] + \mu - \mu^2</m></li>
		<li xml:id="TheoreticalMeasuresAlternatesSkew"><m>\gamma_1 = \frac{1}{\sigma^3} \cdot \left [ E[X^3] - 3 \mu E[X^2] + 2\mu^3 \right ]</m></li>
		<li xml:id="TheoreticalMeasuresAlternatesKurt"><m>\gamma_2 = \frac{1}{\sigma^4} \cdot \left [ E[X^4] - 4 \mu E[X^3] + 6\mu^2 E[X^2] - 3 \mu^4 \right ]</m></li>
	</ol>
	</p>
	</statement>
	<proof>
		<p>
		In each case, expand the binomial inside and use the linearity of expected value.
		</p>
	</proof>
	</theorem>
	</p>
	
	<p>
	Consider the following example when computing these statistics for a discrete variable. In this case, we will utilize a variable with a relatively small space so that the summations can be easily done by hand. Indeed, consider
	
	<table halign="left">
		<title>Discrete Probability Function Example</title>
      	<tabular halign="right">
      
<row><cell bottom="medium" right="medium">X</cell><cell bottom="medium">f(x)</cell></row>      
<row><cell right="medium">0</cell><cell>0.10</cell></row>
<row><cell right="medium">1</cell><cell>0.25</cell></row>
<row><cell right="medium">2</cell><cell>0.40</cell></row>
<row><cell right="medium">4</cell><cell>0.15</cell></row>
<row><cell right="medium">7</cell><cell>0.10</cell></row>
		</tabular>
	</table>
	
	<image source="images/DiscreteHistogramExample.png" />
	</p>
	<p>
	Using the definition of <xref ref="TheoreticalMean">mean</xref> as a sum,
	<md>
		<mrow>\mu &amp; = 0 \cdot 0.10 + 1 \cdot 0.25 + 2 \cdot 0.40 + 4 \cdot 0.15 + 7 \cdot 0.10</mrow>
		<mrow> &amp; = 0 + 0.25 + 0.80 + 0.60 + 0.70</mrow>
		<mrow> &amp; = 2.35</mrow>
	</md>
	Notice where this lies on the probability histogram for this distribution.
	</p>
	
	<p>For the <xref ref="TheoreticalVariance">variance</xref> and using <xref ref="TheoreticalMeasuresAlternatesVar">the alternate formulation</xref>
	<md>
		<mrow>\sigma^2 &amp; = E[X^2] - \mu^2</mrow>
		<mrow> &amp; = \left [ 0^2 \cdot 0.10 + 1^2 \cdot 0.25 + 2^2 \cdot 0.40 + 4^2 \cdot 0.15 + 7^2 \cdot 0.10 \right ] - 2.35^2</mrow>
		<mrow> &amp; = 0 + 0.25 + 1.60 + 2.40 + 4.90 - 5.5225</mrow>
		<mrow> &amp; = 9.15 - 5.225</mrow>
		<mrow> &amp; = 3.6275 </mrow>
	</md>	
	and so the standard deviation <m>\sigma = \sqrt{3.6275} \approx 1.90</m>. Notice that 4 times this value encompasses almost all of the range of the distribution.
	</p>
	
	<p>For the <xref ref="TheoreticalSkewness">skewness</xref> and using <xref ref="TheoreticalMeasuresAlternatesSkew">the alternate formulation</xref>
	<md>
		<mrow> \text{Numerator = } &amp; E[X^3] - 3 \mu E[X^2] + 2\mu^3</mrow>
		<mrow> &amp; = \left [ 0^3 \cdot 0.10 + 1^3 \cdot 0.25 + 2^3 \cdot 0.40 + 4^3 \cdot 0.15 + 7^3 \cdot 0.10 \right ] - 3 \cdot 2.35 \cdot 9.15 + 2 \cdot 2.35^3</mrow>
		<mrow> &amp; \approx 0 + 0.25 + 3.20 + 9.60 + 34.3 - 64.5075 + 25.96</mrow>
		<mrow> &amp; = 47.35 - 64.5075 + 25.96</mrow>
		<mrow> &amp; \approx 8.80</mrow>
	</md>
	which yields a skewness of <m>\gamma_1 = 8.80 / \sigma^3 \approx 1.27 </m>. This indicates a slight skewness to the right of the mean. You can notice the 4 and 7 entries on the histogram illustrate a slight trailing off to the right.
	</p>
	
	<p>Finally, for <xref ref="TheoreticalKurtosis">kurtosis</xref> and using <xref ref="TheoreticalMeasuresAlternatesKurt">the alternate formulation</xref>
	<md>
		<mrow> \text{Numerator = } &amp; E[X^4] - 4 \mu E[X^3] + 6 \mu^2 E[X^2] - 3\mu^4</mrow>
		<mrow> &amp; = \left [ 0^4 \cdot 0.10 + 1^4 \cdot 0.25 + 2^4 \cdot 0.40 + 4^4 \cdot 0.15 + 7^4 \cdot 0.10 \right ] - 4 \cdot 2.35 \cdot 47.35 + 6 \cdot 2.35^2 \cdot 9.15^2 - 3 \cdot 2.35^4</mrow>
		<mrow> &amp; \approx 0 + 0.25 + 6.40 + 38.4 + 240.1 - 445.09 + 303.19 - 91.49</mrow>
		<mrow> &amp; \approx 285.15 - 445.09 + 303.19 - 91.49</mrow>
		<mrow> &amp; \approx 51.75</mrow>
	</md>
	which yields a kurtosis of <m>\gamma_2 = 51.75 / \sigma^4 \approx 3.93</m> which also notes that the data appears to have a modestly bell-shaped distribution.
	</p>



<exercise><title>WebWork - Random Variables</title>

		<webwork source="Library/Mizzou/Finite_Math/Probability_Random_Variables/EV1.pg">
		</webwork>
</exercise>



<exercise><title>WebWork - More Random Variables</title>
		<webwork source="Library/Rochester/setProbability7RandomVariables/ur_pb_7_6.pg">
		</webwork>
</exercise>

	

<exercise><title>WebWork - Even More</title>
		<webwork source="Library/UBC/STAT/STAT241_251/setAssignment-01/HW01-01.pg">
		</webwork>
</exercise>



	<p>
	Consider the following example when computing these statistics for a continuous variable. 
	Let <m>f(x) = \frac{3}{4} \cdot (1-x^2)</m> over <m>R = [-1,1]</m>.  
		<image source="images/ContinuousDistributionExample.png" />
	</p>
	<p>
	Then for the <xref ref="TheoreticalMean">mean</xref>
	<md>
		<mrow>\mu &amp; = \int_{-1}^1 x \cdot \frac{3}{4} \cdot (1-x^2) dx</mrow>
		<mrow> &amp; = \int_{-1}^1 \frac{3}{4} \cdot (x-x^3) dx</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot (x^2/2-x^4/4) \big |_{-1}^1</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot [(1/2)-(1/4)] - [(1/2) - (1/4)]</mrow>
		<mrow> &amp; = 0</mrow>
	</md>
	as expected since the probability function is symmetric about x=0.
	</p>
	
	<p>For the <xref ref="TheoreticalMeasuresAlternates">variance</xref>
	<md>
		<mrow>\sigma^2 &amp; = \int_{-1}^1 x^2 \cdot \frac{3}{4} \cdot (1-x^2) dx - \mu^2</mrow>
		<mrow> &amp; = \int_{-1}^1 \cdot \frac{3}{4} \cdot (x^2-x^4) dx - 0</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot (x^3 /3 -x^5 / 5) \big |_{-1}^1</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot 2 \cdot (1/3-1/5)</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot \frac{4}{15}</mrow>
		<mrow> &amp; = \frac{1}{5}</mrow>
	</md>
	and taking the square root gives a standard deviation slightly less than 1/2. Notice that four times this value encompasses almost all of the range of the distribution.
	</p>
	
	<p>For the <xref ref="TheoreticalSkewness">skewness</xref>, notice that the graph is symmetrical about the mean and so we would expect a skewness of 0.  Just to check it out
	<md>
		<mrow> \text{Numerator = } &amp; E[X^3] - 3 \mu E[X^2] + 2\mu^3</mrow>
		<mrow> &amp; = \int_{-1}^1 x^3 \cdot \frac{3}{4} \cdot (1-x^2) dx - 3 E[X^2] \cdot 0 + 0^3 </mrow>
		<mrow> &amp; = \int_{-1}^1 \cdot \frac{3}{4} \cdot (x^3-x^5) dx</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot (x^4/4-x^6/6) \big |_{-1}^1</mrow>
		<mrow> &amp; = 0</mrow>
	</md>
	as expected without having to actually complete the calculation by dividing by the cube of the standard deviation.
	</p>
	
	<p>Finally, note that the probability function in this case is modestly close to a bell shaped curve so we would expect a <xref ref="TheoreticalKurtosis">kurtosis</xref> and using <xref ref="TheoreticalKurtosis">the alternate formulation</xref> in the vicinity of 3. Indeed, noting that (conveniently) <m>\mu = 0</m> gives
	<md>
		<mrow> \text{Numerator = } &amp; E[X^4] - 4 \mu E[X^3] + 6 \mu^2 E[X^2] - 3 \mu^4</mrow>
		<mrow> &amp; = \int_{-1}^1 x^4 \cdot \frac{3}{4} \cdot (1-x^2) dx</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot (x^5 /5-x^7 /7) \big |_{-1}^1</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot 2(1/5-1/7)</mrow>
		<mrow> &amp; = \frac{3}{35}</mrow>
	</md>
	and so by dividing by <m>\sigma^4 = \sqrt{\frac{1}{5}}^4 = \frac{1}{25}</m> gives a kurtosis of
	<me>\gamma_2 = \frac{3}{35} / \frac{1}{25} = \frac{75}{35} \approx 2.14.</me>
	</p>
	
<p>
<example>
<p>
Consider <xref ref="ContinuousEV-example">our previous example</xref>. To compute the <xref ref="TheoreticalMean">mean</xref> and <xref ref="TheoreticalVariance">variance</xref> (and hence the standard deviation) for this distribution,
<me>\mu = \int_{-1}^2 x \cdot x^2/3 dx = \int_{-1}^2 x^3/3 dx = \frac{2^4}{12} - \frac{(-1)^4}{12} = \frac{15}{12} = \frac{5}{4}</me>
and by using <xref ref="TheoreticalMeasuresAlternates">the alternate formulas</xref>
<md>
	<mrow>\sigma^2 &amp; = E[X^2] - \mu^2</mrow>
	<mrow> &amp; = \int_{-1}^2 x^2 \cdot x^2/3 \; dx - \mu^2</mrow>
	<mrow> &amp; = \int_{-1}^2 x^4/3 \; dx - \left ( \frac{5}{4} \right )^2</mrow>
	<mrow> &amp; = \frac{2^5}{15} - \frac{(-1)^5}{15} - \frac{25}{16}</mrow>
	<mrow> &amp; = \frac{33}{15} - \frac{25}{16} = \frac{51}{80}</mrow>
</md>
which gives 
<me>\sigma = \sqrt{\frac{51}{80}} \approx 0.7984.</me>
</p>
<p>
For <xref ref="TheoreticalSkewness">skewness</xref>, note that in computing the variance above you also found that
<me>E[X^2] = \frac{11}{5}.</me>
So, once again by using <xref ref="TheoreticalMeasuresAlternates">the alternate forumulas</xref>
<me>E[X^3] = \int_{-1}^2 x^3 \cdot x^2/3 dx = \frac{x^6}{18} |_{-1}^2 = \frac{7}{2}</me>
and so
<me>\gamma_1 = \frac{\frac{7}{2} - 3 \cdot \frac{5}{4} \cdot \frac{11}{5} + 2 \cdot (\frac{5}{4})^3}{\sqrt{\frac{51}{80}}^3}</me>
</p>
<p>
For <xref ref="TheoreticalKurtosis">kurtosis</xref>, you can reuse <m>E[X^3] = \frac{7}{2}</m> and <m>E[X^2] = \frac{11}{5}</m> and 
<xref ref="TheoreticalMeasuresAlternates">the alternate forumulas</xref> to determine
<me>E[(X-\mu)^4] = E[X^4] - 4 \mu \cdot E[X^3] + 6 \mu^2 \cdot E[X^2] - 3 \mu^4</me>
which is the numerator for the kurtosis.  
</p>
</example>
</p>
	
<p>
<example><title>Roulette</title>
<p>
Roulette is a gambling game popular in may casinos in which a player attempts to win money from the casino by predicting the location that a ball lands on in a spinning wheel.  There are two variations of this game...the American version and the European version. The difference being that the American version has one additional numbered slot on the wheel. The American version of the game will be used for the purposes of this example.
</p>
<p>
A Roulette wheel consists of 38 equally-sized sectors identified with the numbers 1 through 36 plus 0 and 00. The 0 and 00 sectors are colored green and half of the remaining numbers are in sectors colored red with the remainder colored black.  A steel ball is dropped onto a spinning wheel and as the wheel comes to rest the sector in which it comes to rest is noted.  It is easy to determine that the probability of landing on any one of the 38 sectors is 1/38. A picture of a typical American-style wheel and betting board is given by
<image source="images/6-1-American-Roulette-1024x463.png" />. (Found at BigFishGames.com.)
</p>
<p>
Since this is a game in a casino, there must be the opportunity to bet (and likely lose) money. For the remainder of this example we will assume that you are betting 1 dollar each time. If you were to bet more then the values would scale correspondingly. However, if you place your bet on any single number and the ball ends up on the sector corresponding to that number, you win a net of 35 dollars.  If the ball lands elsewhere you lose your dollar. Therefore the expected value of winning if you bet on one number is
<me>E[\text{win on one}] = 35 \cdot \frac{1}{38} - 1 \cdot \frac{37}{38} = - \frac{2}{38}</me>
which is a little more than a nickel loss on average.
</p>
<p>
You can bet on two numbers as well and if the ball lands on either of the two then you win a payout in this case of 17 dollars.  Therefore the expected value of winning if you bet on two numbers is
<me>E[\text{win on two numbers}] = 17 \cdot \frac{2}{38} - 1 \cdot \frac{36}{38} = - \frac{2}{38}.</me>
</p>
<p>
Continuing, you can bet on three numbers and if the ball lands on any of the three then you win a payout of 11 dollars.  Therefore the expected value of winning if you bet on three numbers is
<me>E[\text{win on three numbers}] = 11 \cdot \frac{3}{38} - 1 \cdot \frac{35}{38} = - \frac{2}{38}.</me>
</p>
<p>
You can bet on all reds, all blacks, all evens (ignoring 0 and 00), or all odds and get your dollar back. The expected value for any of these options is
<me>E[\text{win on eighteen numbers}] = 1 \cdot \frac{18}{38} - 1 \cdot \frac{20}{38} = - \frac{2}{38}.</me>
</p>
<p>There is one special way to bet which uses the the 5 numbers {0, 00, 1, 2, 3} and pays 6 dollars. This is called the "top line of basket".  Notice that the use of five numbers will make getting the same expected value as the other cases impossible using regular dollars and cents. The expected value of winning in this case us
<me>E[\text{win on top line of basket}] = 6 \cdot \frac{5}{38} - 1 \cdot \frac{33}{38} = - \frac{3}{38}</me>
which is of course worse and is the only normal way to bet on roulette which has a different expected value. 
</p>
<p>
There are other possible ways to bet on roulette but none provide a better expected value of winning.  The moral of this story is that you should never bet on the 5 number option and if you ever get ahead by winning on roulette using any of the possible options then you should probably stop quickly since over a long period of time it is expected that you will lose an average of <m>\frac{1}{19}</m> dollars per game.
</p>
</example>
</p>
	
<p>
Going back to Pascal's wager, let 
<ul>
<li>X = 0 represent disbelief when God doesn't exist</li>
<li>X = 1 represent disbelief when God does exist</li>
<li>X = 2 represent belief when God does exist</li>
<li>X = 3 represent belief when God does not exist</li>
</ul>
Presume that p is the likelihood that God exists. Then you can compute the expected value of disbelief and the expect value of belief by first creating a value function. Below, for argument sake we are somewhat randomly assign a value of one million to disbelief if God doesn't exist. The conclusions are the same if you choose any other finite number...
	<md>
		<mrow>v(0) = 1,000,000, f(0) = 1-p</mrow>
		<mrow>v(1) = -\infty, f(1) = p</mrow>
		<mrow>v(2) = \infty, f(2) = p</mrow>
		<mrow>v(3) = 0, f(3) = 1-p</mrow>
	</md>
	Then, 
	<md>
		<mrow>E[\text{disbelief}] &amp; = v(0)f(0) + v(1)f(1)</mrow>
		<mrow>&amp; = 1000000 \times (1-p) - \infty \times p</mrow>
		<mrow>&amp; = -\infty</mrow>
	</md>
	if p>0. On the other hand, 
	<md>
		<mrow>E[\text{belief}] &amp; = v(2)f(2) + v(3)f(3)</mrow>
		<mrow>&amp; = \infty \times p + 0 \times (1-p)</mrow>
		<mrow>&amp; = \infty</mrow>
	</md>
	if p>0. So Pascal's conclusion is that if there is even the slightest chance that God exists then belief is the smart and scientific choice.
	</p>

</section>

<section xml:id="GeneratingFunctions"><title>Generating Functions</title>
<p>In the previous section, you were able to take a given probability distribution and create several "special" expected value metrics that provided useful or interesting facts about the particular distribution...such as the mean and variance. It is possible however to embed a lot of these metrics in a "generating function". </p>

<definition xml:id="DefnMomentGeneratingFunction"><title>Moment Generating Function</title>
	<p>Given a probability function <m>f(x)</m>, the moment generating function is a transformation given by
	<me>M(t) = E[e^{tx}]</me>
where the expected value is a summation or integral dependent upon the nature of the random variable x. If the expected value does not exist (due perhaps to a f(x) with asymptotes) then the M(t) does not exist.
	</p>
</definition>

<definition xml:id="DefnProbabilityGeneratingFunction"><title>Probability Generating Function</title>
<p>Given a probability function <m>f(x)</m>, the probability generating function is a transformation given by
<me>N(t) = E[t^x]</me>
where the expected value is a summation or integral dependent upon the nature of the random variable x and again presuming that the resulting expected value exists.</p>
</definition>

<theorem><title>Relationship between <m>M(t)</m> and <m>N(t)</m></title>
<statement> <m>M(t) = N(e^t)</m> and <m>N(t) = M(\ln(t))</m></statement>
<proof><p>Obvious?  :)</p></proof>
</theorem>


<theorem xml:id="MomentGeneratingFunctionProperties"><title><m>M(t)</m> Properties</title>
<statement>
<p>
<me>M(0)=1</me>
<me>M'(0)=\mu</me>
<me>M''(0)=\sigma^2 + \mu^2</me>
</p>
</statement>
<proof>
<p>For the first result, notice
<me>M(0) = E[e^{0}] = E[1] = 1</me>
is pretty trivial.
</p>
<p>For the next results, take derivatives and use the linearity of the summation or integral.  Here, let's consider the case where <m>X</m> is a continuous variable and leave the discrete case for you.
<me>M'(t) = D_t \left [ \int_R e^{tx} f(x) dx \right ] = \int_R x e^{tx} f(x) dx</me>
and then evaluating at t=0 gives
<me>M'(0) = \int_R x e^{0} f(x) dx = \mu .</me>
Continuing,
<me>M''(t) = D_t \left [ \int_R x e^{tx} f(x) dx \right ] = \int_R x^2 e^{tx} f(x) dx</me>
and evaluating at t=0 gives
<me>M''(0) = \int_R x^2 e^{0} f(x) dx = E[x^2] = \sigma^2 + \mu^2.</me>
It should be noted that one may also determine the skewness and the kurtosis in a similar manner.
</p>
</proof>
</theorem>

<theorem xml:id="ProbabilityGeneratingFunctionProperties"><title>N(t) Properties</title>
<statement>
<p>For discrete variable <m>X</m>, 
<me> P(X=k) = \frac{N^{(k)}(0)}{k!} </me>
</p>
</statement>
<proof>
<p>Notice, in the discrete case where <m>R</m> = {0, 1, 2, ... }
<me>N(t) = E[t^x] = \sum_R t^x f(x) = f(0) + t f(1) + t^2 f(x) + t^3 f(3) + ...</me>
Therefore, by continually taking derivatives with respect to t and then evaluating those derivatives at t=0,
<md>
<mrow>N(0) = f(0)</mrow>
<mrow>N'(0) = f(1)</mrow>
<mrow>N''(0) = 2 f(2)</mrow>
<mrow>N'''(0) = 6 f(3) </mrow>
</md>
etc.
</p>
</proof>
</theorem>

<corollary><title>Identical Random Variables have Identical Probability Generating Functions</title>
<statement>If random variables <m>X_1</m> and <m>X_2</m> have the same probability generating function (and therefore the same moment generating function) then the variables have identical distributions.</statement>
<proof>
<p>
Apply previous result and notice that both variables have the same probability function <m>f(x)</m>.
</p>
</proof>
</corollary>

</section>

	
<section xml:id="StandardUnits"><title>Standard Units</title>
	<p>Any distribution variable can be converted to “standard units” using the linear translation 
	<me>\displaystyle z = \frac{x-\mu}{\sigma}.</me> 
	In doing so, values of z will always represent the number of
	standard deviations x is from the mean and will provide “dimensionless” comparisons.
	</p>
	
	<p>
	<example>
	<p>
	Consider our earlier 
	<xref ref="ContinuousEV-example">continuous example</xref> 
	in which we found 
	<m>\mu = \frac{5}{4}</m> and 
	<m>\sigma = \sqrt{\frac{51}{80}}</m>.  Then, 
	<me>P(0 &lt; X &lt; 1) = P \left ( \frac{0-\frac{5}{4}}{\sqrt{\frac{51}{80}}} &lt; \frac{X - \frac{5}{4}}{\sqrt{\frac{51}{80}}} &lt; \frac{1-\frac{5}{4}}{\sqrt{\frac{51}{80}}} \right )</me>
	gives the middle term is Z and the other endpoints are now in standard units that indicate the number of standard deviations from the mean rather than actual problem units.
	</p>
	</example>
	</p>
	
</section>   


<section xml:id="ProbabilityFunctionsSummary"><title>Summary</title>
<p>
TBA
</p>
</section>

<section xml:id="ProbabilityFunctionsExercises"><title>Exercises</title>
<p>
Here are some exercises to consider related to Probability Functions.
</p>	
	<exercise><title>Flipping A Fixed Number of Coins</title>
	<p>
	Consider the random variable from the previous section where you flip three coins and measure the number of heads obtained. Determine f(0), f(1), f(2), and f(3) and the corresponding distribution function F(x). These can be expressed in a table format. Generalize your answer to the case when you flip a n coins where n is a fixed natural number.
	</p>
	</exercise>
	
	
	<exercise><title>Flipping Fixed Number of Coins</title>
	<p>
	You flip three coins and measure the number of heads obtained. Determine the space <m>R</m> for the corresponding random variable <m>X</m>. From the eight possible outcomes, determine all outcomes corresponding to <m>X = 2</m>. Identify the random variable as discrete or continuous.
	</p>
	</exercise>
	
	<exercise><title>Flipping Coins till success</title>
	<p>
	You flip one coin repeatedly until you get a second head. Determine the space <m>R</m> for the corresponding random variable <m>X</m>. From the possibilities, determine all outcomes corresponding to <m>X = 4</m>. Identify the random variable as discrete or continuous.
	</p>
	</exercise>
	
	<exercise><title>Time between Accidents</title>
	<p>
	Now you want to measure the time between accidents at a particular intersection in town. Determine the space <m>R</m> for the corresponding random variable <m>X</m>. Describe all outcomes corresponding to <m>X \lt 1</m>. Be purposeful in the problem to describe the units you are using to measure time. Identify the random variable as discrete or continuous.
	</p>
	</exercise>
	
</section>

</chapter>
