<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="ProbabilityFunctions" xmlns:xi="http://www.w3.org/2001/XInclude">
<title>Probability Functions</title>

<introduction>
	<p>This chapter is a definitions general probability functions.</p>
</introduction>


<section><title>Random Variables</title>
	<p>For a given set of events, we might have difficulty doing mathematics since the outcomes
	are not numerical. In order to accomodate our desire to convert to numerical measures we want
	to assign numerical values to all outcomes. The process of doing this creates what is known as a random
	variable.
	</p>

	<definition>
	<title>Random Variable</title>
		<statement>Given a random experiment with sample space S, a function X mapping each 
		element of S to a unique real number is called a random variable. 
		For each element s from the sample space S, denote this function by
		X(s) = x
		and call the range of X the space of X: R={ x : X(s)=x, for some s in S} 
		</statement>
	</definition>


<image source="images/randomvariable.png" />


	<p>We will make various restrictions on the range of the random variable to fit different 
	generalized problems. Then, we will be able to work on a problem (which may be 
	inherently non-numerical) by using the random variable in subsequent calculations.
	</p>
	<example>
	<title>Success vs Failure</title>
	<p>When dealing with only two outcomes, one might use 
	<me>S = \text{{ success, failure }}.</me>
	Choose 
	<md>
		<mrow>X(success)=1</mrow>
		<mrow>X(failure)=0.</mrow>
	</md>
	Then, R={0,1}.
	</p>
	</example>
	<example>
	<title>Standard Dice Pairs</title>
	<p>When gambling with a pair of dice, one might use
	S=ordered pairs of all possible rolls.  Then 
	<me>S = \text{ {(a,b): a=die 1 outcome, b=die 2 outcome}}.</me>
	Choose 
	<me>X( (a,b) ) = a+b.</me> 
	Then, R={2, 3, 4, 5, ..., 12}.
	</p>
	</example>
	<example>
	<title>Other Dice Options</title>
	<p>When rolling dice in a board game (like RISK), one might use
	<me>S= \text{{(a,b): a=die 1 outcome, b=die 2 outcome}}</me>
	Choose 
	<me>X( (a,b) ) = \text{max{a,b}}.</me> 
	Then, R={1, 2, 3, 4, 5, 6}.
	</p>
	</example>

	<definition>
		<statement>
		R contains a countable number of points if either R is finite or there 
		is a one to one correspondence between R and the positive integers. 
		Such a set will be called discrete. We will see that often the set R is not countable. 
		If R consists of an interval of points (or a union of intervals), 
		then we call X a continuous random variable. 
		</statement>
	</definition>
	
	<subsection><title>HOMEWORK</title>
	<p>
	A. You flip three coins and measure the number of heads obtained. Determine the space R for the corresponding random variable X. From the eight possible outcomes, determine all outcomes corresponding to X=2. Identify the random variable as discrete or continuous.
	</p>
	
	<p>
	B. You flip one coin repeatedly until you get a second head. Determine the space R for the corresponding random variable X. From the possibilities, determine all outcomes corresponding to X=4. Identify the random variable as discrete or continuous.
	</p>
	
	<p>
	C. Now you want to measure the time between accidents at a particular intersection in town. Determine the space R for the corresponding random variable X. Describe all outcomes corresponding to <m>X \lt 1</m>. Be purposeful in the problem to describe the units you are using to measure time. Identify the random variable as discrete or continuous.
	</p>
	</subsection>

</section>


<section><title>Probability Functions</title>
	<p>In the formulas below, we will presume that we have a random variable X which maps the sample space S onto some range of real numbers R.  From this set, we then can define a probability function f(x) which acts on the numerical values in R and returns another real number.  We attempt to do so to obtain (for discrete values) P(sample space value s)<m> = f(X(s))</m>.  That is, the probability of a given outcome s is equal to the composition which takes s to a numerical value x which is then plugged into f to get the same final values.</p>

	<definition><title>Probability "Mass" Function</title>
		<statement>Given a discrete random variable X on a space R, a probability mass function on X is given by a function <m>f:R \rightarrow \mathbb{R}</m> such that:
		<md>
			<mrow>&amp; \forall x \in R , f(x) \gt 0</mrow>
			<mrow>&amp; \sum_{x \in R} f(x) = 1</mrow>
			<mrow>&amp; A \subset R \Rightarrow P(X \in A) = \sum_{x \in A}f(x)</mrow>
		</md>
		For <m>x \not\in R</m>, you can use the convention f(x)=0.
		</statement>
	</definition>
	
	<definition><title>Probability "Density" Function</title>
		<statement>Given a continuous random variable X on a space R, a probability density function on X is given by a function <m>f:R \rightarrow \mathbb{R}</m> such that:
			<md>
				<mrow>&amp; \forall x \in R , f(x) \gt 0</mrow>
				<mrow>&amp; \int_{R} f(x) dx = 1</mrow>
				<mrow>&amp; A \subset R \Rightarrow P(X \in A) = \int_{A} f(x) dx</mrow>
			</md>
		For <m>x \not\in R</m>, you can use the convention f(x)=0.
		</statement>
	</definition>
	
	<p>
	For the purposes of this book, we will use the term "Probability Function" to refer to either of these options.
	</p>
	
	<example>
	<title>Discrete Probability Function</title>
	<p>
	Consider <m>f(x) = x/10</m> over R = {1,2,3,4}.  Then, f(x) is obviously positive for each of the values in R and certainly <m>\sum_{x \in R} f(x) = f(1) + f(2) + f(3) + f(4) = 1/10 + 2/10 + 3/10 + 4/10 = 1</m>. Therefore, f(x) is a probability mass function over the space R.
	</p>
	</example>
	
<sage><title>Sampling Discrete Probability Function</title>
	<input>
# Combining all of the above into one interactive cell
@interact
def _(D = input_box([1,2,3,5,6,8,9,11,12,14],label="Enter domain R (in brackets):"), 
       Probs = input_box([1/20,1/20,1/20,3/20,1/20,4/20,4/20,1/20,1/20,3/20],label="Enter corresponding f(x) (in brackets):"),
       n_samples=slider(100,10000,100,100,label="Number of times to sample from this distribution:")):
    n = len(D)
    R = range(n)
    one_huh = sum(Probs)
    pretty_print('\n\nJust to be certain, we should check to make certain the probabilities sum to 1\n')
    pretty_print(html('<center>$\sum_{x\epsilon R} f(x) = %s$</center>'%str(one_huh)))
    
    G = Graphics()
    if len(D)==len(Probs):
        f = zip(D,Probs)
        meanf = 0
        variancef = 0
        for k in R:
            meanf += D[k]*Probs[k]
            variancef += D[k]^2*Probs[k]
            G += line([(D[k],0),(D[k],Probs[k])],color='green')
        variancef = variancef - meanf^2
        sd = sqrt(variancef)
        G += points(f,color='blue',size=50)
        G += point((meanf,0),color='yellow',size=60,zorder=3)
        G += line([(meanf-sd,0),(meanf+sd,0)],color='red',thickness=5)
    
        g = DiscreteProbabilitySpace(D,Probs)
        pretty_print('     mean = %s'%str(meanf))
        pretty_print(' variance = %s'%str(variancef))
    
        #  perhaps to add mean and variance for pmf here
    else:
        print 'Domain D and Probabilities Probs must be lists of the same size'
    
    #  Now, let's sample from the distribution given above and see how a random sampling matches up

    counts = [0] * len(Probs)
    X = GeneralDiscreteDistribution(Probs)
    sample = []

    for _ in range(n_samples):
        elem = X.get_random_element()
        sample.append(D[elem])
        counts[elem] += 1
    Empirical = [1.0*x/n_samples for x in counts] # random
    
    samplemean = mean(sample)
    samplevariance = variance(sample)
    sampdev = sqrt(samplevariance)
    
    E = points(zip(D,Empirical),color='orange',size=40)
    E += point((samplemean,0.005),color='brown',size=60,zorder=3)
    E += line([(samplemean-sampdev,0.005),(samplemean+sampdev,0.005)],color='orange',thickness=5)    
    (G+E).show(ymin=0,figsize=(8,5))	
	</input>
</sage>

	<example>
	<title>Continuous Probability Function</title>
	<p>
	Consider <m>f(x) = x^2/c</m> for some positive real number c and presume R = [-1,2]. Then f(x) is nonnegative (and only equals zero at one point). To make f(x) a probability density function, we must have
	<me>\int_{x \in R} f(x) = 1.</me>
	In this instance you get
	<me>1 = \int_{-1}^2 x^2/c = x^3/(3c) |_{-1}^2 = \frac{8}{3c} - \frac{-1}{3c} = \frac{3}{c}</me>
	Therefore, f(x) is a probability density function over R provided   = 3.
	</p>
	</example>
	
	<definition><title>Distribution Function</title>
		<statement>Given a random variable X on a space R, a probability distribution function on X is given by a function 
				   <m>F:\mathbb{R} \rightarrow \mathbb{R}</m> such that <m>\displaystyle F(x)=P(X \le x)</m>
		</statement>
	</definition>
	
	<example>
	<title>Discrete Distribution Function</title>
	<p>Using <m>f(x) = x/10</m> over R = {1,2,3,4} again, note that F(x) will only change at these four domain values. We get
	
	<table halign="left">
      	<tabular halign="right">
      
<row><cell bottom="medium" right="medium">X</cell><cell bottom="medium">F(x)</cell></row>      
<row><cell right="medium"><m>x \lt 1</m></cell><cell>0</cell></row>
<row><cell right="medium"><m>1 \le x \lt 2</m></cell><cell>1/10</cell></row>
<row><cell right="medium"><m>2 \le x \lt 3</m></cell><cell>3/10</cell></row>
<row><cell right="medium"><m>3 \le x \lt 4</m></cell><cell>6/10</cell></row>
<row><cell right="medium"><m>4 \le x </m></cell><cell>1</cell></row>
		</tabular>
	</table>
	</p>
	</example>
	
	<example>
	<title>Continuous Distribution Function</title>
	<p>
	Consider <m>f(x) = x^2/3</m> over R = [-1,2].  Then, for <m>-1 \le x \le 2</m>,
	<me>F(x) = \int_{-1}^x u^2/3 du = x^3/9 + 1/9.</me>
	Notice, F(-1) = 0 since nothing has yet been accumulated over values smaller than -1 and F(2)=1 since by that time everything has been accumulated. In summary:
	
	<table halign="left">
      	<tabular halign="right">
      
<row><cell bottom="medium" right="medium">X</cell><cell bottom="medium">F(x)</cell></row>      
<row><cell right="medium"><m>x \lt -1</m></cell><cell>0</cell></row>
<row><cell right="medium"><m>-1 \le x \lt 2</m></cell><cell><m>x^3/9 + 1/9</m></cell></row>
<row><cell right="medium"><m>2 \le x</m></cell><cell>1</cell></row>
	
		</tabular>
	</table>
	
	
	</p>
	</example>
		
	<theorem xml:id="theorem-Fmin">
		<statement><m>F(x)=0, \forall x \lt \inf(R)</m></statement>
		<proof>
		<p>
		Let a = inf(R). Then, for 
		<me>x \lt a, F(x) = P(X \le x) \le P(X \lt a) = 0</me> 
		since none of the x-values in this range are in R.
		</p>
		</proof>
	</theorem>
	
	<theorem xml:id="theorem-Fmax">
		<statement><m>F(x)=1, \forall x \ge \sup(R)</m></statement>
		<proof>
		<p>
		Let b = sup(R). Then, for 
		<me>x \ge b, F(x) = P(X \le x)  = P(X \le b) + P( b \lt X le x) = P(X le b) = 1</me> 
		since all of the x-values in this range are in R and therefore will either sum over or integrate over all of R.
		</p>
		
		</proof>
	</theorem>
	
	<theorem>
		<statement xml:id="theorem-F-non-decreasing">F is non-decreasing</statement>
		<proof>
			<p>Case 1: R discrete</p>
			<md>
				<mrow>\forall x_1,x_2 \in \mathbb{Z} \ni x_1 \lt x_2</mrow>
				<mrow>F(x_2) &amp; = \sum_{x \le x_2} f(x) </mrow>
				<mrow>&amp; = \sum_{x \le x_1} f(x) + \sum_{x_1 \lt x \le x_2} f(x)</mrow>
				<mrow>&amp; \ge \sum_{x \le x_1} f(x) = F(x_1)</mrow>
			</md>
			<p>Case 2: R continuous</p>
			<md>
				<mrow>\forall x_1,x_2 \in \mathbb{R} \ni x_1 \lt x_2</mrow>
				<mrow>F(x_2) &amp; = \int_{-\infty}^{x_2} f(x) dx </mrow>
				<mrow> &amp; = \int_{-\infty}^{x_1} f(x) dx + \int_{x_1}^{x_2} f(x) dx</mrow>
				<mrow> &amp; \ge \int_{-\infty}^{x_1} f(x) dx</mrow>
				<mrow> &amp; = F(x_1)</mrow>
			</md>
		</proof>
	</theorem>
	
	<theorem xml:id="theorem-Fvsf-discrete">
		<title>Using Discrete Distribution Function to compute probabilities</title>
		<statement>for <m>x \in R, f(x) = F(x) - F(x-1)</m></statement>
		<proof>
		<p>Assume <m>x \in R</m> for some discrete R. Then,
		<me>F(x) - F(x-1) = \sum_{u \le x} f(u) - \sum_{u \lt x} f(u) = f(x)</me>
		</p>
		</proof>
	</theorem>
	
	<theorem xml:id="theorem-Fvsf-continuyous">
		<title>Using Continuous Distribution function to compute probabilities</title>
		<statement>for <m>a \lt b, (a,b) \in R, P(a \lt X \le b) = F(b) - F(a)</m></statement>
		<proof>
		<p>
		For a and b as noted, consider 
		<md>
		<mrow>F(b) - F(a) &amp; = \int_{-\infty}^b f(x) dx - \int_{-\infty}^a f(x) dx</mrow>
		<mrow> &amp; = \int_a^b f(x) dx </mrow>
		<mrow> &amp; = P(a \lt x \le b)</mrow>
		</md>
		</p>
		</proof>
	</theorem>
	
	<corollary xml:id="corollary-ProbPointZero-continuous">
		<statement>For continuous distributions, P(X = a) = 0</statement>
		<proof>
		<p>
		We will assume that F(x) is a continuous function. With that assumption, note
		<me>P(a-\epsilon \lt  x \le a)  = \int_{a-\epsilon}^a f(x) dx = F(a) - F(a-\epsilon)</me>
		Take the limit as <m> \epsilon \rightarrow 0^+</m> to get the result noting that
		</p>
		</proof>
	</corollary>
	
	<subsection><title>HOMEWORK</title>
	<p>
	A.  Consider the random variable from the previous section where you flip three coins and measure the number of heads obtained. Determine f(0), f(1), f(2), and f(3) and the corresponding distribution function F(x). These can be expressed in a table format. Generalize your answer to the case when you flip a n coins where n is a fixed natural number.
	</p>
	<p>
	B.  
	</p>
	</subsection>
		
</section> 

<section><title>Expected Value</title>
	<p>Blaise Pascal was a 	17th century mathematician and philosopher who was accomplished in many areas but may likely be best known to you for his creation of what is now known as Pascal's Triangle. As part of his philosophical pursuits, he proposed what is known as "Pascal's wager". It suggests two  mutually exclusive outcomes: that God exists or that he does not. His argument is that a rational person should live as though God exists and seek to believe in God. If God does not actually exist, such a person will have only a finite loss (some pleasures, luxury, etc.), whereas they stand to receive infinite gains as represented by eternity in Heaven and avoid an infinite losses of eternity in Hell. This type of reasoning is part of what is known as "decision theory".
	</p>
	<p>You may not confront such dire payouts when making your daily decisions but we need a formal method for making these determinations precise. The procedure for doing so is what we call expected value.
	</p>
	
	<definition><title>Expected Value</title>
	<p>Given a random variable X over space R, corresponding probability function f(x) and "value function" u(x), the expected value of u(x) is given by
	<me>E = E[u(X)] = \sum_{x \in R} u(x) f(x)</me>
	provided X is discrete, or
	<me>E = E[u(X)] = \int_R u(x)f(x) dx</me>
	provided X is continuous.
	</p>
	</definition>
	
	<theorem><title>Expected Value is a Linear Operator</title>
	<statement>
	<p>
	<ol>
		<li>E[c] = c</li>
		<li>E[c u(X)] = c E[u(X)]</li>
		<li>E[u(X) + v(X)] = E[u(X)] + E[v(X)]</li>
	</ol>
	</p>
	</statement>
	<proof>
	<p>Each of these follows by utilizing the corresponding linearity properties of the summation and integration operations. For example, to verify part three in the continuous case:
	<md>
		<mrow>E[u(X) + v(X)] &amp; = \int_{x \in R} [u(x)+v(x)]f(x) dx</mrow>
		<mrow> &amp; = \int_{x \in R} u(x)f(x) dx + \int_{x \in R} u(x)f(x) dx</mrow>
		<mrow> &amp; = E[u(X)] + E[v(X)].</mrow>
	</md>
	</p>
	</proof>
	</theorem>
	
	<example><title>Discrete Expected Value</title>
	<p>Consider <m>f(x) = x/10</m> over R = {1,2,3,4} where the payout is 10 euros if x=1, 5 euros if x=2, 2 euros if x=3 and -7 euros if x = 4.  Then your value function would be u(1)=10, u(2) = 5, u(3)=2, and u(4) = -7. Computing the expect payout gives
	<me>E = 10 \times 1/10 + 5 \times 2/10 + 2 \times 3/10 - 7 \times 4/10 = -2/10</me>
	Therefore, the expected payout is actually negative due to a relatively large negative payout associated with the largest likelihood outcome and the larger positive payout only associated with the least likely outcome.
	</p>
	</example>
	
	<example><title>Continuous Expected Value</title>
	<p>
	Consider <m>f(x) = x^2/3</m> over R = [-1,2] with value function given by <m>u(x) = e^x - 1</m>. Then, the expected value for u(x) is given by
	<me>E = \int_{-1}^2 (e^x-1) \cdot x^2/3 = -1/9 \cdot (e + 15) \cdot e^{-1} + 2/3 \cdot e^2 - 8/9 \approx 3.3129</me>
	</p>
	</example>

	<definition><title>Theoretical Measures</title>
	<statement>
	<p>
	Given a random variable with probability function f(x) over space R
	<ol>
		<li>The mean of X = <m>\mu = E[x]</m></li>
		<li>The variance of X = <m>\sigma^2 = E[(x-\mu)^2]</m></li>
		<li>The skewness of X = <m>\gamma_1 = \frac{E[(x-\mu)^3]}{\sigma^3}</m></li>
		<li>The kurtosis of X = <m>\gamma_2 = \frac{E[(x-\mu)^4]}{\sigma^4}</m></li>
	</ol>
	</p>
	</statement>
	</definition>
	
	<theorem><title>Alternate Formulas for Theoretical Measures</title>
	<statement>
	<p>
	<ol>
		<li><m>\sigma^2 = E[x^2] - \mu^2 = E[X(x-1)] + \mu - \mu^2</m></li>
		<li><m>\gamma_1 = \frac{1}{\sigma^3} \cdot \left [ E[X^3] - 3 \mu E[X^2] + 2\mu^3 \right ]</m></li>
		<li><m>\gamma_2 = \frac{1}{\sigma^4} \cdot \left [ E[X^4] - 4 \mu E[X^3] + 6\mu^2 E[X^2] - 3 \mu^4 \right ]</m></li>
	</ol>
	</p>
	</statement>
	<proof>
		<p>
		In each case, expand the binomial inside and use the linearity of expected value.
		</p>
	</proof>
	</theorem>
	
	<p>
	Consider the following example when computing these statistics for a discrete variable. In this case, we will utilize a variable with a relatively small space so that the summations can be easily done by hand. Indeed, consider
	
	<table halign="left">
      	<tabular halign="right">
      
<row><cell bottom="medium" right="medium">X</cell><cell bottom="medium">f(x)</cell></row>      
<row><cell right="medium">0</cell><cell>0.10</cell></row>
<row><cell right="medium">1</cell><cell>0.25</cell></row>
<row><cell right="medium">2</cell><cell>0.40</cell></row>
<row><cell right="medium">4</cell><cell>0.15</cell></row>
<row><cell right="medium">7</cell><cell>0.10</cell></row>
		</tabular>
	</table>
	
	<image source="images/DiscreteHistogramExample.png" />
	</p>
	<p>
	Using the definition of mean as a sum,
	<md>
		<mrow>\mu &amp; = 0 \cdot 0.10 + 1 \cdot 0.25 + 2 \cdot 0.40 + 4 \cdot 0.15 + 7 \cdot 0.10</mrow>
		<mrow> &amp; = 0 + 0.25 + 0.80 + 0.60 + 0.70</mrow>
		<mrow> &amp; = 2.35</mrow>
	</md>
	Notice where this lies on the probability histogram for this distribution.
	</p>
	
	<p>For the variance
	<md>
		<mrow>\sigma^2 &amp; = E[X^2] - \mu^2</mrow>
		<mrow> &amp; = \left [ 0^2 \cdot 0.10 + 1^2 \cdot 0.25 + 2^2 \cdot 0.40 + 4^2 \cdot 0.15 + 7^2 \cdot 0.10 \right ] - 2.35^2</mrow>
		<mrow> &amp; = 0 + 0.25 + 1.60 + 2.40 + 4.90 - 5.5225</mrow>
		<mrow> &amp; = 9.15 - 5.225</mrow>
		<mrow> &amp; = 3.6275 </mrow>
	</md>	
	and so the standard deviation <m>\sigma = \sqrt{3.6275} \approx 1.90</m>. Notice that 4 times this value encompasses almost all of the range of the distribution.
	</p>
	
	<p>For the skewness
	<md>
		<mrow> \text{Numerator = } &amp; E[X^3] - 3 \mu E[X^2] + 2\mu^3</mrow>
		<mrow> &amp; = \left [ 0^3 \cdot 0.10 + 1^3 \cdot 0.25 + 2^3 \cdot 0.40 + 4^3 \cdot 0.15 + 7^3 \cdot 0.10 \right ] - 3 \cdot 2.35 \cdot 9.15 + 2 \cdot 2.35^3</mrow>
		<mrow> &amp; \approx 0 + 0.25 + 3.20 + 9.60 + 34.3 - 64.5075 + 25.96</mrow>
		<mrow> &amp; = 47.35 - 64.5075 + 25.96</mrow>
		<mrow> &amp; \approx 8.80</mrow>
	</md>
	which yields a skewness of <m>\gamma_1 = 8.80 / \sigma^3 \approx 1.27 </m>. This indicates a slight skewness to the right of the mean. You can notice the 4 and 7 entries on the histogram illustrate a slight trailing off to the right.
	</p>
	
	<p>Finally, for kurtosis
	<md>
		<mrow> \text{Numerator = } &amp; E[X^4] - 4 \mu E[X^3] + 6 \mu^2 E[X^2] - 3\mu^4</mrow>
		<mrow> &amp; = \left [ 0^4 \cdot 0.10 + 1^4 \cdot 0.25 + 2^4 \cdot 0.40 + 4^4 \cdot 0.15 + 7^4 \cdot 0.10 \right ] - 4 \cdot 2.35 \cdot 47.35 + 6 \cdot 2.35^2 \cdot 9.15^2 - 3 \cdot 2.35^4</mrow>
		<mrow> &amp; \approx 0 + 0.25 + 6.40 + 38.4 + 240.1 - 445.09 + 303.19 - 91.49</mrow>
		<mrow> &amp; \approx 285.15 - 445.09 + 303.19 - 91.49</mrow>
		<mrow> &amp; \approx 51.75</mrow>
	</md>
	which yields a kurtosis of <m>\gamma_2 = 51.75 / \sigma^4 \approx 3.93</m> which also notes that the data appears to have a modestly bell-shaped distribution.
	</p>
	
	<p>
	Consider the following example when computing these statistics for a continuous variable. 
	Let <m>f(x) = \frac{3}{4} \cdot (1-x^2)</m> over R = [-1,1].  
		<image source="images/ContinuousDistributionExample.png" />
	</p>
	<p>
	Then for the mean
	<md>
		<mrow>\mu &amp; = \int_{-1}^1 x \cdot \frac{3}{4} \cdot (1-x^2) dx</mrow>
		<mrow> &amp; = \int_{-1}^1 \frac{3}{4} \cdot (x-x^3) dx</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot (x^2/2-x^4/4) \big |_{-1}^1</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot [(1/2)-(1/4)] - [(1/2) - (1/4)]</mrow>
		<mrow> &amp; = 0</mrow>
	</md>
	as expected since the probability function is symmetric about x=0.
	</p>
	
	<p>For the variance
	<md>
		<mrow>\sigma^2 &amp; = \int_{-1}^1 x^2 \cdot \frac{3}{4} \cdot (1-x^2) dx - \mu^2</mrow>
		<mrow> &amp; = \int_{-1}^1 \cdot \frac{3}{4} \cdot (x^2-x^4) dx - 0</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot (x^3 /3 -x^5 / 5) \big |_{-1}^1</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot 2 \cdot (1/3-1/5)</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot \frac{4}{15}</mrow>
		<mrow> &amp; = \frac{1}{5}</mrow>
	</md>
	and taking the square root gives a standard deviation slightly less than 1/2. Notice that four times this value encompasses almost all of the range of the distribution.
	</p>
	
	<p>For the skewness, notice that the graph is symmetrical about the mean and so we would expect a skewness of 0.  Just to check it out
	<md>
		<mrow> \text{Numerator = } &amp; E[X^3] - 3 \mu E[X^2] + 2\mu^3</mrow>
		<mrow> &amp; = \int_{-1}^1 x^3 \cdot \frac{3}{4} \cdot (1-x^2) dx - 3 E[X^2] \cdot 0 + 0^3 </mrow>
		<mrow> &amp; = \int_{-1}^1 \cdot \frac{3}{4} \cdot (x^3-x^5) dx</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot (x^4/4-x^6/6) \big |_{-1}^1</mrow>
		<mrow> &amp; = 0</mrow>
	</md>
	as expected without having to actually complete the calculation by dividing by the cube of the standard deviation.
	</p>
	
	<p>Finally, note that the probability function in this case is modestly close to a bell shaped curve so we would expect a kurtosis in the vicinity of 3. Indeed, noting that (conveniently) <m>\mu = 0</m> gives
	<md>
		<mrow> \text{Numerator = } &amp; E[X^4] - 4 \mu E[X^3] + 6 \mu^2 E[X^2] - 3 \mu^4</mrow>
		<mrow> &amp; = \int_{-1}^1 x^4 \cdot \frac{3}{4} \cdot (1-x^2) dx</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot (x^5 /5-x^7 /7) \big |_{-1}^1</mrow>
		<mrow> &amp; = \frac{3}{4} \cdot 2(1/5-1/7)</mrow>
		<mrow> &amp; = \frac{3}{35}</mrow>
	</md>
	and so by dividing by <m>\sigma^4 = \sqrt{\frac{1}{5}}^4 = \frac{1}{25}</m> gives a kurtosis of
	<me>\gamma_2 = \frac{3}{35} / \frac{1}{25} = \frac{75}{35} \approx 2.14.</me>
	</p>
	

	
	<p>Going back to Pascal's wager, let X = 0 represent disbelief when God doesn't exist and X = 1 represent disbelief when God does exist, X = 2 represent belief when God does exist, and X = 3 represent belief when God does not exist. Let p be the likelihood that God exists. Then you can compute the expected value of disbelief and the expect value of belief by first creating a value function. Below, for argument sake we are somewhat randomly assign a value of one million to disbelief if God doesn't exist. The conclusions are the same if you choose any other finite number...
	<md>
		<mrow>u(0) = 1,000,000, f(0) = 1-p</mrow>
		<mrow>u(1) = -\infty, f(1) = p</mrow>
		<mrow>u(2) = \infty, f(2) = p</mrow>
		<mrow>u(3) = 0, f(3) = 1-p</mrow>
	</md>
	Then, 
	<md>
		<mrow>E[\text{disbelief}] &amp; = u(0)f(0) + u(1)f(1)</mrow>
		<mrow>&amp; = 1000000 \times (1-p) - \infty \times p</mrow>
		<mrow>&amp; = -\infty</mrow>
	</md>
	if p>0. On the other hand, 
	<md>
		<mrow>E[\text{belief}] &amp; = u(2)f(2) + u(3)f(3)</mrow>
		<mrow>&amp; = \infty \times p + 0 \times (1-p)</mrow>
		<mrow>&amp; = \infty</mrow>
	</md>
	if p>0. So Pascal's conclusion is that if there is even the slightest chance that God exists then belief is the smart and scientific choice.
	</p>

</section>

	
<section><title>Standard Units</title>
	<p>Any distribution variable can be converted to “standard units” using the linear translation 
	<m>\displaystyle z = \frac{x-\mu}{\sigma}</m>. In doing so, then values of z will always represent the number of
	standard deviations x is from the mean and will provide “dimensionless” comparisons.</p>
</section>   

</chapter>