<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="ProbabilityFunctions" xmlns:xi="http://www.w3.org/2001/XInclude">
<title>Probability Functions</title>

<introduction>
	<p>This chapter is a definitions general probability functions.</p>
</introduction>


<section><title>Random Variables</title>
	<p>For a given set of events, we might have difficulty doing mathematics since the outcomes
	are not numerical. In order to accomodate our desire to convert to numerical measures we want
	to assign numerical values to all outcomes. The process of doing this creates what is known as a random
	variable.
	</p>

	<definition>
	<title>Random Variable</title>
		<statement>Given a random experiment with sample space S, a function X mapping each 
		element of S to a unique real number is called a random variable. 
		For each element s from the sample space S, denote this function by
		X(s) = x
		and call the range of X the space of X: R={ x : X(s)=x, for some s in S} 
		</statement>
	</definition>

	<p>We will make various restrictions on the range of the random variable to fit different 
	generalized problems. Then, we will be able to work on a problem (which may be 
	inherently non-numerical) by using the random variable in subsequent calculations.
	</p>
	<example>
	<title>Success vs Failure</title>
	<p>When dealing with only two outcomes, one might use S = { success, failure}.
	Choose X(success)=1, X(failure)=0. Then, R={0,1}.</p>
	</example>
	<example>
	<title>Standard Dice Pairs</title>
	<p>When gambling with a pair of dice, one might use
	S=ordered pairs of all possible rolls = {(a,b): a=die 1 outcome, b=die 2 outcome}.
	Choose X( (a,b) ) = a+b. Then, R={2, 3, 4, 5, ..., 12}.</p>
	</example>
	<example>
	<title>Other Dice Options</title>
	<p>When rolling dice in a board game (like RISK), one might use
	S={(a,b): a=die 1 outcome, b=die 2 outcome}
	Choose X( (a,b) ) = max{a,b}. Then, R={1, 2, 3, 4, 5, 6}
	</p>
	</example>

	<definition>
		<statement>
		R contains a countable number of points if either R is finite or there 
		is a one to one correspondence between R and the positive integers. 
		Such a set will be called discrete. We will see that often the set R is not countable. 
		If R consists of an interval of points (or a union of intervals), 
		then we call X a continuous random variable. 
		</statement>
	</definition>


<image source="images/randomvariable.png" />

</section>


<section><title>Probability Functions</title>
	<p>In the formulas below, we will presume that we have a random variable X which maps the sample space S onto some range of real numbers R.  From this set, we then can define a probability function f(x) which acts on the numerical values in R and returns another real number.  We attempt to do so to obtain (for discrete values) P(sample space value s)<m> = f(X(s))</m>.  That is, the probability of a given outcome s is equal to the composition which takes s to a numerical value x which is then plugged into f to get the same final values.</p>

	<definition><title>Probability Mass Function</title>
		<statement>Given a discrete random variable X on a space R, a probability mass function on X is given by a function <m>f:R \rightarrow \mathbb{R}</m> such that:
		<md>
			<mrow>&amp; \forall x \in R , f(x) \gt 0</mrow>
			<mrow>&amp; \sum_{x \in R} f(x) = 1</mrow>
			<mrow>&amp; A \subset R \Rightarrow P(X \in A) = \sum_{x \in A}f(x)</mrow>
		</md>
		</statement>
	</definition>
	
	<definition><title>Probability Density Function</title>
		<statement>Given a continuous random variable X on a space R, a probability density function on X is given by a function <m>f:R \rightarrow \mathbb{R}</m> such that:
			<md>
				<mrow>&amp; \forall x \in R , f(x) \gt 0</mrow>
				<mrow>&amp; \int_{R} f(x) = 1</mrow>
				<mrow>&amp; A \subset R \Rightarrow P(X \in A) = \int_{A} f(x) dx</mrow>
			</md>
		</statement>
	</definition>
	
	<example>
	<title>Discrete Probability Function</title>
	<p>
	Consider <m>f(x) = x/10</m> over R = {1,2,3,4}.  Then, f(x) is obviously positive for each of the values in R and certainly <m>\sum_{x \in R} f(x) = f(1) + f(2) + f(3) + f(4) = 1/10 + 2/10 + 3/10 + 4/10 = 1</m>. Therefore, f(x) is a probability mass function over the space R.
	</p>
	</example>
	
<sage><title>Sampling Discrete Probability Function</title>
	<input>
# Combining all of the above into one interactive cell
@interact
def _(D = input_box([1,2,3,5,6,8,9,11,12,14],label="Enter domain R (in brackets):"), 
       Probs = input_box([1/20,1/20,1/20,3/20,1/20,4/20,4/20,1/20,1/20,3/20],label="Enter corresponding f(x) (in brackets):"),
       n_samples=slider(100,10000,100,100,label="Number of times to sample from this distribution:")):
    n = len(D)
    R = range(n)
    one_huh = sum(Probs)
    pretty_print('\n\nJust to be certain, we should check to make certain the probabilities sum to 1\n')
    pretty_print(html('<center>$\sum_{x\epsilon R} f(x) = %s$</center>'%str(one_huh)))
    
    G = Graphics()
    if len(D)==len(Probs):
        f = zip(D,Probs)
        meanf = 0
        variancef = 0
        for k in R:
            meanf += D[k]*Probs[k]
            variancef += D[k]^2*Probs[k]
            G += line([(D[k],0),(D[k],Probs[k])],color='green')
        variancef = variancef - meanf^2
        sd = sqrt(variancef)
        G += points(f,color='blue',size=50)
        G += point((meanf,0),color='yellow',size=60,zorder=3)
        G += line([(meanf-sd,0),(meanf+sd,0)],color='red',thickness=5)
    
        g = DiscreteProbabilitySpace(D,Probs)
        pretty_print('     mean = %s'%str(meanf))
        pretty_print(' variance = %s'%str(variancef))
    
        #  perhaps to add mean and variance for pmf here
    else:
        print 'Domain D and Probabilities Probs must be lists of the same size'
    
    #  Now, let's sample from the distribution given above and see how a random sampling matches up

    counts = [0] * len(Probs)
    X = GeneralDiscreteDistribution(Probs)
    sample = []

    for _ in range(n_samples):
        elem = X.get_random_element()
        sample.append(D[elem])
        counts[elem] += 1
    Empirical = [1.0*x/n_samples for x in counts] # random
    
    samplemean = mean(sample)
    samplevariance = variance(sample)
    sampdev = sqrt(samplevariance)
    
    E = points(zip(D,Empirical),color='orange',size=40)
    E += point((samplemean,0.005),color='brown',size=60,zorder=3)
    E += line([(samplemean-sampdev,0.005),(samplemean+sampdev,0.005)],color='orange',thickness=5)    
    (G+E).show(ymin=0,figsize=(8,5))	
	</input>
</sage>

	<example>
	<title>Continuous Probability Function</title>
	<p>
	Consider <m>f(x) = x^2/c</m> for some positive real number c and presume R = [-1,2]. Then f(x) is nonnegative (and only equals zero at one point). To make f(x) a probability density function, we must have
	<me>\int_{x \in R} f(x) = 1.</me>
	In this instance you get
	<me>1 = \int_{-1}^2 x^2/c = x^3/(3c) |_{-1}^2 = \frac{8}{3c} - \frac{-1}{3c} = \frac{3}{c}</me>
	Therefore, f(x) is a probability density function over R provided   = 3.
	</p>
	</example>
	
	<definition><title>Distribution Function</title>
		<statement>Given a random variable X on a space R, a probability distribution function on X is given by a function 
				   <m>F:\mathbb{R} \rightarrow \mathbb{R}</m> such that <m>\displaystyle F(x)=P(X \le x)</m>
		</statement>
	</definition>
	
	<example>
	<title>Discrete Distribution Function</title>
	<p>Using <m>f(x) = x/10</m> over R = {1,2,3,4} again, note that F(x) will only change at these four domain values. We get
	
	<table halign="left">
      	<tabular halign="right">
      
<row><cell bottom="medium">X</cell><cell bottom="medium">F(x)</cell></row>      
<row><cell><m>x \lt 1</m></cell><cell>0</cell></row>
<row><cell><m>1 \le x \lt 2</m></cell><cell>1/10</cell></row>
<row><cell><m>2 \le x \lt 3</m></cell><cell>3/10</cell></row>
<row><cell><m>3 \le x \lt 4</m></cell><cell>6/10</cell></row>
<row><cell><m>4 \le x </m></cell><cell>1</cell></row>
		</tabular>
	</table>
	</p>
	</example>
	
	<example>
	<title>Continuous Distribution Function</title>
	<p>
	Consider <m>f(x) = x^2/3</m> over R = [-1,2].  Then, for <m>-1 \le x \le 2</m>,
	<me>F(x) = \int_{-1}^x u^2/3 du = x^3/9 + 1/9.</me>
	Notice, F(-1) = 0 since nothing has yet been accumulated over values smaller than -1 and F(2)=1 since by that time everything has been accumulated. In summary:
	
	<table halign="left">
      	<tabular halign="right">
      
<row><cell bottom="medium">X</cell><cell bottom="medium">F(x)</cell></row>      
<row><cell><m>x \lt -1</m></cell><cell>0</cell></row>
<row><cell><m>-1 \le x \lt 2</m></cell><cell><m>x^3/9 + 1/9</m></cell></row>
<row><cell><m>2 \le x</m></cell><cell>1</cell></row>
	
		</tabular>
	</table>
	
	
	</p>
	</example>
</section>	
<section><title>Properties of the Distribution Function</title>
		
		<theorem xml:id="theorem-Fmin">
			<statement><m>F(x)=0, \forall x \le \inf(R)</m></statement>
			<proof>TBA</proof>
		</theorem>
		
		<theorem xml:id="theorem-Fmax">
			<statement><m>F(x)=1, \forall x \ge \sup(R)</m></statement>
			<proof>TBA</proof>
		</theorem>
		
		<theorem>
			<statement xml:id="theorem-F-non-decreasing">F is non-decreasing</statement>
			<proof>
				<p>Case 1: R discrete</p>
				<md>
					<mrow>\forall x_1,x_2 \in \mathbb{Z} \ni x_1 \lt x_2</mrow>
					<mrow>F(x_2) &amp; = \sum_{x \le x_2} f(x) </mrow>
					<mrow>&amp; = \sum_{x \le x_1} f(x) + \sum_{x_1 \lt x \le x_2} f(x)</mrow>
					<mrow>&amp; \ge \sum_{x \le x_1} f(x) = F(x_1)</mrow>
				</md>
				<p>Case 2: R continuous</p>
				<md>
					<mrow>\forall x_1,x_2 \in \mathbb{R} \ni x_1 \lt x_2</mrow>
					<mrow>F(x_2) &amp; = \int_{-\infty}^{x_2} f(x) dx </mrow>
					<mrow> &amp; = \int_{-\infty}^{x_1} f(x) dx + \int_{x_1}^{x_2} f(x) dx</mrow>
					<mrow> &amp; \ge \int_{-\infty}^{x_1} f(x) dx</mrow>
					<mrow> &amp; = F(x_1)</mrow>
				</md>
			</proof>
		</theorem>
		
		<theorem xml:id="theorem-Fvsf-discrete">
			<title>Using Discrete Distribution Function to compute probabilities</title>
			<statement>for <m>x \in R, f(x) = F(x) - F(x-1)</m></statement>
		</theorem>
		
		<theorem xml:id="theorem-Fvsf-continuyous">
			<title>Using Continuous Distribution function to compute probabilities</title>
			<statement>for <m>a \lt b, (a,b) \in R, P(a \lt X \lt b) = F(b) - F(a)</m></statement>
		</theorem>
		
		<corollary xml:id="corollary-ProbPointZero-continuous">
			<statement>For continuous distributions, P(X = a) = 0</statement>
		</corollary>
		
</section>
	
<section>
		<title>Standard Units</title>
			<p>Any distribution variable can be converted to “standard units” using the linear translation 
			<m>\displaystyle z = \frac{x-\mu}{\sigma}</m>. In doing so, then values of z will always represent the number of
			standard deviations x is from the mean and will provide “dimensionless” comparisons.</p>
</section>    


<section><title>Expected Value</title>
	<p>Blaise Pascal was a 	17th century mathematician and philosopher who was accomplished in many areas but may likely be best known to you for his creation of what is now known as Pascal's Triangle. As part of his philosophical pursuits, he proposed what is known as "Pascal's wager". It suggests two  mutually exclusive outcomes: that God exists or that he does not. His argument is that a rational person should live as though God exists and seek to believe in God. If God does not actually exist, such a person will have only a finite loss (some pleasures, luxury, etc.), whereas they stand to receive infinite gains as represented by eternity in Heaven and avoid an infinite losses of eternity in Hell. This type of reasoning is part of what is known as "decision theory".
	</p>
	<p>You may not confront such dire payouts when making your daily decisions but we need a formal method for making these determinations precise. The procedure for doing so is what we call expected value.
	</p>
	<definition><title>Expected Value</title>
	<p>Given a random variable X over space R, corresponding probability function f(x) and "value function" u(x), the expected value of u(x) is given by
	<me>E = E[u(X)] = \sum_{x \in R} u(x) f(x)</me>
	provided X is discrete, or
	<me>E = E[u(X)] = \int_R u(x)f(x) dx</me>
	provided X is continuous.
	</p>
	</definition>
	
	<example><title>Discrete Expected Value</title>
	<p>Consider <m>f(x) = x/10</m> over R = {1,2,3,4} where the payout is 10 euros if x=1, 5 euros if x=2, 2 euros if x=3 and -7 euros if x = 4.  Then your value function would be u(1)=10, u(2) = 5, u(3)=2, and u(4) = -7. Computing the expect payout gives
	<me>E = 10 \times 1/10 + 5 \times 2/10 + 2 \times 3/10 - 7 \times 4/10 = -2/10</me>
	Therefore, the expected payout is actually negative due to a relatively large negative payout associated with the largest likelihood outcome and the larger positive payout only associated with the least likely outcome.
	</p>
	</example>
	
	<example><title>Continuous Expected Value</title>
	<p>
	Consider <m>f(x) = x^2/3</m> over R = [-1,2] with value function given by <m>u(x) = e^x - 1</m>. Then, the expected value for u(x) is given by
	<me>E = \int_{-1}^2 (e^x-1) \cdot x^2/3 = -1/9 \cdot (e + 15) \cdot e^{-1} + 2/3 \cdot e^2 - 8/9 \approx 3.3129</me>
	</p>
	</example>
	
	<p>So, going back to Pascal's wager, let X = 0 represent disbelief when God doesn't exist and X = 1 represent disbelief when God does exist, X = 2 represent belief when God does exist, and X = 3 represent belief when God does not exist. Let p be the likelihood that God exists. Then you can compute the expected value of disbelief and the expect value of belief by first creating a value function. Below, for argument sake we are somewhat randomly assign a value of one million to disbelief if God doesn't exist. The conclusions are the same if you choose any other finite number...
	<md>
		<mrow>u(0) = 1,000,000, f(0) = 1-p</mrow>
		<mrow>u(1) = -\infty, f(1) = p</mrow>
		<mrow>u(2) = \infty, f(2) = p</mrow>
		<mrow>u(3) = 0, f(3) = 1-p</mrow>
	</md>
	Then, 
	<md>
		<mrow>E[disbelief] &amp; = u(0)f(0) + u(1)f(1)</mrow>
		<mrow>&amp; = 1000000 \times (1-p) - \infty \times p</mrow>
		<mrow>&amp; = -\infty</mrow>
	</md>
	if p>0. On the other hand, 
	<md>
		<mrow>E[belief] &amp; = u(2)f(2) + u(3)f(3)</mrow>
		<mrow>&amp; = \infty \times p + 0 \times (1-p)</mrow>
		<mrow>&amp; = \infty</mrow>
	</md>
	if p>0. So Pascal's conclusion is that if there is even the slightest chance that God exists then belief is the smart and scientific choice.
	</p>

</section>



</chapter>