<?xml version="1.0" encoding="UTF-8" ?>

<!-- Overall element for a Mathbook XML document; mandatory, always-->
<mathbook>

    <!--
    "docinfo" is like metadata, it mostly migrates
    various places, or gets used multiple times  -->
    <docinfo>
        <!-- There can be several authors here -->
        <author>
            <personname>John Travis</personname>
            <department>Department of Mathematics</department>
            <!-- Use "break" tag anywhere for forced line break -->
            <institution>Mississippi College<br />Clinton, MS, USA</institution>
            <email>travis@mc.edu</email>
        </author>
        <!-- Could set date manually or use the "today" element 
        <date><today /></date>  -->
        <!--
        TeX/LaTeX macros can be written in the usual way.
        Whitespace and line breaks are irrelevant here.
        -->
        <macros>
        \newcommand{\definiteintegral}[4]{\int_{#1}^{#2}\,#3\,d#4}
        \newcommand{\indefiniteintegral}[2]{\int#1\,d#2}
        \newcommand{\SUM}[3]{\sum_{#1}^{#2} #3  }
        </macros>
        <latex-image-preamble>
        \usepackage{pgfplots}
        \pgfplotsset{axis x line = middle,
                     axis y line = middle}
        \usepackage{tikz}
        \usetikzlibrary{backgrounds}
        \usetikzlibrary{arrows,matrix}
        </latex-image-preamble>
    </docinfo>

    <!--
    The article element is top-level for anything short of a book.
    There is a book element, which is not demonstrated here.
    The "filebase" attribute is used for the filename of the
    HTML output, s part of a chunking scheme that is not complete.
  -->
	<chapter xml:id="Probability">
        <title>Probability</title>

	
	
	
        
        
		
		<section xml:id="PoissonDistribution">
			<title>Poisson</title>		
			<subsection>
				<title>Derivation of Poisson Formulas</title>
				<p>Consider the situation when you might want to look at the number of successes which could happen 
				over a given interval of time, say [0,T].  The number of these possible successes a discrete value 
				from the set {0, 1, 2, 3, ...} even though the world over which these successes may occur is continuous. 
				Let’s also assume that successive successes are independent of each
				other, that the likelihood of one success in an interval of time is proportional to the length of the 
				interval, and that the likelihood of more than one success in a sufficiently small interval is 
				essentially zero.  If so, then the resulting distribution of probabilities
				is called a Poisson Distribution.</p>
				<p>For a Poisson Distribution, let’s determine the probability of exactly m successes in [0,T]. 
				(Notice, this is similar to the relationship
				between Binomial and Negative Binomial where Negative Binomial measures the number of successes 
				in a fixed number of trials whereas Poisson
				measures the number of successes in a fixed interval.</p>
				
				<p>Toward this goal, let’s take the interval [0,T] and break it up into n equally-sized subintervals, 
				each of width <m>\delta = T/n</m>.  If
				n is sufficiently large then the width of any of these intervals should be very small and so should 
				contain at most one success according
				to the assumptions above.  If so, then counting the number of successes in [0,T] would be similar to 
				counting the number of successes in
				the n “fixed” trials... i.e., a binomial problem with p = <m>\frac{\lambda T}{n}</m> (i.e. proportional 
				to the size of the intervals.) 
				So, we can approximate the answer to the Poisson question by approximating with a Binomial
				solution. As n approaches infinity, this discrete approximation of chopping the interval up into discrete 
				pieces becomes a continous 
				interval and therefore the Poisson problem. </p>

<md>
<mrow> P(X=m) &amp; = \lim_{n \rightarrow \infty} \binom{n}{m} \left ( \frac{\lambda T}{n} \right )^m \left (1-\frac{\lambda T}{n} \right )^{n-m} </mrow>
<mrow> &amp; = \lim _{n \rightarrow \infty} \frac{n!}{m!(n-m)!}(\frac{\lambda T}{n})^m (1-\frac{\lambda T}{n})^{n-m} </mrow>
<mrow> &amp; = \frac{(\lambda T)^m}{m!} \lim _{n \rightarrow \infty} \frac{n(n-1)...(n-m+1)}{n^m} (1-\frac{\lambda T}{n})^n (1-\frac{\lambda T}{n})^{-m} </mrow>
<mrow> &amp; = \frac{(\lambda T)^m}{m!} 1 \cdot \lim _{n \rightarrow \infty} (1-\frac{\lambda T}{n})^n \cdot 1</mrow>
<mrow> &amp; = \frac{(\lambda T)^m}{m!} \cdot \lim _{n \rightarrow \infty} {e^{n \cdot \ln{(1-\frac{\lambda T}{n})}}} </mrow>
<mrow> &amp; = \frac{(\lambda T)^m}{m!} \cdot \lim _{n \rightarrow \infty} {e^{ \ln{(1-\frac{\lambda T}{n})} / \frac{1}{n}}} </mrow>
<mrow> &amp; = \frac{(\lambda T)^m}{m!} \cdot 
	  \lim _{n \rightarrow \infty} {e^{ \frac{ \frac{\lambda T}{n^2}}{(1-\frac{\lambda T}{n}) } / \frac{-1}{n^2} }} </mrow>
<mrow> &amp; = \frac{(-\lambda T)^m}{m!} \cdot e^{-\lambda T}</mrow>
</md>

				<theorem>
					<title>Verifying the Poisson Distribution</title>
					<statement>For a Poisson variable, the function above sums to one.</statement>
					<proof>
						<md>
							<mrow> \SUM {m=0} {\infty} {\frac{(\lambda T)^m}{m!} \cdot e^{-\lambda T}} &amp; = e^{-\lambda T} \SUM {m=0} {\infty} {\frac{(\lambda T)^m}{m!} }</mrow>
							<mrow> &amp; = e^{-\lambda T} e^{\lambda T}</mrow>
							<mrow> &amp; = 1</mrow>
							
						</md>
					</proof>
				</theorem>
				<theorem>
					<title>The Mean of the Poisson Distribution</title>
					<statement>For a Poisson variable, <m>\mu = \lambda T</m></statement>
					<proof>

<md>
<mrow> \mu =  E[m] = &amp; = \SUM {m=0} {\infty} {m \cdot \frac{(\lambda T)^m}{m!} \cdot e^{-\lambda T}}</mrow>
<mrow>  &amp; = e^{-\lambda T} \SUM {m=1} {\infty} {\frac{(\lambda T)^m}{(m-1)!} }</mrow>
<mrow>  &amp; = e^{-\lambda T} \cdot \lambda T \SUM {m=1} {\infty} {\frac{(\lambda T)^{m-1}}{(m-1)!} }</mrow>
<mrow>  &amp; = e^{-\lambda T} \cdot \lambda T \SUM {k=0} {\infty} {\frac{(\lambda T)^k}{k!} }</mrow>
<mrow>  &amp; = e^{-\lambda T} \cdot \lambda T e^{\lambda T}</mrow>
<mrow> &amp; = \lambda T </mrow>							
</md>

					</proof>
				</theorem>
				<corollary>
					<title>Alternate form for Poisson Probability Function</title>
					<statement>Given the mean <m>\mu</m> number of successes in the interval [0,T], 
					  <m>\displaystyle P(X=m) = \frac{\mu^m e^{-\mu}}{m!}</m> </statement>

				</corollary>

				<theorem>
					<title>The Variance of the Poisson Distribution</title>
					<statement>For a Poisson variable, <m>\sigma^2 = \mu</m></statement>
					<proof>

<md>
<mrow> \sigma^2 =  E[m(m-1)] + \mu - \mu^2 &amp; = \SUM {m=0} {\infty} {m(m-1) \cdot \frac{\mu^m}{m!} \cdot e^{-\mu}} + \mu - \mu^2</mrow>
<mrow>  &amp; = e^{-\mu} \SUM {m=2} {\infty} {\frac{\mu^m}{(m-2)!} } + \mu - \mu^2</mrow>
<mrow>  &amp; = e^{-\mu} \cdot \mu^2 \SUM {m=2} {\infty} {\frac{\mu^{m-2}}{(m-2)!} } + \mu - \mu^2</mrow>
<mrow>  &amp; = e^{-\mu} \cdot \mu^2 \SUM {k=0} {\infty} {\frac{\mu^k}{k!} } + \mu - \mu^2</mrow>
<mrow>  &amp; = e^{-\mu} \cdot \mu^2 e^{\mu} + \mu - \mu^2</mrow>
<mrow> &amp; = \mu^2 + \mu - \mu^2 </mrow>							
<mrow> &amp; = \mu </mrow>							
</md>

					</proof>
				</theorem>
			</subsection>
		</section>
		
		<section xml:id="ExponentialDistribution">
			<title>Exponential and Gamma Distributions</title>
			<p>Consider the situation where you want to measure the time until a desired number of successes. 
			That is, the counterpart to the 
			Poisson Distribution above. This would be a continous distribution since the variable is time. 
			If you measure the time till the
			1st success then the resulting distribution is called the “exponential distribution” and if you 
			measure till the kth success, k>1, 
			then the resulting distribution is called the “gamma distribution”.</p>
			<subsection>
				<title>Exponential Distribution Derivation</title>
				<p>Let X measure the time till the first success. 
				Then P(first success occurs in the interval [0,x]) = F(x) = 1 - P(no successes in the interval [0,x]), 
				which is a Poisson question
				using the interval [0,x].  Since we know for Poisson that the mean number of successes in [0,T] is 
				<m>\lambda T</m> then the
				mean in this case is <m> \lambda x </m>.  Applying the known formulas...</p>
				<md>
					<mrow>F(x) &amp; = 1 - P(no successes) </mrow>
					<mrow> &amp; = 1 - (\lambda x)^0 e^{-\lambda x}/0!</mrow>
					<mrow> &amp; = 1 - e^{-\lambda x}</mrow>
				</md>
				<p>and so we get the probability function</p>
				<m> \displaystyle f(x) = F’(x) = \lambda e^{-\lambda x}</m>
				<subsection>
					<title>Verification of exponential probability function</title>
					<p>Presuming <m>\lambda > 0</m>,</p>
					<md>
						<mrow>\int_0^{\infty} \lambda e^{-\lambda x} dx = -e^{-\lambda x} \Bigr|_0^{\infty} = -0 + 1 = 1</mrow>
					</md>
				</subsection>
				<subsection>
					<title>Mean of the exponential distribution</title>
					<md>
						<mrow>\mu = E[x] &amp; = \int_0^{\infty} x \cdot \lambda e^{-\lambda x} dx</mrow>
						<mrow>&amp; = -x \cdot e^{-\lambda x} \Bigr|_0^{\infty} + \int_0^{\infty} e^{-\lambda x} dx</mrow>
						<mrow>&amp; = 0 + \frac{e^{-\lambda x}}{\lambda} \Bigr|_0^{\infty} </mrow>
						<mrow>&amp; = \frac{1}{\lambda}</mrow>
					</md>
				</subsection>
				<subsection>
					<title>Alternate form for the exponential distribution</title>
						<p>Given the mean time until the first success <m>\mu</m>,</p>
						<md>
							<mrow>f(x) = \frac{e^{\frac{-x}{\mu}}}{\mu}</mrow>
						</md>
				</subsection>
				<subsection>
					<title>Variance of the exponential distribution</title>
					<md>
						<mrow>\sigma^2 &amp; = E[X^2] - \mu^2</mrow>
						<mrow>&amp; = \int_0^{\infty} x^2 \frac{e^{\frac{-x}{\mu}}}{\mu} dx - \mu^2</mrow>
						<mrow>&amp; = -(2 \mu^3 + 2 \mu^2 x + \mu x^2) e^{\frac{-x}{\mu}}/ \mu \Bigr|_0^{\infty} - \mu^2</mrow>
						<mrow>&amp; = 2 \mu^2  - \mu^2</mrow>
						<mrow>&amp; = \mu^2</mrow>
					</md>
				</subsection>
			</subsection>
			<subsection>
				<title>Gamma Distribution Derivation</title>
			</subsection>
			
        </section>
        
        
        <section xml:id="NormalDistribution">
        	<title>The Normal Distribution</title>
        	<p>A “bell curve” is often utilized when analyzing data. The reasons for doing so can be justified 
        	mathematically. For the purposes
        	of these notes, we will just presume that a given problem presumes the need for such a probability 
        	distribution and use it. In general,
        	given two parameters <m>\mu</m> and <m>\sigma</m>, the density function for the bell curve is given by</p>
        	<md><mrow>f(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{- \left ( \frac{x-\mu}{\sigma}\right )^2 / 2}</mrow></md>
        	<p>The resulting probability distribution is called the “Normal Distribution”.</p>
        
        	<subsection>
        		<title>Derivation of properties of the Normal Distribution</title>
        		<subsection>
        			<title>Standard Units and the Standard Normal Curve</title>
        			<p>Converting to standard units using <m>z = \frac{x-\mu}{\sigma}</m> gives the standard normal 
        			distribution density function</p>
        			<md><mrow>\Phi(z) = \frac{1}{\sqrt{2\pi}}e^{-z^2 / 2}</mrow></md>.
        			<p>In particular, when integrating this always given the simplification</p>
        				<md><mrow>\int_{-\infty}^{\infty} \frac{1}{\sigma \sqrt{2\pi}}e^{- \left ( \frac{x-\mu}{\sigma}\right )^2 / 2} dx
        						= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}e^{-z^2 / 2} dz</mrow></md>
        			
        		</subsection>
        		<subsection>
        			<title>Total Area under normal curve</title>
        			<p>By converting to standard units, it is sufficient to show that</p>
        				<md><mrow>I = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}e^{-z^2 / 2} dz = 1</mrow></md>
        			<p>Toward that end, consider <m>I^2</m> with a couple of changes in the variable of integration 
        			(not related to the x above)</p>

<md>
<mrow>I^2 &amp; = \left [ \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}e^{-x^2 / 2} dx \right ]
			\left [ \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}e^{-y^2 / 2} dy \right ]</mrow>
<mrow> &amp; = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{2\pi}e^{-x^2 / 2}e^{-y^2 / 2} dx dy</mrow>
<mrow> &amp; = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{2\pi}e^{-(x^2+y^2)/ 2} dx dy</mrow>
<mrow> &amp; = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{2\pi}e^{-(x^2+y^2)/ 2} dx dy</mrow>
<mrow> &amp; = \int_{0}^{2\pi} \int_{0}^{\infty} \frac{1}{2\pi}e^{-r^2/ 2} r dr d\theta</mrow>
<mrow> &amp; = \frac{1}{2\pi} \int_{0}^{2\pi} -e^{-r^2/ 2} \Big |_0^{\infty} d\theta</mrow>
<mrow> &amp; = \frac{1}{2\pi} \int_{0}^{2\pi} 1 d\theta</mrow>
<mrow> &amp; = \frac{1}{2\pi} \cdot 2\pi = 1</mrow>
</md>

        		</subsection>
        		<theorem>
        			<title>Mean of the Normal Distribution</title>
        			<statement>The mean of the normal distribution is the provided value of the parameter <m>\mu</m></statement>
        			<proof>
        			<p>Using <m>z = \frac{x-\mu}{\sigma}</m> gives <m>x = \sigma z + \mu</m> resulting in</p>

<md>
<mrow>E[X] &amp; = \int_{-\infty}^{\infty} \frac{1}{\sigma \sqrt{2\pi}} x \cdot e^{- \left ( \frac{x-\mu}{\sigma}\right )^2 / 2} dx</mrow>
<mrow> &amp; = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}(\sigma z + \mu) e^{-z^2 / 2} dz </mrow>
<mrow> &amp; = \sigma \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} z e^{-z^2 / 2} dz 
	 + \mu \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-z^2 / 2} dz </mrow>
<mrow> &amp; = \sigma \cdot -e^{-z^2/2} \Big |_{-\infty}^{\infty} + \mu \cdot 1</mrow>
<mrow> &amp; = 0 + \mu</mrow>
</md>

   					<p>Therefore, the provided parameter <m>\mu</m> is indeed the mean of the distribution.</p>
   					</proof>

        		</theorem>
        		<theorem>
        			<title>Variance of the Normal Distribution</title>
        			<statement>The variance of the normal distribution is the provided value of the parameter <m>\sigma^2</m></statement>
        			<proof>
        			<p>Once again by using <m>x = \sigma z + \mu</m> yields (using integration by parts on the final integral)</p>

<md>
<mrow>Var(X) &amp; = E[X^2] - \mu^2 </mrow>
<mrow> &amp; = 
	   \int_{-\infty}^{\infty} \frac{1}{\sigma \sqrt{2\pi}} x^2 \cdot e^{- \left ( \frac{x-\mu}{\sigma}\right )^2 / 2} dx - \mu^2</mrow>
<mrow> &amp; = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}(\sigma z + \mu)^2 e^{-z^2 / 2} dz - \mu^2</mrow>
<mrow> &amp; = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \sigma^2 z^2 e^{-z^2 / 2} dz
		+ \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} 2\sigma z \cdot \mu e^{-z^2 / 2} dz
		+ \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \mu^2 e^{-z^2 / 2} dz - \mu^2</mrow>
<mrow> &amp; = \sigma^2 \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} z^2 e^{-z^2 / 2} dz
		+ 2\sigma \mu \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} z e^{-z^2 / 2} dz
		+ \mu^2 \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-z^2 / 2} dz - \mu^2</mrow>
<mrow> &amp; = \sigma^2 \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} z \cdot z e^{-z^2 / 2} dz + 2\sigma \mu \cdot 0 + \mu^2 -\mu^2</mrow>
<mrow> &amp; = \sigma^2 \left [ -z e^{-z^2/2} \Big |_{-\infty}^{\infty} 
		- \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} -e^{-z^2 / 2} dz \right ] + 0</mrow>
<mrow> &amp; = \sigma^2 \left [ (0-0) - (-1) \right ]</mrow>
<mrow> &amp; = \sigma^2</mrow>
</md>
   					
   					<p>Therefore, the provided parameter <m>\sigma</m> is indeed the standard deviation of the 
   					distribution and it’s square is the variance.</p>
   					</proof>
        		</theorem>
        		<theorem>
        			<title>Maximum of the Normal Distribution</title>
        			<statement>The maximum of the normal curve occurs at the mean.</statement>
        			<proof>
        			
<md>
<mrow>f(x) &amp; = \frac{1}{\sigma \sqrt{2\pi}}e^{- \left ( \frac{x-\mu}{\sigma}\right )^2 / 2}</mrow>
<mrow>\Rightarrow f’(x) &amp; 
  = \frac{1}{\sigma \sqrt{2\pi}}e^{- \left ( \frac{x-\mu}{\sigma}\right )^2 / 2} \cdot -\frac{x-\mu}{\sigma} \cdot \frac{1}{\sigma}</mrow>
</md>
        			
        			<p>Setting this first derivative equal to zero to determine the critical values yields 
        			<m>\displaystyle x = \mu</m> as the 
        			only critical value.  Easily, it can be seen that this yields a maximum (see second derivative below) 
        			and therefore the normal 
        			curve obtains a maximum at the mean and has maximum value <m>\frac{1}{\sigma \sqrt{2\pi}}</m>.</p>
        			</proof>
        		</theorem>
        		<theorem>
        			<title>Points of Inflection of the Normal Distribution</title>
        			<statement>The normal curve has points of inflection when <m>\displaystyle x = \mu + \sigma</m> 
        			and <m>\displaystyle x = \mu - \sigma</m>.</statement>
        			<proof>
        			
<md>
<mrow>f’(x) &amp; = \frac{-1}{\sigma^3 \sqrt{2\pi}}e^{- \left ( \frac{x-\mu}{\sigma}\right )^2 / 2} \cdot (x-\mu)</mrow>
<mrow>\Rightarrow f’’(x) &amp; 
= \frac{-1}{\sigma^3 \sqrt{2\pi}}e^{- \left ( \frac{x-\mu}{\sigma}\right )^2 / 2} \cdot \left [(x-\mu) \cdot (\frac{x-\mu}{\sigma}) \cdot (\frac{1}{\sigma}) - 1\right ] </mrow>
<mrow> &amp; = \frac{-1}{\sigma^3 \sqrt{2\pi}} e^{- \left ( \frac{x-\mu}{\sigma}\right )^2 / 2} \cdot \left [  (\frac{x-\mu}{\sigma})^2 - 1 \right ] </mrow>
</md>
        			
        			<p>Setting this second derivative equal to zero to determine the locations of possible points of inflection yields 
        			    <m>\displaystyle (\frac{x-\mu}{\sigma})^2 = 1</m> or <m>\displaystyle \Big | \frac{x-\mu}{\sigma} \Big | = 1</m>
        			    or finally 
        			    <m>\displaystyle x = \mu + \sigma</m> and <m>\displaystyle x = \mu - \sigma</m>.</p>
        			</proof>
        		</theorem>
        	</subsection>
        </section>

    </article>

</mathbook>