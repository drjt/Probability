<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="RepresentingData" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Statistical Measures</title>


<section><title>Introduction</title>
<p>
To compute your final grade in a class your teacher will first make several assignments and examinations for you to take. These assessments ultimately will be assigned some numerical score indicating your level of success. However, you final grade can only be one value and it would make sense that the grade be a reflection of your work on these tasks. But, it can only be a single value that represents the totality of your work in the course. 
</p>
<p>
In this chapter, you will consider a number of ways to use point values to represent different aspects of a provided set of data. In doing so, you will also need to take into account whether that data set is the entire list of possibilities--known as the population--or just a subset of that population perhaps obtained by taking repeated measurements from that population--that is, a sample. Each of these values will be called a "statistical measure".
</p>
</section>
<section>	
	<title>Measurement Scales</title>
	<p>In creating statistical measures, you might want to consider one of the following general types.
	<ul>
		<li>Nominal measures - In this case, data falls into mutually exclusive and exhaustive categories for which the numerical value is only used for identification purposes. For example, assigning Male = 1, Female = -1.</li>
		<li>Ordinal measures - In this case, data consists of discrete numerical values which can be ranked from lowest to highest or vice versa. For example, your grades in a class grades which are used to compute your GPA.</li>
		<li>Interval measures - In this case, data possesses an order and where the distance between data values is of significance. For example, heights and weights.</li>
		<li>Ratio measures - In this case, data can be expressed as a position in some interval and where ratios between observations have meaning. For example, percentile rankings</li>
	</ul>
	In the subsequent sections of this chapter, you will see that a number of different measures are available for most data sets. Determining which "correct" measure to use for describing any given data set will depend the actual situation surrounding the collection of the data.
	</p>
<p>
The following categorizes several statistical measures which will be developed in this chapter. Details on each are provided in subsequent sections.
<ul>
	<li>Tabular Methods - 
	based on the entire population yielding a global picture
		<ul>
			<li>frequency distributions</li>
			<li>relative frequency distributions</li>
			<li>cummulative frequency distributions</li>
			<li>Stem-and-Leaf Displays</li>
			<li>Box-and-Whisker Diagrams</li>
		</ul>
	</li>
	<li>Summary Methods
		<ul>
			<li>Measures of the center
				<ol>
				 <li>Mean</li>
				 <li>Median</li>
				 <li>Mode</li>
				</ol>
			</li>
			<li>Measures of spread
				<ol>
				 <li>Range</li>
				 <li>Variance and Standard Deviation</li>
				 <li>Interquartile Range</li>
				 <li>Quantiles</li>
				 </ol>
			</li>
			<li>Measures of Skewness 
			- indicates the level of symmetry of the data
				<ol>
				 <li>Standard Skewness</li>
				 <li>Pearson Coefficient</li>
				</ol>
			</li>
			<li>Measures of Kurtosis 
			- indicates flatness or roundedness of the peak of the data
				<ol>
				 <li>Standard Kurtosis</li>
				 <li>Coefficient of Kurtosis</li>
				</ol>
			</li>
			<li>Detection of Outliers 
			- indicates whether abnormally large or small data distorts other 
			techniques
				<ol>
				 <li>Z-scores</li>
				 <li>Trimming</li>
				 <li>Winsorizing</li>
				</ol>
			</li>
			<li>Tests for Normality 
			- indictes if the data is bell-shaped
				<ol>
				 <li>Standard Percentages relative to standard deviations from the mean</li>
				</ol>
			</li>
		</ul>
	</li>

</ul>
</p>
<p>Remark: Many of these measures above are relative and some are absolute.</p>
	
</section>




<section><title>Statistical Measures of Position</title>
	<p>Given a collection of data, sorting the data may provide several useful descriptors. When sorting data, you can easily use something like a spreadsheet for larger data sets but in this section you will also see there are ways to perform a sort by hand. In either case, statistical measures of position generally involve very little computational work and take into account only the order of the data from lowest to highest.  To assist with notation, we will generally use x-values to represent the original raw data and y-values to represent that same data but now in order with the subscript indicating the positional placement.
	</p>
	
	
	<definition>
	<title>Order Statistic</title>
	<p>From the data set <m>x_1, x_2, ... , x_n</m>, label the sorted data as <m>y_1, y_2, ..., y_n</m> where  
	<me> y_1 \le y_2 \le ... \le y_n.</me> 
	Define <m>y_k</m> as the kth order statistic. </p>
	</definition>
	
	<example><title>Age of Presidents - order statistics</title>
	<p>
	For example, the age at inauguration for presidents from 1981-2016 gives the data <m>x_1 = 69, x_2 = 64, x_3 = 46, x_4 = 54, x_5 = 47</m> (Reagan, Bush, Clinton, Bush, Obama). For this data, the order statistics are denoted <m>y_1 = 46, y_2 = 47, y_3 = 54, y_4 = 64, y_5 = 69</m>.
	</p>
	</example>
	
	<p>
	Once the data is sorted, it should be very easy for you to locate the smallest and largest values. 
	</p>
	<definition><title>Minimum/Maximum:</title>  
	<p>The smallest and largest values in the data set. Using the notation above, minimum = <m>y_1</m> and the maximum = <m>y_n</m></p>
	</definition>
	
	<example><title>Age of Presidents - Minimum/Maximum</title>
	<p>
	Using the Presidential ages above, minimum = <m>y_1 = 46</m> and maximum = <m>y_5 = 69</m>.
	</p>
	</example>

	<p>
	Below, you will see how to determine a value which separates the ordered data into two groups with a desired percentage on each side. Note that many utilize approximate methods for computing "percentiles" including many graphing and scientific calculators. The definition below provides for a unique measure for each unique value of p.
	</p>
	
	<definition><title>Percentiles</title>
	<p>A percentile is a numerical value <m>P^p</m> at which approximately 100p% of the given data is smaller.</p>
	<p>To compute the percentile value with <m>0 \lt p \lt 1</m> for order statistics <m>y_1, y_2, ..., y_n</m> use the formula 
		<me>P^{p} = (1-r)y_m + ry_{m+1}</me>
	where m is the integer part of (n+1)p, namely 
		<me>m = \left\lfloor (n+1)p \right\rfloor</me> 
	and 
		<me>r = (n+1)p - m,</me>
	
	the fractional part of (n+1)p. 
	</p>
	</definition> 
	
<p>The formula for percentiles determines a weighted average between <m>y_m</m> and <m>y_{m+1}</m> which is unique for distinct values of p provided each of the data values are distinct. Note that if some of the y-values are equal then some of these averages might be averages of equal numbers and will then be the common value.</p>
	
	<example><title>Small Example - Quartiles</title>
	<p>
Consider the following data set: {2,5,8,10}. The 50th percentile should be a numerical value for which approximately 50% of the data is smaller. In this case, that would be some number between 5 and 8.  For now, let's just take 6.5 so that two numbers in the set lie below 6.5 and two lie above. This is a perfect 50% for the 50th percentile. In a similar manner, the 25th percentile would be some number between 2 and 5, say 2.75, so that one number lies below 2.75 and three numbers lie above.
	</p>

		<p>More precisely	, the 25th percentile is computed by considering 
		<me>(n+1)p = (4+1)0.25 = 5/4 = 1.25</me>.  
		So, m = 1 and r = 0.25. Therefore 
		<me>P^{0.25} = 0.75 \times 2 + 0.25 \times 5 = 2.75</me> 
		as noted above. 
		</p>
		<p>
		Similarly, the 75th percentile is given by
		<me>(n+1)p = (4+1)0.75 = 15/4 = 3.75</me>.  
		So, m = 3 and r = 0.75. Therefore 
		<me>P^{0.75} = 0.25 \times 8 + 0.75 \times 10 = 9.5</me> 
		
		It is interesting to note that 3 also lies between 2 and 5 as does 2.75 and has the same percentages above (75 percent) and below (25 percent). However, it should designate a slightly larger percentile location. Indeed, going backward:
		<md>
			<mrow>3 = (1-r) \times 2 + r \times 5</mrow>
			<mrow>\Rightarrow r = \frac{1}{3}</mrow>
			<mrow>\Rightarrow (n+1)p = 1 + \frac{1}{3} = \frac{4}{3}</mrow>
			<mrow>\Rightarrow p = \frac{4}{15} \approx 0.267</mrow>
		</md>
		and so 3 would actually be at approximately the 26.7th percentile.
		</p>
	</example>

	<p>Some special percentiles are provided special names...
	</p>

	<definition><title>Quartiles</title> 
	<p>Given a sorted data set, the first, second, and third quartiles are the values of 
	<m>Q_1 = P^{0.25}, Q_2 = P^{0.5}</m> and <m>Q_3 = P^{0.75}</m>.
	</p>
	</definition>

	<definition><title>Deciles:</title>
	<p>Given a sorted data set, the first, second, ..., ninth deciles are the value of 
	<m>D_1 = P^{0.1}, D_2 = P^{0.2}, ... , D_9 = P^{0.9}</m>
	</p>
	</definition>
	
	<p>
	For your data set {2,5,8,10}, 
	<m>Q_1 = 2.75, Q_2 = 6.5,</m> and <m>Q_3 = 9.5</m>.
	</p>

	<p>
	For a given data set, a summary of these statistics is often desired in order to give the user a quick overview of the more important order statistics.
	</p>

	<definition><title>5-number summary</title>
	<p>Given a set of data, the 5-number summary is a vector of the order statistics given by <m>\lt</m> minimum, <m>Q_1</m>, <m>Q_2</m>, <m>Q_3</m>, maximum <m>\gt</m>. 
	</p>
	</definition>
	
	<example><title>Small example - 5 number summary</title>
	<p>Returning to our previous example, the five number summary would be
	<m>\lt 2, 2.75, 6.5, 9.5, 10 \gt</m>
	</p>
	</example>


</section>




<section>
<title>Statistical Measures of the Middle</title>

	<definition><title>Arithmetic Mean</title>
	<p>Suppose X is a discrete random variable with range 
	<m>R = {x_1, x_2, ..., x_n}</m>. 
	The arithmetic mean is given by
		<me>
		AM = \frac{x_1 + ... + x_n}{n} = \frac{\sum_{k=1}^n x_k}{n}.
		</me>
	If this data comes from sample data then we call it a sample mean and denote this value by <m>\overline{x}</m>. If this data comes from the entire universe of possibilities then we call it a population mean and denote this value by <m>\mu</m>.  When presented with raw data, it might be good to generally presume that data comes from a sample and utilize <m>\overline{x}</m>.</p>
	</definition>
	
	<p>
	To illustrate, consider the previous data set: {2,5,8,10}. The arithmetic mean is given by
	<me>\overline{x} = \frac{2+5+8+10}{4} = \frac{25}{4} = 6.25.</me>
	</p>
	
	<p>
	The mean is often called the centroid in the sense that if the x values were locations of objects of equal weight, then the centroid
	would be the point where this system of n masses would balance. 
	</p>
	<p>
	The values can all be provided with varying weights if desired and the result is called the weighted arithmetic mean and is given by
		<me>
		\frac{m_1 x_1 + ... + m_n x_n}{m_1 + ... + m_n} = \frac{\sum_{k=1}^n m_k x_k}{\sum_{k=1}^n m_k}.
		</me>
	</p>


<definition><title>Median:</title>
<p>A positional measure of the middle is often utilized by finding the location of the 50th percentile. This value is also called the median and indicates the value at which approximately half the sorted data lies below and half lies above.</p>
</definition>

<p>
For data sets with an odd number of values, this is the "middle" data value if one were to successively cross off pairs from the two ends of the sorted date. For data sets with an even number of values, this is a average of the two data values left after crossing off these pairs.  Using the order statistics, the median equals
	<me>y_{\frac{n+1}{2}}</me>
if n is odd and
	<me>\frac{y_\frac{n}{2} + y_{\frac{n}{2}+1}}{2}</me>
if n is even.
</p>

<p>From the Presidential data, note that you are considering an odd number of data values and so the median is given by 54. </p>


<definition><title>Midrange:</title>
<p>A mixture of the mean and median where one takes the simple average of the maximum and minimum values in the data set. Using the order statistics, this equals 
	<me>\frac{y_1+y_n}{2}</me>
</p>
</definition>

<p>From the Presidential data, the maximum is 69 and the minimum is 46 so the midrange is 57.5, the average of these two. </p>

<p>
<title>Advantages and Disadvantages of Measures of the Middle</title>
Mean utilizes all of the data values so each term is important. Utilizes them all even if some of the data values might suffer from collection errors.  Median ignores outliers (which might be a result of collection errors) but does not account for the relative differences between terms. Midrange is very easy to compute but ignores the relative differences for all terms but the two extremes.
</p>

<example><title>Numerical Example of these Quantitative Measures</title>
<p>The US Census Bureau reported the following state populations (in millions) for 2013:
<url href="Data/USA_States_Populations_2014.xlsx">Spreadsheet</url>
</p>
<p>
<table halign="left">
      <tabular halign="right">
      
<row><cell bottom="medium">State</cell><cell bottom="medium">Population</cell></row>      
<row><cell>Wyoming</cell><cell>0.6</cell></row>
<row><cell>Vermont</cell><cell>0.6</cell></row>
<row><cell>District of Columbia</cell><cell>0.6</cell></row>
<row><cell>North Dakota</cell><cell>0.7</cell></row>
<row><cell>Alaska</cell><cell>0.7</cell></row>
<row><cell>South Dakota</cell><cell>0.8</cell></row>
<row><cell>Delaware</cell><cell>0.9</cell></row>
<row><cell>Montana</cell><cell>1</cell></row>
<row><cell>Rhode Island</cell><cell>1.1</cell></row>
<row><cell>New Hampshire</cell><cell>1.3</cell></row>
<row><cell>Maine</cell><cell>1.3</cell></row>
<row><cell>Hawaii</cell><cell>1.4</cell></row>
<row><cell>Idaho</cell><cell>1.6</cell></row>
<row><cell>West Virginia</cell><cell>1.9</cell></row>
<row><cell>Nebraska</cell><cell>1.9</cell></row>
<row><cell>New Mexico</cell><cell>2.1</cell></row>
<row><cell>Nevada</cell><cell>2.8</cell></row>
<row><cell>Kansas</cell><cell>2.9</cell></row>
<row><cell>Utah</cell><cell>2.9</cell></row>
<row><cell>Arkansas</cell><cell>3</cell></row>
<row><cell>Mississippi</cell><cell>3</cell></row>
<row><cell>Iowa</cell><cell>3.1</cell></row>
<row><cell>Connecticut</cell><cell>3.6</cell></row>
<row><cell>Oklahoma</cell><cell>3.9</cell></row>
<row><cell>Oregon</cell><cell>3.9</cell></row>
<row><cell>Kentucky</cell><cell>4.4</cell></row>
<row><cell>Louisiana</cell><cell>4.6</cell></row>
<row><cell>South Carolina</cell><cell>4.8</cell></row>
<row><cell>Alabama</cell><cell>4.8</cell></row>
<row><cell>Colorado</cell><cell>5.3</cell></row>
<row><cell>Minnesota</cell><cell>5.4</cell></row>
<row><cell>Wisconsin</cell><cell>5.7</cell></row>
<row><cell>Maryland</cell><cell>5.9</cell></row>
<row><cell>Missouri</cell><cell>6</cell></row>
<row><cell>Tennessee</cell><cell>6.5</cell></row>
<row><cell>Indiana</cell><cell>6.6</cell></row>
<row><cell>Arizona</cell><cell>6.6</cell></row>
<row><cell>Massachusetts</cell><cell>6.7</cell></row>
<row><cell>Washington</cell><cell>7</cell></row>
<row><cell>Virginia</cell><cell>8.3</cell></row>
<row><cell>New Jersey</cell><cell>8.9</cell></row>
<row><cell>North Carolina</cell><cell>9.8</cell></row>
<row><cell>Michigan</cell><cell>9.9</cell></row>
<row><cell>Georgia</cell><cell>10</cell></row>
<row><cell>Ohio</cell><cell>11.6</cell></row>
<row><cell>Pennsylvania</cell><cell>12.8</cell></row>
<row><cell>Illinois</cell><cell>12.9</cell></row>
<row><cell>Florida</cell><cell>19.6</cell></row>
<row><cell>New York</cell><cell>19.7</cell></row>
<row><cell>Texas</cell><cell>26.4</cell></row>
<row><cell>California</cell><cell>38.3</cell></row>      
      
      </tabular>
      </table>

Determine the minimum, maximim, midrange, and mean for this data.
	<solution>
		<p>Notice that these are already in order so you can presume 
		<m>y_1 = 0.6</m> million is the minimum and <m>y_{50} = 38.3</m> 
		million is the maximum. Therefore, the midrange is given by
		<me>\frac{0.6+38.3}{2} = \frac{38.9}{2} = 19.45 \text{million}.</me>

		Note, in this collection of "states" data the District of Columbia is included so that the number of data items is n=51. The mean of this data takes a bit of arithmetic but gives
		<me>\overline{x} = \frac{\sum_{k=1}^{51} y_k }{51} = \frac{316.1}{51} \approx 6.20</me>
		million residents. 
		</p>
		<p>
		Since the number of states is odd, the median is found by looking at the 26th order statistics. In this case, that is the 4.6 million residents of Louisiana.
		</p>
	</solution>

</p>
</example>

</section>





<section>
<title>Statistical Measures of Variation</title>
<p>These measures provide some indication of how much the data set is "spread out".
</p>

<definition><title>Range:</title>
<p>Using the order statistics, <me>y_n - y_1.</me>  
Easy to compute. Ignores the spread of all the data in between.
</p>
</definition>

<p>From the Presidential data, the maximum is 69 and the minimum is 46 so the range is 23, the difference of these two. </p>

<definition><title>Interquartile Range (IQR):</title>
<p><m>P^{0.75} - P^{0.25}</m>. 
</p>
</definition>

<p>
For the data set {2, 5, 8, 10}, you have found that <m>Q_1 = 2.75</m> and <m>Q_3 = 9.5</m>. Therefore, <me>IQR = 9.5 - 2.75 = 6.75.</me>
</p>

<p>
Average Deviation from the Mean (Population):  Given a population data set <m>x_1, x_2, ... , x_n</m> with mean <m>\mu</m> each term deviates from the mean by the value <m>x_k - \mu</m>. So, averaging these gives
<me> \frac{\sum_{k=1}^n (x_k-\mu)}{n} = \frac{\sum_{k=1}^n x_k}{n} - \frac{\sum_{k=1}^n \mu}{n} = \mu - \mu = 0</me>
which is always zero for any provided set of data. This cancellation makes this measure not useful. To avoid cancellation, perhaps removing negatives would help.
</p>

<p>Average Absolute Deviation from the Mean (Population):  
<me> \frac{\sum_{k=1}^n \left | x_k-\mu \right |}{n} </me>
which, although nicely stated, is difficult to deal with algebraically since the absolute values do not simplify well algebraically. To avoid this algebraic roadblock, we can look for another way to nearly accomplish the same goal by squaring and then square rooting. 
</p>

<p>Average Squared Deviation from the Mean (Population):
<me> \frac{\sum_{k=1}^n ( x_k-\mu )^2}{n} </me>
which will always be non-negative but can be easily expanded using algebra. Since this is a mouthful, this measure is generally called the variance. 
</p>

<p>
Using the average squared deviation from the mean, differences have been squared. Thus all values added are non-negative but very small ones have been made even smaller and larger ones have possibly been made much larger. To undo this scaling issue, one must take a square root to get things back into the right ball park. 
</p>

<p>The variance is the average squared deviation from the mean. If this data comes from the entire universe of possibilities then we call it a population variance and denote this value by <m>s^2</m>. Therefore
	<me>s^2 = \frac{\sum_{k=1}^n ( x_k-\mu )^2}{n} </me>
</p>
<p>The standard deviation is the square root of the variance. If this data comes from the entire universe of possibilities then we call it a population standard deviation and denote this value by <m>\sigma</m>. Therefore
<me> \sigma = \sqrt{\frac{\sum_{k=1}^n ( x_k-\mu )^2}{n}}.</me>
</p>

<p>From the data {2,5,8,10}, you have found that the mean is 6.25. Computing the variance then involves accumulating and averaging the squared differences of each data value and this mean. Then
<md>
	<mrow>&amp; \frac{1}{4} \left ( (2-6.25)^2 + (5-6.25)^2 + (8-6.25)^2 + (10-6.25)^2 \right ) </mrow>
	<mrow>&amp; = \frac{18.0625 + 1.5625 + 3.0625 + 14.0625}{4} </mrow>
	<mrow>&amp; = \frac{36.75}{4}</mrow>
	<mrow>&amp; = 9.1875.</mrow>
	Therefore, the standard deviation for this data set is given by
	<me>\sqrt{9.1875} \approx 3.031.</me>
</md>
</p>

<p>
If data comes from a sample of the population then we call it a sample variance and denote this value by v. Since sample data tends to reflect certain "biases" then we increase this value slightly by <m>\frac{n}{n-1}</m> to give the sample variance
<me>s^2 = \frac{n}{n-1}\frac{\sum_{k=1}^n ( x_k-\overline{x} )^2}{n} = \frac{\sum_{k=1}^n ( x_k-\overline{x} )^2}{n-1}.</me>
and the sample standard deviation similarly as the square root of the sample variance.
</p>

<theorem><title>Alternate Forms for Variance</title>
<statement>
<md>
<mrow>\sigma^2 &amp; = \left ( \frac{\sum_{k=1}^n x_k^2 }{n} \right ) - \mu^2 </mrow>
<mrow>&amp; = \left [ \frac{\sum_{k=1}^n x_k(x_k - 1)}{n} \right ] + \mu - \mu^2</mrow>
</md>
</statement>
<proof>
TBA
</proof>
</theorem>


<p>The Population of the individual USA states according to the 2013 Census 
Consider the data set {}</p>

<exercise><title>Numerical Example of these Quantitative Measures</title>
<p>The US Census Bureau reported the following state populations (in millions) for 2013:</p>
<p>
<table halign="left">
      <tabular halign="right">
      
<row><cell bottom="medium">State</cell><cell bottom="medium">Population</cell></row>      
<row><cell>Wyoming</cell><cell>0.6</cell></row>
<row><cell>Vermont</cell><cell>0.6</cell></row>
<row><cell>District of Columbia</cell><cell>0.6</cell></row>
<row><cell>North Dakota</cell><cell>0.7</cell></row>
<row><cell>Alaska</cell><cell>0.7</cell></row>
<row><cell>South Dakota</cell><cell>0.8</cell></row>
<row><cell>Delaware</cell><cell>0.9</cell></row>
<row><cell>Montana</cell><cell>1</cell></row>
<row><cell>Rhode Island</cell><cell>1.1</cell></row>
<row><cell>New Hampshire</cell><cell>1.3</cell></row>
<row><cell>Maine</cell><cell>1.3</cell></row>
<row><cell>Hawaii</cell><cell>1.4</cell></row>
<row><cell>Idaho</cell><cell>1.6</cell></row>
<row><cell>West Virginia</cell><cell>1.9</cell></row>
<row><cell>Nebraska</cell><cell>1.9</cell></row>
<row><cell>New Mexico</cell><cell>2.1</cell></row>
<row><cell>Nevada</cell><cell>2.8</cell></row>
<row><cell>Kansas</cell><cell>2.9</cell></row>
<row><cell>Utah</cell><cell>2.9</cell></row>
<row><cell>Arkansas</cell><cell>3</cell></row>
<row><cell>Mississippi</cell><cell>3</cell></row>
<row><cell>Iowa</cell><cell>3.1</cell></row>
<row><cell>Connecticut</cell><cell>3.6</cell></row>
<row><cell>Oklahoma</cell><cell>3.9</cell></row>
<row><cell>Oregon</cell><cell>3.9</cell></row>
<row><cell>Kentucky</cell><cell>4.4</cell></row>
<row><cell>Louisiana</cell><cell>4.6</cell></row>
<row><cell>South Carolina</cell><cell>4.8</cell></row>
<row><cell>Alabama</cell><cell>4.8</cell></row>
<row><cell>Colorado</cell><cell>5.3</cell></row>
<row><cell>Minnesota</cell><cell>5.4</cell></row>
<row><cell>Wisconsin</cell><cell>5.7</cell></row>
<row><cell>Maryland</cell><cell>5.9</cell></row>
<row><cell>Missouri</cell><cell>6</cell></row>
<row><cell>Tennessee</cell><cell>6.5</cell></row>
<row><cell>Indiana</cell><cell>6.6</cell></row>
<row><cell>Arizona</cell><cell>6.6</cell></row>
<row><cell>Massachusetts</cell><cell>6.7</cell></row>
<row><cell>Washington</cell><cell>7</cell></row>
<row><cell>Virginia</cell><cell>8.3</cell></row>
<row><cell>New Jersey</cell><cell>8.9</cell></row>
<row><cell>North Carolina</cell><cell>9.8</cell></row>
<row><cell>Michigan</cell><cell>9.9</cell></row>
<row><cell>Georgia</cell><cell>10</cell></row>
<row><cell>Ohio</cell><cell>11.6</cell></row>
<row><cell>Pennsylvania</cell><cell>12.8</cell></row>
<row><cell>Illinois</cell><cell>12.9</cell></row>
<row><cell>Florida</cell><cell>19.6</cell></row>
<row><cell>New York</cell><cell>19.7</cell></row>
<row><cell>Texas</cell><cell>26.4</cell></row>
<row><cell>California</cell><cell>38.3</cell></row>      
      
      </tabular>
      </table>
</p>
<p>
Determine the range, quartiles, and variance for this sample data.
	<solution> 
	<p>Again, you should note that these are already in order so the range is quickly found to be 
	<me>y_n - y_1 = 38.3 - 0.6 = 37.7</me> 
	million residents.
	</p>
	<p>
	For IQR, we first must determine the quartiles. The median (found earlier) already is the second quartile so we have <m>Q_2 = 4.5</m> million. For the other two, the formula for computing percentiles gives you the 25th percentiile
	<md>
		<mrow>(n+1)p = 51(1/4) = 12.75</mrow>
		<mrow>Q_1 = P^{0.25} = 0.25 \times 1.9 + 0.75 \times 2.1 = 2.05</mrow>
	</md>
	and the 75th percentile
	<md>
		<mrow>(n+1)p = 51(3/4) = 38.25</mrow>
		<mrow>Q_3 = P^{0.75} = 0.75 \times 7 + 0.25 \times 8.3 = 7.325.</mrow>
	</md>
	Hence, the IQR = 7.325 - 2.05 = 5.275 million residents.
	</p>

	<p>
	From the computation before, again note that n=51 since the District of Columbia is included. The mean of this data found before was found to be approximately 6.20 million residents. So, to determine the variance you may find it easier to compute using the theorem above. 
	<md>
	<mrow> v &amp; = \left ( \frac{\sum_{k=1}^n y_k^2 }{n} \right ) - \mu^2
	</mrow>
	<mrow> &amp; \approx \frac{4434.37}{51} - (6.20)^2
	</mrow>
	<mrow> &amp; = 48.51
	</mrow>
	</md>
	and so you get a sample variance of
	<me> s^2 \approx \frac{51}{50} \cdot 48.51 = 49.48</me>
	and a sample standard deviation of
	<me>s \approx \sqrt{49.48} \approx 7.03</me> million residents.
	</p>
	</solution>
</p>
</exercise>

</section>




<section><title>Adjusting Statistical Measures for Grouped Data</title>
<p>As you considered the measures of the center and spread before, each data point was considered individually. Often, data may however be grouped into categories and perhaps expressed as a frequency distribution. In this case, rather than considering <m>x_k</m> to be the kth data value can take advantage of the grouping to perhaps save a bit on arithmetic.</p>
<p>Indeed, let's assume that data is grouped into m categories <m>x_1, x_2, ..., x_m</m> with corresponding frequencies <m>f_1, f_2, ..., f_m</m>. Then, for example, when computing the mean rather than adding <m>x_1</m> with itself <m>f_1</m> times just compute <m>x_1 \times f_1</m> for the first category and continuing through the remaining categories. This gives the following grouped data formula for the mean
		<me>
		\mu = \frac{x_1 f_1 + ... + x_m f_m}{f_1 + ... + f_m} = \frac{\sum_{k=1}^m x_k f_k}{\sum_{k=1}^m f_k}.
		</me>
and the following grouped data formula for the variance
		<me> 
		\sigma^2 = \frac{\sum_{k=1}^m ( x_k-\mu )^2 f_k}{\sum_{k=1}^m f_k} = \frac{\sum_{k=1}^m x_k^2 f_k}{\sum_{k=1}^m f_k} - \mu^2
		</me>

</p>

<exercise>
<p>
Consider the following data set
</p>
<p>{3,
1,
2,
2,
3,
1,
3,
4,
5,
5,
1,
4,
5,
1,
2,
4,
5,
3,
2,
5,
2,
1,
2,
2,
5}</p>
<p>
Create a frequency distribution and determine the sample mean and variance.
	<solution>
	<p>
	Collecting this data into a frequency distribution gives
	<table halign="left">
		  <tabular halign="right">
	  
			<row><cell bottom="medium">x</cell><cell bottom="medium">frequency</cell></row>      
			<row><cell>1</cell><cell>5</cell></row>
			<row><cell>2</cell><cell>7</cell></row>
			<row><cell>3</cell><cell>4</cell></row>
			<row><cell>4</cell><cell>3</cell></row>
			<row><cell>5</cell><cell>6</cell></row>
		</tabular>
	</table>
	Therefore, 
		<me>
		\overline{x} = \frac{1 \times 5 + 2 \times 7 + 3 \times 4 + 4 \times 3 + 5 \times 6}{5+7+4+3+6} = \frac{5 + 14 + 12 + 12 + 30}{25} = \frac{43}{25}
		</me>
	and 
		<md>
			<mrow>v &amp; = \frac{1^2 \times 5 + 2^2 \times 7 + 3^2 \times 4 + 4^2 \times 3 + 5^2 \times 6}{5+7+4+3+6} - \left ( \frac{43}{25} \right )^2 </mrow>
			<mrow> &amp; = \frac{5 + 28 + 36 + 48 + 150}{25} - \left ( \frac{43}{25} \right )^2 </mrow>
			<mrow> &amp; = \frac{4826}{625}</mrow>
			<mrow> &amp; \approx 7.7216</mrow>
		</md>
	and so <m>s^2 = \frac{25}{24} \frac{4826}{625} \approx 8.043</m>. 
	</p>
	</solution>
</p>
</exercise>

</section>



<section><title>Other Statistical Point Measures</title>

<p>Beyond measures of the middle and of spread includes a way you can determine if data is heaped up to one side or the other of the mean. One such measure is the skewness.
</p>

<definition><title>Skewness</title>
<p>For sample data, the Skewness of <m>x_1, x_2, ..., x_n</m> is given by
<me> \frac{1}{s^3} \frac{\sum_{k=1}^n ( x_k-\overline{x} )^3}{n}</me>
and similarly for population data but using <m>\mu, \sigma</m>.</p>
</definition>

<p>
A positive skewness indicates that the positive <m>(x_k - \overline{x})^3</m> terms overwhelm the negative terms. Therefore, this indicates data which is strung out to the right. Likewise, a negative skewness indicates data which is strung out to the left.
</p>

<p>
In addition to skewness, data might tend to be clustered around the mean and often in a "bell-shaped" manner. The kurtosis can be used o measure how closely data resembles a bell-shaped collection.
</p>

<definition><title>Kurtosis</title>
<p>The Kurtosis of <m>x_1, x_2, ..., x_n</m> is given by
<me> \frac{1}{s^4} \frac{\sum_{k=1}^n ( x_k-\overline{x} )^4}{n}</me>
and similarly for population data again using <m>\mu, \sigma</m>.
</p>
</definition>

<p>
A kurtosis of 3 indicates that the data is perfectly bell shaped (a "normal" distribution) whereas data further away from 3 indicates data that is less bell shaped.
</p>

<theorem><title>Alternate Formulas for Skewness and Kurtosis</title>
<statement>
Skewness = 
<me>\frac{1}{s^3} 
\left [ \frac{\sum_{k=1}^n x_k^3}{n} - 3 v \overline{x} - \overline{x}^3 \right ]</me>
and Kurtosis =
<me>\frac{1}{s^4} 
\left [ \frac{\sum_{k=1}^n x_k^4}{n} - 4 \overline{x} \frac{\sum_{k=1}^n x_k^3 }{n} + 6 \overline{x}^2 v - 3 \overline{x}^4  \right ]</me>
<proof>
<p>
For skewness, expand the cubic and break up the sum. Factoring out constants (such as <m>\overline{x}</m>) gives
<md>
	<mrow>&amp; \frac{\sum_{k=1}^n ( x_k-\overline{x} )^3}{n}</mrow>
	<mrow>&amp; = \frac{\sum_{k=1}^n x_k^3}{n} - 3 \overline{x} \frac{\sum_{k=1}^n x_k^2 }{n} + 3 \overline{x}^2 \frac{\sum_{k=1}^n x_k}{n} - \frac{\sum_{k=1}^n \overline{x}^3}{n}</mrow>
	<mrow>&amp; = \frac{\sum_{k=1}^n x_k^3}{n} - 3 \overline{x}(v + \overline{x}^2) + 3 \overline{x}^3 - \overline{x}^3</mrow>
	<mrow>&amp; = \frac{\sum_{k=1}^n x_k^3}{n} - 3 \overline{x}v - \overline{x}^3</mrow>
</md>
and divide by the cube of the standard deviation to finish. Note that the first expansion in the derivation above can be used quickly if the data is collected in a table and powers easily computed.
</p>
<p>
For kurtosis, similarly expand the quartic and break up the sum as before. Note that you can extract the value of the cubic term by solving for that term in the skewness formula above. Then,
<md>
	<mrow>&amp; \frac{\sum_{k=1}^n ( x_k-\overline{x} )^4}{n}</mrow>
	<mrow>&amp; = \frac{\sum_{k=1}^n x_k^4}{n} - 4 \overline{x} \frac{\sum_{k=1}^n x_k^3 }{n} + 6 \overline{x}^2 \frac{\sum_{k=1}^n x_k^2}{n} - 4 \overline{x}^3 \frac{\sum_{k=1}^n x_k}{n} + \frac{\sum_{k=1}^n \overline{x}^4}{n}</mrow>
	<mrow>&amp; = \frac{\sum_{k=1}^n x_k^4}{n} - 4 \overline{x} \frac{\sum_{k=1}^n x_k^3 }{n} + 6 \overline{x}^2 (v+\overline{x}^2) - 4 \overline{x}^4 + \overline{x}^4</mrow>
	<mrow>&amp; = \frac{\sum_{k=1}^n x_k^4}{n} - 4 \overline{x} \frac{\sum_{k=1}^n x_k^3 }{n} + 6 \overline{x}^2 v - 3 \overline{x}^4 </mrow>
</md>
and then divide by the fourth power of the standard deviation.  Note again that the first expansion in the derivation above might also be a useful shortcut.
</p>
</proof>
</statement>
</theorem>

</section>



<section><title>Visual Statistical Measures - Graphical Representation of Data</title>
<p>Data sets can range from small to very large. Visual representations of these data sets often allow you to see trends and reveal a lot about the distribution of the data values.</p>

<p>
Also, probability mass functions for discrete variables can be graphed as a set of points but sometimes these points do not convey size very well. A visual representation of these functions needs to be addressed.
</p>
	<subsection><title>Histograms</title>
		<p>Frequency Histograms - height matters</p>
		<p>Consider the data set given by
		
<table halign="left">
      <tabular halign="right">
      
<row><cell bottom="medium">k</cell><cell bottom="medium"><m>x_k</m></cell></row>      
<row><cell>1</cell><cell>8</cell></row>		
<row><cell>2</cell><cell>12</cell></row>		
<row><cell>3</cell><cell>6</cell></row>		
<row><cell>4</cell><cell>3</cell></row>		
<row><cell>5</cell><cell>1</cell></row>		
<row><cell>6</cell><cell>2</cell></row>		

      
      </tabular>
</table>
	
A frequency histogram representing this data can be given by

<image source="images/frequencyhistogram.png" />
		
<!--
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{pgfplots}

\title{Histogram}
\author{travis }
\date{August 2016}

\begin{document}

\pgfplotsset{
  compat=newest,
  xlabel near ticks,
  ylabel near ticks
}

\begin{document}
  \begin{tikzpicture}[font=\small]
    \begin{axis}[
      ybar,
      bar width=20pt,
      xlabel={X},
      ylabel={Frequency},
      ymin=0,
      ytick=\empty,
      xtick=data,
      axis x line=bottom,
      axis y line=left,
      enlarge x limits=0.2,
      symbolic x coords={1,2,3,4,5,6},
      xticklabel style={anchor=base,yshift=-\baselineskip},
      nodes near coords={\pgfmathprintnumber\pgfplotspointmeta\%}
    ]
      \addplot[fill=white] coordinates {
        (1,8)
        (2,12)
        (3,6)
        (4,3)
        (5,1)
        (6,2)
      };
    \end{axis}
  \end{tikzpicture}
\end{document}
-->		
</p>

<p>Experiment with creating your own histogram by inputting your data into the interactive cell below.
<sage>
<input>
#  This function is used to convert an input string into separate entries
def g(s): return str(s).replace(',',' ').replace('(',' ').replace(')',' ').split()

@interact
def _(freq = input_box("1,1,1,1,2,2,2,3,3,3,3,1,5",label="Enter data separated by commas")):
    freq = g(freq)
    freq = [int(k) for k in freq]
    m = min(freq)
    M = max(freq)
    bn = M-m+1
    histogram( freq, range=[m-1/2,M+1/2], bins = bn, align="mid", linewidth=2, edgecolor="blue", color="yellow").show()
</input>
</sage>

		</p>
		<p>Relative Frequency Histograms - In this case, area describes your data.  Notice in the interactive cell above that each bar is of width one. Therefore, frequency = area. In some instances where data may be grouped the total width of the interval may be different and so the height will need to be adjusted so that the total area of each bar corresponds to the relative frequency of that category.</p>
		<p>Cummulative Histograms.  In these a running total is presented using all values from the given point and below.
<sage>
<input>
#  This function is used to convert an input string into separate entries
def g(s): return str(s).replace(',',' ').replace('(',' ').replace(')',' ').split()

@interact
def _(freq = input_box("1,1,1,1,2,2,2,3,3,3,3,1,5",label="Enter data separated by commas")):
    freq = g(freq)
    freq = [int(k) for k in freq]
    top = len(freq)
    m = min(freq)
    M = max(freq)
    bn = M-m+1
    histogram( freq, range=[m-1/2,M+1/2], cumulative = "true", bins = bn, align="mid", linewidth=2, edgecolor="blue", color="yellow").show(ymax=top)
</input>
</sage>		
		</p>
		<p>Stem-and-Leaf Plot - Histogram with data. Using the state population data above, consider organizing the data but using a "two-pass sort" where you first roughly break data up into groups based upon ranges which relate to their first digit(s). In this case, let's break up into groups according to populations corresponding to 0-4 million, 5-9 million, 10-14 million, 15-19, million, 20-24 million, 25-29 million, 30-35 million, and 35-39 million. We can represent these classes by using the stems 0L, 0H, 1L, 1H, 2L, 2H, 3L, and 3H where the L and H represent the one's digits L in {0, 1, 2, 3, 4} and H in {5, 6, 7, 8, 9}.  Once we group the data into these smaller groups then we can write the remaining portion of the number horizontally as leaves (in this case with one decimal place for all values.) This gives a step-and-leaf plot. If we additionally sort the data in the leaves then this gives you an ordered stem-and-leaf plot. For the state population data, the ordered stem-and-leaf plot is given by
		
	
<image source="images/stemandleaf.png" />

Notice how it is easy to now see that most state populations are relatively small and that there are relatively few states with larger population. Also, notice that you can use this plot to relatively easily identify minimum, maximum, and other order statistics.		
		
		</p>
		<p>Box and Whisker Diagram - visual order statistics. This graphical display identifies the "5-number-summary" associated with the minimum, quartiles, and the maximum. That is, <m>y_1, Q_1, Q_2, Q_3, y_n</m>.  These values separate the data roughly into quarters. To distinguish these quarters connect <m>y_1</m> and <m>Q_1</m> with a straight line (a whisker) and do the same with <m>Q_3</m> and <m>y_n</m>. Use a box to connect <m>Q_1</m> with <m>Q_2</m> and the same to connect <m>Q_2</m> with <m>Q_3</m>. Then the boxed areas also identify the IQR.    
<sage>
<input>
from pylab import boxplot,savefig,close
@interact
def _(data = input_box([1,2,3,4,6,7,8,9,11,15,21],label="Enter Your Data:")):
    B = boxplot(data, notch=True, sym='x', vert=False)
    savefig("boxplot.png")
    close()
</input>
</sage>
		</p>
		
	</subsection>
</section>




<section><title>Exercises</title>
<p>Complete the online homework "Computational Measures".</p>

<exercise>
<p>Create a data set with about 10 elements. For your data set, compute each of the measures from this chapter and present your data using a frequency histogram.</p>
</exercise>

<exercise>
<p>Find a "real-world" data set (similar perhaps to the Census data presented above.) Compute each of the measures from this chapter. Interpret and present your conclusions in an electronic report which can include an excel spreadsheet.</p>
</exercise>

</section>

</chapter>