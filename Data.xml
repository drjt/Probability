<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="RepresentingData" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Statistical Measures</title>


<section><title>Introduction</title>
<p>
To compute your final grade in a class your teacher will likely consider the scores you have earned on various assignments and examinations completed during the duration of the course. However, she ultimately will likely be required to assign some numerical score indicating your level of success in the course. One grade to rule them all. This final grade can only be one value and it would make sense that the grade be a reflection of your work on these tasks. So, what is a fair way for your teacher to complete this task? 
</p>

<p>
Through this process, you will also often need to take into account whether that data set is the entire list of possibilities--known as the population--or just a subset of that population perhaps obtained by taking repeated measurements --that is, a sample. 
</p>

<p>
In general, it is often useful to make decisions using quantitative data but making those decisions can be somewhat arbitrary without a mathematical basis supporting those decisions. In this chapter, you will consider a number of ways to use point values to represent a given set of data. Each of these quantitative metrics will be called a "statistical measure" and will, in some fashion, describe using one number some property of the entire data set. Such measures are part of what is known as "descriptive statistics".  Later, you will learn about how other metrics can be used to predict properties of the underlying situation. Doing this is part of what is known as "inferential statistics".
</p>

<p>
So, let's go and hopefully you will in some measure enjoy the ride!
</p>
</section>



<section><title>Measurement Scales</title>
<p>
In creating statistical measures, you might want to consider one of the following general types.
</p>
<p>
	<ul>
		<li><p>Nominal measures - In this case, data falls into mutually exclusive and exhaustive categories for which the numerical value is only used for identification purposes. For example, assigning Male = 1, Female = -1.</p></li>
		<li><p>Ordinal measures - In this case, data consists of discrete numerical values which can be ranked from lowest to highest or vice versa. For example, your grades in a number of classes are used to compute your GPA--which is a single number.</p></li>
		<li><p>Interval measures - In this case, data possesses an order and where the distance between data values is of significance. For example, heights and weights.</p></li>
		<li><p>Ratio measures - In this case, data can be expressed as a position in some interval and where ratios between observations have meaning. For example, percentile rankings</p></li>
	</ul>
</p>

<p>In the subsequent sections of this chapter, you will see that a number of different measures are available for most data sets. Determining which "correct" measure to use for describing any given data set will depend the actual situation surrounding the collection of the data.
</p>
	

</section>




<section><title>Statistical Measures of Position</title>
	<p>Given a collection of data, sorting the data may provide several useful descriptors. When sorting data, you can easily use something like a spreadsheet for larger data sets but in this section you will also see there are ways to perform a sort by hand. In either case, statistical measures of position generally involve very little computational work once the data is sorted and take into account only the order of the data from lowest to highest.  To assist with notation, we will generally use x-values to represent the original raw data and y-values to represent that same data when ordered with the subscript indicating the positional placement.
	</p>
	
<p>	
<definition>
	<title>Order Statistic</title>
	<statement>
	<p xml:id="OrderStatistics">From the data set <m>x_1, x_2, ... , x_n</m>, assume that when sorted it is denoted <m>y_1, y_2, ..., y_n</m> where  
	<me> y_1 \le y_2 \le ... \le y_n.</me> 
	Then, <m>y_k</m> is known as the kth order statistic. </p>
	</statement>
</definition>
</p>

<p>	
<example xml:id="PresidentAge"><title>Age of Presidents - order statistics</title>
	<p>
	The age at inauguration for presidents from 1981-2019 gives the data <m>x_1 = 69, x_2 = 64, x_3 = 46, x_4 = 54, x_5 = 47, x_6 = 70</m> (Reagan, Bush, Clinton, Bush, Obama, Trump). For this data, the order statistics are denoted <m>y_1 = 46, y_2 = 47, y_3 = 54, y_4 = 64, y_5 = 69, y_6 = 71</m>.
	</p>
</example>
</p>
	
	<p>
	Once the data is sorted, it should be very easy for you to locate the smallest and largest values. 
	</p>

<p>
<definition><title>Minimum/Maximum:</title>  
	<statement>
	<p xml:id="MaxMin">For a given data set, the smallest and largest values are known as the minimum and maximum, respectively. In our notation, minimum = <m>y_1</m> and the maximum = <m>y_n</m></p>
	</statement>
</definition>
</p>

<p>	
<example><title>Age of Presidents - Minimum/Maximum</title>
	<statement>
	<p>
	Using the <xref ref="PresidentAge"> President inauguration data</xref>, minimum = <m>y_1 = 46</m> and maximum = <m>y_6 = 70</m>.
	</p>
	</statement>
</example>
</p>

<p>
A value that separates ordered data into two groups with a desired percentage on each side is called a percentile. There are multiple ways that have been created that achieve this goal. In this text we present two and will consistently use the first one presented below. For each, in general, a given percentile is a numerical value at which approximately a given percentage of the data is smaller. 
</p>
<p>
The definition presented below provides for a unique measure for each unique value of p that corresponds to the PERCENTILE.EXC macro in Excel.  This version starts by computing <m>(n+1)p</m> where 
<m>0 &lt; p &lt; 1</m> and using this to linearly interpolate between two adjacent entries in the sorted list.  Another option that corresponds to PERCENTILE.INC (and PERCENTILE) in Excel is to start with <m>(n-1)p+1</m> for determining how to pick the two adjacent entries and then proceeding with linear interpolation. Again, the definition below utilizes the first approach.
</p>

<p>	
<definition xml:id="PercentileDefn"><title>Percentiles</title>
	<statement>
	<p xml:id="Percentiles">For <m>0 \lt s \lt 1</m> and for order statistics <m>y_1, y_2, ..., y_n</m> define the 100s-th percentile to be 
		<me>P^{s} = (1-r)y_m + ry_{m+1}</me>
	where m is the integer part of (n+1)s, namely 
		<me>m = \left\lfloor (n+1)s \right\rfloor</me> 
	and 
		<me>r = (n+1)s - m,</me>
	
	the fractional part of (n+1)s. 
	</p>
	</statement>
</definition> 
</p>

<p><title>Computing Percentiles</title>
<exercise>
<introduction>
		<p>
		Compute the following percentile values.
		</p>
		</introduction>
		<webwork source="Library/UVA-Stat/setStat212-Homework02/stat212-HW02-18.pg">
		</webwork>
		<conclusion>
		<p>
		
		</p>
		</conclusion>
</exercise>
</p>

<p>
<example><title>Presidential Percentile</title>
<p>To compute, say, the 42nd percentile for the 
<xref ref="PresidentAge"> President inauguration data presented earlier </xref> consider s = 0.42. Since there are 6 numbers in our data set, then

<me>(n+1)s = 7 \cdot 0.42 = 2.94</me>

and so m = 2 and r = 0.94.  Thus, the percentile will lie between <m>y_2 = 47</m> and <m>y_3 = 54</m> and much closer to 54 than 47.  Numerically
<me>P^{0.42} = 0.06 \cdot 47 + 0.94 \cdot 54 = 53.58.</me> 
</p>
</example>
</p>
	
<p>
The formula for percentiles determines a weighted average between <m>y_m</m> and <m>y_{m+1}</m> which is unique for distinct values of p provided each of the data values are distinct. Note that if some of the y-values are equal then some of these averages might be averages of equal numbers and will therefore be the common value.
</p>

<p>
Some special percentiles are provided special names...
</p>

<p>
<definition><title>Quartiles</title> 
	<statement>
	<p xml:id="Quartiles">Given a sorted data set, the first, second, and third quartiles are the values of 
	<m>Q_1 = P^{0.25}, Q_2 = P^{0.5}</m> and <m>Q_3 = P^{0.75}</m>.
	</p>
	</statement>
</definition>
</p>

<p>
It should be noted that many graphing calculators often compute quartiles using a straight average of two adjacent entries rather than by using the formula above. This causes some difficulty and especially so when n mod 4 = 2. 
</p>

<p>
<example><title><m>Q_1</m> and <m>Q_3</m> when n mod 4 = 2</title>
<p>
Suppose n = 22 = 5(4) + 2. Computing the first quartile as defined above gives (n+1)p = 23(0.25) = 5.75 = 5 + 0.75 = m + r.  Therefore,
<me>Q_1 = 0.25 \times y_5 + 0.75 \times y_6</me>
which is a value closer to <m>y_6</m>.  Many graphing calculators however quickly approximate this with
<me>0.5 \times y_5 + 0.5 \times y_6</me>
so you should be aware of this possible difference.  You should also notice that in this case p = 0.25 but r = 0.75 so these values are not required to be the same. 
</p>
</example>
</p>

<p>
<definition><title>Deciles:</title>
	<statement>
	<p xml:id="Deciles">Given a sorted data set, the first, second, ..., ninth deciles are the value of 
	<m>D_1 = P^{0.1}, D_2 = P^{0.2}, ... , D_9 = P^{0.9}</m>
	</p>
	</statement>
</definition>
</p>
	
<p>	
<example><title>Small Example - Quartiles</title>
	<statement>
	<p>
Consider the following data set: {2,5,8,10}. The 50th percentile should be a numerical value for which approximately 50% of the data is smaller. In this case, that would be some number between 5 and 8.  For now, let's just take 6.5 so that two numbers in the set lie below 6.5 and two lie above. This is a perfect 50% for the 50th percentile. In a similar manner, the 25th percentile would be some number between 2 and 5, say 2.75, so that one number lies below 2.75 and three numbers lie above.
	</p>

	<p>Using the <xref ref="PercentileDefn"> definition </xref>, the 25th percentile is computed by considering 
	<me>(n+1)p = (4+1)0.25 = 5/4 = 1.25</me>.  
	So, m = 1 and r = 0.25. Therefore 
	<me>P^{0.25} = 0.75 \times 2 + 0.25 \times 5 = 2.75</me> 
	as noted above. 
	</p>
	<p>
	Similarly, the 75th percentile is given by
	<me>(n+1)p = (4+1)0.75 = 15/4 = 3.75</me>.  
	So, m = 3 and r = 0.75. Therefore 
	<me>P^{0.75} = 0.25 \times 8 + 0.75 \times 10 = 9.5</me> 
	
	It is interesting to note that 3 also lies between 2 and 5 as does 2.75 and has the same percentages above (75 percent) and below (25 percent). However, it should designate a slightly larger percentile location. Indeed, going backward:
	<md>
		<mrow>3 = (1-r) \times 2 + r \times 5</mrow>
		<mrow>\Rightarrow r = \frac{1}{3}</mrow>
		<mrow>\Rightarrow (n+1)p = 1 + \frac{1}{3} = \frac{4}{3}</mrow>
		<mrow>\Rightarrow p = \frac{4}{15} \approx 0.267</mrow>
	</md>
	and so 3 would actually be at approximately the 26.7th percentile.
	</p>
	</statement>
</example>
</p>

<p>
<exercise>
<p>
In general, given a numerical value within the range of a given data set, one can determine the percentile ranking of that value by reversing the general formula for percentile and solving for p, given <m>P^s</m>.  Determine such a formula/process for doing this in general.
</p>
</exercise>
</p>
	
<p>
	For your data set {2,5,8,10}, 
	<m>Q_1 = 2.75, Q_2 = 6.5,</m> and <m>Q_3 = 9.5</m>.
</p>

<p>
	For a given data set, a summary of these statistics is often desired in order to give the user a quick overview of the more important order statistics.
</p>

<p>
<definition><title>5-number summary</title>
	<statement>
	<p xml:id="FiveNumberSummary">Given a set of data, the 5-number summary is a vector of the order statistics given by 
	
	<me>\lt \text{minimum}, Q_1, Q_2, Q_3, \text{maximum} \gt .</me> 
	</p>
	</statement>
</definition>
</p>

<p>
You can also compute these statistics automatically using the opensource statistical software known simply as "R".  The following interactive cell uses the opensource software "Sage" to perform this calculation using the freely available web portal at sagemath.sagecell.org. You can change the data list if you want to use this to compute values for a different collections of numbers. The five-number-summary is displayed graphically using a "Box-Plot". Graphical representations of data will be discussed later in this chapter. You should compare the answers found using R with the values produced by our <xref ref="PercentileDefn"> definition </xref>
</p>

<p>
<sage language = 'r'>
<input>
data &lt;- c( 1, 2, 5, 7, 7, -1, 3, 2)   # concatenate the following items into a list
paste("Quartiles:")
quantile(data)
paste("Specific Percentiles:")
quantile(data, c(.32, .57, .98))   # find the 32nd, 57th and 98th percentiles
paste("Box and Whisker Diagram:")
boxplot(data, horizontal=TRUE) 
</input>
</sage>
</p>

<p>	
<example><title>Small example - 5 number summary</title>
	<statement>
	<p>Returning to our previous example, the five number summary would be
	<me>\lt 2, 2.75, 6.5, 9.5, 10 \gt .</me>
	</p>
	</statement>
</example>
</p>

<p>
Of course, the data sets you can utilize by manually entering data will be relatively small and tedious to deal with. The open source statistical software R however has a number of built in data sets with many of them including a relatively large number of values. We will be utilizing some of those data sets throughout the remainder of this text. To see what data sets are actually available, execute the command data(). You can load the data from one of these sets by executing the data command with the desired data set name inside the parenthesis and then print out the first few data values with headers using the head command. Execute the interactive cell below and play around with some of the data sets. Remove the hash tag symbol to 'uncomment' those lines as needed.
</p>

<p>
<sage language = 'r'>
<input>
data()
# data(faithful)
# head(faithful,10)
# nrow(faithful)  # returns the number of observations (rows)
# ncol(faithful)  # returns the number of variables per observation (columns)
# print(? faithful)            # returns a more exhaustive description of the data 

</input>
</sage>
</p>

<p>
Now that you can access large ready-to-go data sets, you will want to evaluate them in various ways. The following interactive cell helps you calculate the measures presented in this section as well as a few measures we will investigate in the next section. Note that an R cell only routinely outputs the final command. If you want to output any of the intermediate values, use the print() command as shown below. Also, to make things easier to enter, let's just go ahead and give the formal data set name a smaller local identifier x.
</p>

<p>
<sage language = 'r'>
<input>
data(faithful)
x &lt;- faithful
x1 &lt;- x[,1]
x2 &lt;- x[,2]
m &lt;- min(faithful, 2)
M &lt;- max(faithful, 2)
print(paste("Minimum of the second outcome = ",m))
print(paste("Maximum of the second outcome = ",M))
cat("\n\n")   #  a couple of blank lines in the displayed output
quantile(x[,1], c(.25,.29,.57))
cat("\n\n")   #  a couple of blank lines in the displayed output
# mu1 &lt;- mean(x1)
# mu2 &lt;- mean(x2)
# med1 &lt;- median(x1)
# med2 &lt;- median(x2)
# print(paste("Mean of the first outcome = ",mu1))
# print(paste("Mean of the second outcome = ",mu2))
# print(paste("Median of the first outcome = ",med1))
# print(paste("Median of the second outcome = ",med2))
</input>
</sage>
</p>

</section>




<section>
<title>Statistical Measures of the Middle</title>

	<definition><title>Arithmetic Mean</title>
	<statement>
	<p xml:id="Mean">Suppose X is a discrete random variable with range 
	<m>R = {x_1, x_2, ..., x_n}</m>. 
	The arithmetic mean is given by
		<me>
		\frac{x_1 + ... + x_n}{n} = \frac{\sum_{k=1}^n x_k}{n}.
		</me>
	If this data comes from sample data then we call it a sample mean and denote this value by <m>\overline{x}</m>. If this data comes from the entire universe of possibilities then we call it a population mean and denote this value by <m>\mu</m>.  When presented with raw data, it might be good to generally presume that data comes from a sample and utilize <m>\overline{x}</m>.
	</p>
	</statement>
	</definition>
	
	<p>
	To illustrate, consider the previous data set: {2,5,8,10}. The arithmetic mean is given by
	<me>\overline{x} = \frac{2+5+8+10}{4} = \frac{25}{4} = 6.25.</me>
	</p>
	
	<p>
	The mean is often called the centroid in the sense that if the x values were locations of objects of equal weight, then the centroid
	would be the point where this system of n equal masses would balance. Play around with the interactive cell below by entering your own data values into the first list.
	</p>
	
<p>
<sage>
<input>
x = [2, 5, 8, 10, 11]     # Put your data values in this list
x.sort()

mu = mean(x)
n = len(x)
pts = [(x[0],0.05)]
M = 0.2
for k in range(1,n):
    if x[k]==x[k-1]:
        pts.append((x[k],pts[k-1][1]+0.1))
        M += 0.1
    else:
        pts.append((x[k],0.05))
G = points(pts,size=100,figsize=[10,2])
G += polygon([(mu,0), (mu+0.2,-0.5), (mu-0.2,-0.5)],color='brown')
G.show(ymin=-0.5, ymax = M)
</input>
</sage>
</p>
	
	<p>
	The values can all be provided with varying weights if desired and the result is called the weighted arithmetic mean and is given by
		<me>
		\frac{m_1 x_1 + ... + m_n x_n}{m_1 + ... + m_n} = \frac{\sum_{k=1}^n m_k x_k}{\sum_{k=1}^n m_k}.
		</me>
	This is often how your teacher will actually compute your final grade in a class where the <m>m_k</m> are the relative weights for each assignment grade.
	</p>

<p>
<sage>
<input>
x = [2, 5, 8, 10]     # Put _unique_ data values in this list
w = [1, 2.5, 2.5, 4]  # Scale to be at most 10 and not tiny for good image
wsum = sum(w)

n = len(x)
pts = [(x[0],0.05)]
M = 0.2
mu = 0
for k in range(1,n):
    mu += x[k]*w[k]
    if x[k]==x[k-1]:
        pts.append((x[k],pts[k-1][1]+0.1))
        M += 0.2
    else:
        pts.append((x[k],0.05))
mu = mu/wsum
G = Graphics()
for k in range(n):
    G += point(pts[k],size=100*w[k]) 
P = polygon([(mu,0), (mu+0.2,-0.5), (mu-0.2,-0.5)],color='brown')
(G+P).show(ymin=-0.5, ymax = M)
</input>
</sage>
</p>


<p>
<example><title>Computing class final grade</title>
<p>
Suppose in a given class you have a daily grade of 92, exam 1 grade of 85, exam 2 grade of 87, and a final exam grade of 93.  IF the daily grade counts 10 percent, the first two exams count 25 percent each and the final counts 40 percent then your final grade would be
<me>\frac{0.10 \cdot 92 + 0.25 \cdot 85 + 0.25 \cdot 87 + 0.40 \cdot 0.93}{0.10 + 0.25 + 0.25 + 0.40} = 89.4 .</me>
It would then appear that you might want to do some bargaining with your teacher about how nice it would be to round that up.
</p>
</example>
</p>

<p>
<definition><title>Median:</title>
<statement>
<p xml:id="Median">A positional measure of the middle is often utilized by finding the location of the 50th percentile. This value is also called the median and indicates the value at which approximately half the sorted data lies below and half lies above.
</p>
</statement>
</definition>
</p>

<p>
For data sets with an odd number of values, this is the "middle" data value if one were to successively cross off pairs from the two ends of the sorted data. For data sets with an even number of values, this is a average of the two data values left after crossing off all other pairs.  Using the order statistics, the median equals
	<me>y_{\frac{n+1}{2}}</me>
if n is odd and
	<me>\frac{y_\frac{n}{2} + y_{\frac{n}{2}+1}}{2}</me>
if n is even.
</p>

<p>
From the <xref ref="PresidentAge"> Presidential data </xref>, note that you are considering an even number of data values and so the median is given by (54+64)/2 = 59. 
</p>

<p>
<definition><title>Midrange:</title>
<statement>
<p xml:id="Midrange">The midrange is a mixture of the mean and median where one takes the simple average of the maximum and minimum values in the data set. Using the order statistics, this equals 
	<me>\frac{y_1+y_n}{2}</me>
</p>
</statement>
</definition>
</p>

<p>
From the <xref ref="PresidentAge"> Presidential data </xref>, the maximum is 70 and the minimum is 46 so the midrange is 58, the average of these two. 
</p>

<p>
There are several advantages and disadvantages associated with each of these measures. The mean utilizes all of the data values so each term is important. Utilizes them all even if some of the data values might suffer from collection errors. The median ignores outliers (which might be a result of collection errors) but does not account for the relative differences between terms. The midrange is very easy to compute but ignores the relative differences for all terms but the two extremes. A similar collection of features and drawbacks are associated with all descriptive statistics. 
</p>

<p>
You can again compute many statistics automatically using R...
<sage language = 'r'>
<input>
data &lt;- c( 1, 2, 5, 7, 7, -1, 3, 2)   # concatenate the following items into a list
paste("Mean = ", mean(data))
paste("Median =", median(data))
</input>
</sage>
</p>

<p>
<example xml:id="StatePopulations"><title>USA State Population Measures of the Middle</title>
<statement>
<p>The US Census Bureau reported the following state populations (in millions) for 2013:
<url href="Data/USA_States_Populations_2014.xlsx">Spreadsheet</url>
</p>
<p>
<table halign="left">
    <caption>USA State Populations - 2014</caption>
      <tabular halign="right">
      
<row><cell bottom="medium">State</cell><cell bottom="medium">Population</cell></row>      
<row><cell>Wyoming</cell><cell>0.6</cell></row>
<row><cell>Vermont</cell><cell>0.6</cell></row>
<row><cell>District of Columbia</cell><cell>0.6</cell></row>
<row><cell>North Dakota</cell><cell>0.7</cell></row>
<row><cell>Alaska</cell><cell>0.7</cell></row>
<row><cell>South Dakota</cell><cell>0.8</cell></row>
<row><cell>Delaware</cell><cell>0.9</cell></row>
<row><cell>Montana</cell><cell>1</cell></row>
<row><cell>Rhode Island</cell><cell>1.1</cell></row>
<row><cell>New Hampshire</cell><cell>1.3</cell></row>
<row><cell>Maine</cell><cell>1.3</cell></row>
<row><cell>Hawaii</cell><cell>1.4</cell></row>
<row><cell>Idaho</cell><cell>1.6</cell></row>
<row><cell>West Virginia</cell><cell>1.9</cell></row>
<row><cell>Nebraska</cell><cell>1.9</cell></row>
<row><cell>New Mexico</cell><cell>2.1</cell></row>
<row><cell>Nevada</cell><cell>2.8</cell></row>
<row><cell>Kansas</cell><cell>2.9</cell></row>
<row><cell>Utah</cell><cell>2.9</cell></row>
<row><cell>Arkansas</cell><cell>3</cell></row>
<row><cell>Mississippi</cell><cell>3</cell></row>
<row><cell>Iowa</cell><cell>3.1</cell></row>
<row><cell>Connecticut</cell><cell>3.6</cell></row>
<row><cell>Oklahoma</cell><cell>3.9</cell></row>
<row><cell>Oregon</cell><cell>3.9</cell></row>
<row><cell>Kentucky</cell><cell>4.4</cell></row>
<row><cell>Louisiana</cell><cell>4.6</cell></row>
<row><cell>South Carolina</cell><cell>4.8</cell></row>
<row><cell>Alabama</cell><cell>4.8</cell></row>
<row><cell>Colorado</cell><cell>5.3</cell></row>
<row><cell>Minnesota</cell><cell>5.4</cell></row>
<row><cell>Wisconsin</cell><cell>5.7</cell></row>
<row><cell>Maryland</cell><cell>5.9</cell></row>
<row><cell>Missouri</cell><cell>6</cell></row>
<row><cell>Tennessee</cell><cell>6.5</cell></row>
<row><cell>Indiana</cell><cell>6.6</cell></row>
<row><cell>Arizona</cell><cell>6.6</cell></row>
<row><cell>Massachusetts</cell><cell>6.7</cell></row>
<row><cell>Washington</cell><cell>7</cell></row>
<row><cell>Virginia</cell><cell>8.3</cell></row>
<row><cell>New Jersey</cell><cell>8.9</cell></row>
<row><cell>North Carolina</cell><cell>9.8</cell></row>
<row><cell>Michigan</cell><cell>9.9</cell></row>
<row><cell>Georgia</cell><cell>10</cell></row>
<row><cell>Ohio</cell><cell>11.6</cell></row>
<row><cell>Pennsylvania</cell><cell>12.8</cell></row>
<row><cell>Illinois</cell><cell>12.9</cell></row>
<row><cell>Florida</cell><cell>19.6</cell></row>
<row><cell>New York</cell><cell>19.7</cell></row>
<row><cell>Texas</cell><cell>26.4</cell></row>
<row><cell>California</cell><cell>38.3</cell></row>      
      
      </tabular>
      </table>

Determine the minimum, maximim, midrange, and mean for this data.
</p>
</statement>	

<solution>
		<p>Notice that these are already in order so you can presume 
		<m>y_1 = 0.6</m> million is the minimum and <m>y_{50} = 38.3</m> 
		million is the maximum. Therefore, the midrange is given by
		<me>\frac{0.6+38.3}{2} = \frac{38.9}{2} = 19.45  \text{million}.</me>

		In this collection of "states" data the District of Columbia is included so that the number of data items is n=51. The mean of this data takes a bit of arithmetic but gives
		<me>\overline{x} = \frac{\sum_{k=1}^{51} y_k }{51} = \frac{316.1}{51} \approx 6.20</me>
		million residents. 
		</p>
		<p>
		Since the number of states is odd, the median is found by looking at the 26th order statistic. In this case, that is the 4.4 million residents of Kentucky, i.e. <m>y_{26} = 4.4</m>.
		</p>
</solution>
</example>
</p>

</section>





<section>
<title>Statistical Measures of Variation</title>
<p>These measures provide some indication of how much the data set is "spread out". Indeed, note that the data sets {-2,-1,0,1,2} and {-200,-100,0,100,200} have the same mean but one is much more spread out than the other. Measures of variation should catch this difference.
</p>

<p>
<definition><title>Range:</title>
<statement>
<p xml:id="Range">Using the order statistics, <me>y_n - y_1.</me>  
</p>
</statement>
</definition>
</p>

<p>
It is trivial to note that the range is very easy to compute but it completely gnores all data values but the two ends.
</p>

<p>From the <xref ref="PresidentAge"> Presidential data </xref>, the maximum is 69 and the minimum is 46 so the range is 23, the difference of these two. </p>

<p>
<definition><title>Interquartile Range (IQR):</title>
<statement>
<p xml:id="IQR">
<me>IQR = P^{0.75} - P^{0.25}.</me> 
</p>
</statement>
</definition>
</p>

<p>
For the data set {2, 5, 8, 10}, you have found that <m>Q_1 = 2.75</m> and <m>Q_3 = 9.5</m>. Therefore, <me>IQR = 9.5 - 2.75 = 6.75.</me>
</p>

<p>
Average Deviation from the Mean (Population):  Given a population data set <m>x_1, x_2, ... , x_n</m> with mean <m>\mu</m> each term deviates from the mean by the value <m>x_k - \mu</m>. So, averaging these gives
<me> \frac{\sum_{k=1}^n (x_k-\mu)}{n} = \frac{\sum_{k=1}^n x_k}{n} - \frac{\sum_{k=1}^n \mu}{n} = \mu - \mu = 0.</me>
This metric is therefore always zero for any provided set of data since cancellation makes this not useful. So, we need to determine ways to avoid cancellation.
</p>

<p>Average Absolute Deviation from the Mean (Population):  
<me> \frac{\sum_{k=1}^n \left | x_k-\mu \right |}{n} </me>
which, although nicely stated, is difficult to deal with algebraically since the absolute values do not simplify well algebraically. To avoid this algebraic roadblock, we can look for another way to nearly accomplish the same goal by squaring and then square rooting. 
</p>

<p>Average Squared Deviation from the Mean (Population):
<me> \frac{\sum_{k=1}^n ( x_k-\mu )^2}{n} </me>
which will always be non-negative but can be easily expanded using algebra. Since this is a mouthful, this measure is generally called the "variance". 
</p>

<p>
Using the average squared deviation from the mean, differences have been squared. Thus all of the squared differences added are non-negative but very small ones have been made even smaller and larger ones have been made relatively larger. To undo this scaling issue, one must take a square root to get things back into the right ball park. 
</p>

<p>
<definition><title>Variance and Standard Deviation</title>
<statement>
<p xml:id="Variance">The variance is the average squared deviation from the mean. If this data comes from the entire universe of possibilities then we call it a population variance and denote this value by <m>\sigma^2</m>. Therefore
	<me>\sigma^2 = \frac{\sum_{k=1}^n ( x_k-\mu )^2}{n} </me>
</p>
<p>The standard deviation is the square root of the variance. If this data comes from the entire universe of possibilities then we call it a population standard deviation and denote this value by <m>\sigma</m>. Therefore
<me> \sigma = \sqrt{\frac{\sum_{k=1}^n ( x_k-\mu )^2}{n}}.</me>
</p>
<p>
If data comes from a sample of the population then we call it a sample variance and denote this value by v. Since sample data tends to reflect certain "biases" then we increase this value slightly by <m>\frac{n}{n-1}</m> to give the sample variance
<me>s^2 = \frac{n}{n-1}\frac{\sum_{k=1}^n ( x_k-\overline{x} )^2}{n} = \frac{\sum_{k=1}^n ( x_k-\overline{x} )^2}{n-1}.</me>
and the sample standard deviation similarly as the square root of the sample variance.
</p>
</statement>
</definition>
</p>

<p>From the data {2,5,8,10}, you have found that the mean is 6.25. Computing the variance then involves accumulating and averaging the squared differences of each data value and this mean. Then
<md>
	<mrow>&amp; \frac{1}{4} \left ( (2-6.25)^2 + (5-6.25)^2 + (8-6.25)^2 + (10-6.25)^2 \right ) </mrow>
	<mrow>&amp; = \frac{18.0625 + 1.5625 + 3.0625 + 14.0625}{4} </mrow>
	<mrow>&amp; = \frac{36.75}{4}</mrow>
	<mrow>&amp; = 9.1875.</mrow>
	Therefore, the standard deviation for this data set is given by
	<me>\sqrt{9.1875} \approx 3.031.</me>
</md>
</p>

<p>
<theorem xml:id='AlternateVariance'><title>Alternate Forms for Variance</title>
<statement>
<md>
<mrow>\sigma^2 &amp; = \left ( \frac{\sum_{k=1}^n x_k^2 }{n} \right ) - \mu^2 </mrow>
<mrow>&amp; = \left [ \frac{\sum_{k=1}^n x_k(x_k - 1)}{n} \right ] + \mu - \mu^2</mrow>
</md>
</statement>
<proof>
<p>
	<md>
	  <mrow>\sigma^2 &amp; = \frac{\sum_{k=1}^n ( x_k-\mu )^2}{n}</mrow>
	  <mrow> &amp; = \frac{\sum_{k=1}^n ( x_k^2 - 2x_k \mu + \mu^2 )}{n}</mrow>
	  <mrow> &amp; = \frac{\sum_{k=1}^n x_k^2 - 2\mu \sum_{k=1}^n x_k  + n \mu^2 )}{n}</mrow>
	  <mrow> &amp; = \left ( \frac{\sum_{k=1}^n x_k^2 }{n} \right ) - \mu^2</mrow>
	</md>
	
</p>
<p>
The second part is proved similarly. Using the first part of the proof above,
	<md>
	  <mrow>\sigma^2 &amp; = \frac{\sum_{k=1}^n ( x_k-\mu )^2}{n}</mrow>
	  <mrow> &amp; = \left ( \frac{\sum_{k=1}^n x_k^2 }{n} \right ) - \mu^2</mrow>
	  <mrow> &amp; = \left ( \frac{\sum_{k=1}^n x_k (x_k - 1) + x_k }{n} \right ) - \mu^2</mrow>
	  <mrow> &amp; = \left ( \frac{\sum_{k=1}^n x_k (x_k - 1)}{n} \right ) + \mu - \mu^2</mrow>
	</md>	
</p>
</proof>
</theorem>
</p>

<p>
<example><title>Computing means and variances by hand</title>
<p>
In the data table below, notice that the <m>x_k</m> column would be the given data values but the column for <m>x_k^2</m> you could easily compute.

<table halign="left">
<caption>Sample Grouped Data</caption>
      <tabular halign="right">
      
<row>
<cell bottom="medium"><m>x_k</m></cell>
<cell bottom="medium"><m>x_k^2</m></cell></row>      
<row><cell>1</cell><cell>1</cell></row>		
<row><cell>-1</cell><cell>1</cell></row>		
<row><cell>0</cell><cell>0</cell></row>		
<row><cell>2</cell><cell>4</cell></row>		
<row><cell>2</cell><cell>4</cell></row>		
<row><cell>5</cell><cell>25</cell></row>		

      </tabular>
</table>
So, <m>\Sigma x_k = 9</m> and <m>\Sigma x_k^2 = 35</m>.  Therefore <m>\overline{x} = \frac{9}{6} = \frac{3}{2}</m> and <m>v = \frac{\Sigma x_k^2}{6} - (\overline{x})^2 = ( \frac{35}{6} - \frac{3}{2}^2 = \frac{70-18}{12} = \frac{26}{6}</m>.  Therefore, <m>s^2 = \frac{6}{5} \times v = \frac{26}{5}</m>.
</p>
</example>
</p>

<p>
Use R to compute these values...
<sage language = 'r'>
<input>
data &lt;- c( 1, -1, 0, 2, 2, 5)   # concatenate the following items into a list
paste("Variance = ", var(data))
paste("Standard Dev = ", sd(data))
paste("Inter Quantile Range =",IQR(data))
paste("Box and Whisker Diagram:")
boxplot(data, horizontal=TRUE) 
</input>
</sage>
</p>

<p>Once again, the Population of the individual USA states according to the 2013 Census is considered below.</p>

<p>
<exercise><title>USA State Population Measures of Variation</title>
<p>Using the <xref ref="StatePopulations"> US Census Bureau state populations </xref> (in millions) for 2014 provided earlier, determine the range, quartiles, and variance for this sample data.
</p>
<p>
<solution> 
	<p>Again, you should note that these are already in order so the range is quickly found to be 
	<me>y_n - y_1 = 38.3 - 0.6 = 37.7</me> 
	million residents.
	</p>
	<p>
	For IQR, we first must determine the quartiles. The median (found earlier) already is the second quartile so we have <m>Q_2 = 4.5</m> million. For the other two, the formula for computing percentiles gives you the 25th percentiile
	<md>
		<mrow>(n+1)p = 51(1/4) = 12.75</mrow>
		<mrow>Q_1 = P^{0.25} = 0.25 \times 1.9 + 0.75 \times 2.1 = 2.05</mrow>
	</md>
	and the 75th percentile
	<md>
		<mrow>(n+1)p = 51(3/4) = 38.25</mrow>
		<mrow>Q_3 = P^{0.75} = 0.75 \times 7 + 0.25 \times 8.3 = 7.325.</mrow>
	</md>
	Hence, the IQR = 7.325 - 2.05 = 5.275 million residents.
	</p>

	<p>
	From the computation before, again note that n=51 since the District of Columbia is included. The mean of this data found before was found to be approximately 6.20 million residents. So, to determine the variance you may find it easier to compute using the the alternate variance formulas <xref ref="AlternateVariance" />. 
	<md>
	<mrow> v &amp; = \left ( \frac{\sum_{k=1}^n y_k^2 }{n} \right ) - \mu^2
	</mrow>
	<mrow> &amp; \approx \frac{4434.37}{51} - (6.20)^2
	</mrow>
	<mrow> &amp; = 48.51
	</mrow>
	</md>
	and so you get a sample variance of
	<me> s^2 \approx \frac{51}{50} \cdot 48.51 = 49.48</me>
	and a sample standard deviation of
	<me>s \approx \sqrt{49.48} \approx 7.03</me> million residents.
	</p>
</solution>
</p>
</exercise>
</p>

<p>
The state population data set has been entered for you in the R cell below...
<sage language = 'r'>
<input>
data &lt;- c (0.6,0.6,0.6,0.7,0.7,0.8,0.9,1,1.1,1.3,1.3,1.4,1.6,
1.9,1.9,2.1,2.8,2.9,2.9,3,3,3.1,3.6,3.9,3.9,4.4,4.6,
4.8,4.8,5.3,5.4,5.7,5.9,6,6.5,6.6,6.6,6.7,7,8.3,
8.9,9.8,9.9,10,11.6,12.8,12.9,19.6,19.7,26.4,38.3)
paste("Variance = ", var(data))
paste("Standard Dev = ", sd(data))
paste("Inter Quantile Range =",IQR(data))

</input>
</sage>
</p>

</section>




<section><title>Adjusting Statistical Measures for Grouped Data</title>
<introduction>
<p>As you considered the measures of the center and spread before, each data point was considered individually. Often, data may however be grouped into categories. The number of data items in each category is called the "frequency" of that outcome and the collection of these frequencies for all outcomes is called a "frequency distribution". 
</p>
</introduction>

<subsection><title>Data Grouped into Single-valued Categories</title>
<p>In this case, rather than considering <m>x_k</m> to be the kth data value can take advantage of the grouping to perhaps save a bit on arithmetic.</p>
<p>Indeed, let's assume that data is grouped into m categories <m>x_1, x_2, ..., x_m</m> with corresponding frequencies <m>f_1, f_2, ..., f_m</m>. Then, for example, when computing the mean rather than adding <m>x_1</m> with itself <m>f_1</m> times just compute <m>x_1 \times f_1</m> for the first category and continuing through the remaining categories. This gives the following grouped data formula for the mean
		<me>
		\mu = \frac{x_1 f_1 + ... + x_m f_m}{f_1 + ... + f_m} = \frac{\sum_{k=1}^m x_k f_k}{\sum_{k=1}^m f_k}.
		</me>
and the following grouped data formula for the variance (along with one equivalent form)
		<me> 
		\sigma^2 = \frac{\sum_{k=1}^m ( x_k-\mu )^2 f_k}{\sum_{k=1}^m f_k} = \frac{\sum_{k=1}^m x_k^2 f_k}{\sum_{k=1}^m f_k} - \mu^2
		</me>

</p>

<exercise>
<p>
Consider the following data set
</p>
<p>
{3,
1,
2,
2,
3,
1,
3,
4,
5,
5,
1,
4,
5,
1,
2,
4,
5,
3,
2,
5,
2,
1,
2,
2,
5}</p>
<p>
Create a frequency distribution and determine the sample mean and variance.
</p>
<p>
<solution>
	<p>
	Collecting this data into a frequency distribution gives
	<table halign="left">
	<caption>Grouped Discrete Data</caption>
		  <tabular halign="right">
	  
			<row><cell bottom="medium"><m>x_k</m></cell><cell bottom="medium"><m>f_k</m></cell></row>      
			<row><cell>1</cell><cell>5</cell></row>
			<row><cell>2</cell><cell>7</cell></row>
			<row><cell>3</cell><cell>4</cell></row>
			<row><cell>4</cell><cell>3</cell></row>
			<row><cell>5</cell><cell>6</cell></row>
		</tabular>
	</table>
	Therefore, 
		<me>
		\overline{x} = \frac{1 \times 5 + 2 \times 7 + 3 \times 4 + 4 \times 3 + 5 \times 6}{5+7+4+3+6} \\
		= \frac{5 + 14 + 12 + 12 + 30}{25} = \frac{43}{25}
		</me>
	and 
		<md>
			<mrow>v &amp; = \frac{1^2 \times 5 + 2^2 \times 7 + 3^2 \times 4 + 4^2 \times 3 + 5^2 \times 6}{5+7+4+3+6} - \left ( \frac{43}{25} \right )^2 </mrow>
			<mrow> &amp; = \frac{5 + 28 + 36 + 48 + 150}{25} - \left ( \frac{43}{25} \right )^2 </mrow>
			<mrow> &amp; = \frac{4826}{625}</mrow>
			<mrow> &amp; \approx 7.7216</mrow>
		</md>
	and so <m>s^2 = \frac{25}{24} \frac{4826}{625} \approx 8.043</m>. 
	</p>
</solution>
</p>
</exercise>

</subsection>


<subsection><title>Data Grouped into Continuous Intervals</title>
<p>For measures on data grouped into intervals, it is somewhat difficult to do calculations when the data no longer exists as individual values since all you know is the frequencies of each interval. You can use "class marks"...the midpoints of each interval...as representers for all of the items that fell into that interval for computing means and variances.  For positional measures, you want to approach this in the same manner as with percentiles before.  That is, my doing some sort of linear interpolation on the width of each interval.
</p>
<p>So, for medians, consider the following approach:
</p>
<p>
<ol>
<li>Compute frequencies <m>f_k</m> and cummulative frequencies <m>F_k</m> for each class</li>
<li>Set m = total cummulative frequency/2 = <m>F_{last}/2</m></li>
<li>Determine the interval <m>k</m> where <m>m \in [F_{k-1},F_k]</m></li> 
<li>Set median = <m>(b_k-a_k)\frac{m - F_{k-1}}{f_k}+a_k</m></li>
</ol>
</p>

<p>
<example><title>Computing Median for Interval Grouped Data</title>
<p>
	<table halign="left">
	<caption>Interval Frequency Distribution</caption>
		  <tabular halign="right">
	  
			<row><cell bottom="medium"><m>[a_k,b_k]</m></cell><cell bottom="medium"><m>f_k</m></cell></row>      
			<row><cell>[0,5)</cell><cell>5</cell></row>
			<row><cell>[5,10)</cell><cell>7</cell></row>
			<row><cell>[10,20)</cell><cell>4</cell></row>
			<row><cell>[20,23)</cell><cell>3</cell></row>
			<row><cell>[23,30)</cell><cell>6</cell></row>
		</tabular>
	</table>
The total cummulative frequency is 25 and so <m>m = \frac{25}{2} = 12.5</m> which lies in the k = 3 interval [10,20) and <m>F_2 = 12</m>.  Therefore
<me>\text{median} = (20-10) \frac{12.5-12}{4} + 10 = 11.25</me>
</p>
</example>
</p>

</subsection>


</section>



<section><title>Other Statistical Point Measures</title>

<p>Above, we have investigated statistical measures that help determine the middle and the spread of a given data set. There are however other metrics available that help describe the distribution of that data.  Skewness is one of those metrics and describes any lack of symmetry of the data set's distribution and whether data is stretched out to one side or the other.
</p>

<p>
<definition><title>Skewness</title>
<statement>
<p xml:id="Skewness">For population data, the Skewness of <m>x_1, x_2, ..., x_n</m> is given by
<me> \frac{1}{\sigma^3} \frac{\sum_{k=1}^n ( x_k-\mu )^3}{n}.</me>
</p>
<p>For sample data, the Skewness of <m>x_1, x_2, ..., x_n</m> is given by
<me> \frac{1}{s^3} \frac{\sum_{k=1}^n ( x_k-\overline{x} )^3}{n}.</me>
</p>
</statement>
</definition>
</p>

<p>
A positive skewness indicates that the positive <m>(x_k - \mu)^3</m> terms (likewise <m>(x_k - \overline{x})^3</m> terms) overwhelm the negative terms. So, a positive skewness indicates that the data set is strung out to the right. Likewise, a negative skewness indicates a data set that is strung out to the left.
</p>

<p>
Data might tend to be clustered around the mean. The "kurtosis" can be used to measure how closely data resembles a "bell-shaped" collection.
</p>

<p>
<definition><title>Kurtosis</title>
<statement>
<p xml:id="Kurtosis">For population data, the Kurtosis of <m>x_1, x_2, ..., x_n</m> is given by
<me> \frac{1}{\sigma^4} \frac{\sum_{k=1}^n ( x_k-\mu )^4}{n}.</me>
</p>
<p>For sample data, the Kurtosis of <m>x_1, x_2, ..., x_n</m> is given by
<me> \frac{1}{s^4} \frac{\sum_{k=1}^n ( x_k-\overline{x} )^4}{n}.</me>
</p>
</statement>
</definition>
</p>

<p>
A kurtosis of 3 indicates that the data is perfectly bell shaped (a "normal" distribution) whereas data further away from 3 indicates data that is less bell shaped.
</p>

<theorem><title>Alternate Formulas for Skewness and Kurtosis</title>
<statement>
Skewness = 
<me>\text{skewness} = \frac{1}{s^3} 
\left [ \frac{\sum_{k=1}^n x_k^3}{n} - 3 v \overline{x} - \overline{x}^3 \right ]</me>
and Kurtosis =
<me>\text{kurtosis} = \frac{1}{s^4} 
\left [ \frac{\sum_{k=1}^n x_k^4}{n} - 4 \overline{x} \frac{\sum_{k=1}^n x_k^3 }{n} + 6 \overline{x}^2 v - 3 \overline{x}^4  \right ]</me>
<proof>
<p>
For skewness, expand the cubic and break up the sum. Factoring out constants (such as <m>\overline{x}</m>) gives
<md>
	<mrow>&amp; \frac{\sum_{k=1}^n ( x_k-\overline{x} )^3}{n}</mrow>
	<mrow>&amp; = \frac{\sum_{k=1}^n x_k^3}{n} - 3 \overline{x} \frac{\sum_{k=1}^n x_k^2 }{n} + 3 \overline{x}^2 \frac{\sum_{k=1}^n x_k}{n} - \frac{\sum_{k=1}^n \overline{x}^3}{n}</mrow>
	<mrow>&amp; = \frac{\sum_{k=1}^n x_k^3}{n} - 3 \overline{x}(v + \overline{x}^2) + 3 \overline{x}^3 - \overline{x}^3</mrow>
	<mrow>&amp; = \frac{\sum_{k=1}^n x_k^3}{n} - 3 \overline{x}v - \overline{x}^3</mrow>
</md>
and divide by the cube of the standard deviation to finish. Note that the first expansion in the derivation above can be used quickly if the data is collected in a table and powers easily computed.
</p>
<p>
For kurtosis, similarly expand the quartic and break up the sum as before. Note that you can extract the value of the cubic term by solving for that term in the skewness formula above. Then,
<md>
	<mrow>&amp; \frac{\sum_{k=1}^n ( x_k-\overline{x} )^4}{n}</mrow>
	<mrow>&amp; = \frac{\sum_{k=1}^n x_k^4}{n} - 4 \overline{x} \frac{\sum_{k=1}^n x_k^3 }{n} + 6 \overline{x}^2 \frac{\sum_{k=1}^n x_k^2}{n} - 4 \overline{x}^3 \frac{\sum_{k=1}^n x_k}{n} + \frac{\sum_{k=1}^n \overline{x}^4}{n}</mrow>
	<mrow>&amp; = \frac{\sum_{k=1}^n x_k^4}{n} - 4 \overline{x} \frac{\sum_{k=1}^n x_k^3 }{n} + 6 \overline{x}^2 (v+\overline{x}^2) - 4 \overline{x}^4 + \overline{x}^4</mrow>
	<mrow>&amp; = \frac{\sum_{k=1}^n x_k^4}{n} - 4 \overline{x} \frac{\sum_{k=1}^n x_k^3 }{n} + 6 \overline{x}^2 v - 3 \overline{x}^4 </mrow>
</md>
and then divide by the fourth power of the standard deviation.  Note again that the first expansion in the derivation above might also be a useful shortcut.
</p>
</proof>
</statement>
</theorem>

<p>Going back to a previous example...</p>

<p>
Computing skewness and kurtosis by hand can often be better organized using a table. Below, notice that the <m>x_k</m> column would be the given data values but the other columns you could again easily compute.

<table halign="left">
      <tabular halign="right">
      
<row>
<cell bottom="medium"><m>x_k</m></cell>
<cell bottom="medium"><m>x_k^2</m></cell>
<cell bottom="medium"><m>x_k^3</m></cell>
<cell bottom="medium"><m>x_k^4</m></cell></row>      
<row><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row>		
<row><cell>-1</cell><cell>1</cell><cell>-1</cell><cell>1</cell></row>		
<row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row>		
<row><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell></row>		
<row><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell></row>		
<row><cell>5</cell><cell>25</cell><cell>125</cell><cell>625</cell></row>		

      </tabular>
      <caption>Computing data statistics by hand</caption>
</table>

So, <m>\Sigma x_k = 9</m> and <m>\Sigma x_k^2 = 35</m> as before and so
<m>\overline{x} = \frac{3}{2}</m>, <m>v = \frac{26}{6}</m>, <m>s^2 = \frac{6}{5} \times v = \frac{26}{5}</m>, and so <m>s = \sqrt{\frac{26}{5}} = \sqrt{5.2}</m>.

But also, <m>\Sigma x_k^3 = 141</m> and <m>\Sigma x_k^4 = 659</m>.  Use these in the formulas above to obtain skewness of

<me>\left [ \frac{141}{6} - 3 \cdot \frac{26}{5} \cdot \frac{3}{2} - \left ( \frac{3}{2} \right )^2 \right ] / s^3 </me>

and kurtosis of 

<me>\left [ \frac{659}{6} - 4 \cdot \frac{3}{2} \cdot \frac{141}{6} + 6 \left ( \frac{3}{2} \right )^2  \cdot \frac{26}{5}  - 3 \cdot \left ( \frac{3}{2} \right )^4 \right ] / s^4.</me>
</p>



</section>



<section><title>Visual Statistical Measures - Graphical Representation of Data</title>
<introduction>
<p>Data sets can range from small to very large. Visual representations of these data sets often allow you to see trends and reveal a lot about the distribution of the data values.</p>

</introduction>

<subsection><title>Histograms</title>

<p>Frequency Histograms - height matters</p>
<p>Consider the data set given by
		
<table halign="left">
   <caption>Basic Frequency Table</caption>
      <tabular halign="right">
      
<row><cell bottom="medium"><m>x_k</m></cell><cell bottom="medium"><m>f_k</m></cell></row>      
<row><cell>1</cell><cell>8</cell></row>		
<row><cell>2</cell><cell>12</cell></row>		
<row><cell>3</cell><cell>6</cell></row>		
<row><cell>4</cell><cell>3</cell></row>		
<row><cell>5</cell><cell>1</cell></row>		
<row><cell>6</cell><cell>2</cell></row>		

      
      </tabular>
</table>
</p>

<p>	
A frequency histogram representing this data looks like

<image source="images/frequencyhistogram.png" />
</p>

<p>Experiment with creating your own histogram by inputting your data into the interactive Sage cell below.
<sage>
<input>
#  This function is used to convert an input string into separate entries
def g(s): return str(s).replace(',',' ').replace('(',' ').replace(')',' ').split()

@interact
def _(freq = input_box("1,1,1,1,2,2,2,3,3,3,3,1,5",label="Enter data separated by commas")):
    freq = g(freq)
    freq = [int(k) for k in freq]
    m = min(freq)
    M = max(freq)
    bn = M-m+1
    histogram( freq, range=[m-1/2,M+1/2], bins = bn, align="mid", linewidth=2, edgecolor="blue", color="yellow").show()
</input>
</sage>

</p>
<p>
Relative Frequency Histograms - In this case, area describes relative frequency.  Notice in the interactive cell above that each bar is of width one. Therefore, frequency = area. In some instances where data may be grouped the total width of the interval may be different and so the height will need to be adjusted so that the total area of each bar corresponds to the relative frequency of that category.
</p>

<p>Cummulative Histograms.  In these a running total is presented using all values from the given point and below.
<sage>
<input>
#  This function is used to convert an input string into separate entries
def g(s): return str(s).replace(',',' ').replace('(',' ').replace(')',' ').split()

@interact
def _(freq = input_box("1,1,1,1,2,2,2,3,3,3,3,1,5",label="Enter data separated by commas")):
    freq = g(freq)
    freq = [int(k) for k in freq]
    top = len(freq)
    m = min(freq)
    M = max(freq)
    bn = M-m+1
    histogram( freq, range=[m-1/2,M+1/2], cumulative = "true", bins = bn, align="mid", linewidth=2, edgecolor="blue", color="yellow").show(ymax=top)
</input>
</sage>		
</p>
</subsection>

<subsection><title>Stem and Leaf Plot</title>

<p> 
A Stem and Leaf Plot allows you to create a histogram of sorts but maintain the individual data values. To create one of these plots, you will need to consider your particular data set and create a two-step sieve for organizing the set.  The first part is to create "stems" that are often associated with the highest digit(s) of each data value and the "leaves" that are often associated with the remaining digit(s) of the data value.
</p>
<p>
Once the data set is broken down into stems and leaves, it is often simple to sort the leaves under each stem to yield an "ordered Stem and Leaf Plot". Such as mechanism is a simple two-step procedure that allows you to sort a data set by hand.
</p>

<p>
<example><title>Simple Stem and Leaf Plot</title>
<p>
Consider the data points 25, 3, 17, 12, 22, 34, 12, 11, 16, 42, 9, 12, 17.
In this case we will consider the stems to be the tens digits and the leaves to be the ones digits. This gives
</p>

<p>
<table halign="left">
   <caption>Stem and Leaf Plot (unordered)</caption>
      <tabular halign="right">
      
<row><cell bottom="medium">Stems</cell><cell bottom="medium">Leaves</cell></row>      
<row><cell>0</cell><cell halign="left">3 9</cell></row>		
<row><cell>1</cell><cell halign="left">7 2 2 1 6 2 7</cell></row>		
<row><cell>2</cell><cell halign="left">5 2</cell></row>		
<row><cell>3</cell><cell halign="left">4</cell></row>		
<row><cell>4</cell><cell halign="left">2</cell></row>		      
      </tabular>
</table>
Then, an ordered Stem and Leaf Plot would be

<table halign="left">
   <caption>Stem and Leaf Plot (ordered)</caption>
      <tabular halign="right">
      
<row><cell bottom="medium">Stems</cell><cell bottom="medium">Leaves</cell></row>      
<row><cell>0</cell><cell halign="left">3 9</cell></row>		
<row><cell>1</cell><cell halign="left">1 2 2 2 6 7 7</cell></row>		
<row><cell>2</cell><cell halign="left">2 5</cell></row>		
<row><cell>3</cell><cell halign="left">4</cell></row>		
<row><cell>4</cell><cell halign="left">2</cell></row>		      
      </tabular>
</table>
</p>

<p>
Notice, in each case you can extract the original data values by recombining the stem with a corresponding leaf. Indeed, for these 13 data values the median should be be 7th in the sorted list or the value in the 10's stem with leaf 6...that is, 16.
</p>
</example>
</p>

<p>
<example><title>Stem and Leaf Plot for State Populations</title>
<p>
Using the state population data above, consider organizing the data but using a "two-pass sort" where you first roughly break data up into groups based upon ranges which relate to their first digit(s). In this case, let's break up into groups according to populations corresponding to 0-4 million, 5-9 million, 10-14 million, 15-19, million, 20-24 million, 25-29 million, 30-35 million, and 35-39 million. We can represent these classes by using the stems 0L, 0H, 1L, 1H, 2L, 2H, 3L, and 3H where the L and H represent the one's digits L in {0, 1, 2, 3, 4} and H in {5, 6, 7, 8, 9}.  Once we group the data into these smaller groups then we can write the remaining portion of the number horizontally as leaves (in this case with one decimal place for all values.) This gives a step-and-leaf plot. If we additionally sort the data in the leaves then this gives you an ordered stem-and-leaf plot. For the state population data, the ordered stem-and-leaf plot is given by
		
	
<image source="images/stemandleaf.png" />

Notice how it is easy to now see that most state populations are relatively small and that there are relatively few states with larger population. Also, notice that you can use this plot to relatively easily identify minimum, maximum, and other order statistics.		
</p>
</example>
</p>

</subsection>

<subsection><title>Box and Whisker Diagram (Box Plot)</title>

<p>This graphical display identifies the "5-number-summary" associated with the minimum, quartiles, and the maximum. That is, <m>y_1, Q_1, Q_2, Q_3, y_n</m>.  These values separate the data roughly into quarters. To distinguish these quarters connect <m>y_1</m> and <m>Q_1</m> with a straight line (a whisker) and do the same with <m>Q_3</m> and <m>y_n</m>. Use a box to connect <m>Q_1</m> with <m>Q_2</m> and the same to connect <m>Q_2</m> with <m>Q_3</m>. Then the boxed areas also identify the IQR.  
</p>

<p>
<sage language = 'r'>
<input>
data &lt;- c (0.6,0.6,0.6,0.7,0.7,0.8,0.9,1,1.1,1.3,1.3,
1.4,1.6,1.9,1.9,2.1,2.8,2.9,2.9,3,3,3.1,
3.6,3.9,3.9,4.4,4.6,4.8,4.8,5.3,5.4,5.7,
5.9,6,6.5,6.6,6.6,6.7,7,8.3,8.9,9.8,9.9,
10,11.6,12.8,12.9,19.6,19.7,26.4,38.3)
paste("Inter Quantile Range =",IQR(data))
paste("Box and Whisker Diagram - Box Plot):")
boxplot(data, horizontal=TRUE)
</input>
</sage>
</p>

<p><title>Computing Percentiles</title>
<exercise>
<introduction>
		<p>
		Let's use a box plot to determine some order statistics.
		</p>
		</introduction>
		<webwork source="Library/NAU/setStatistics/FiveNumFromPlot.pg">
		</webwork>
		<conclusion>
		<p>
		
		</p>
		</conclusion>
</exercise>
</p>
		
</subsection>

<subsection><title>Density Plots</title>
<p>
A Density Plot can be created to visually interpret if the variable is close to normal
</p>

<sage language='r'>
<input>
library(e1071)
par(mfrow=c(1,2))    # graph into two columns
plot(density(cars$speed), main="Density Plot: Speed", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(cars$speed),2)))

</input>
</sage>


<p>
www.machinelearningplus.com
</p>
</subsection>
</section>


<section><title>Summary</title>
<introduction>
<p>
Links to the main formulas related to descriptive statistics:
</p>
</introduction>
<p><xref ref="OrderStatistics">Order Statistics</xref></p>
<p><xref ref="MaxMin">Maximum and Minimum</xref></p>
<p><xref ref="Percentiles">Percentiles</xref></p>
<p><xref ref="Quartiles">Quartiles</xref></p>
<p><xref ref="Deciles">Deciles</xref></p>
<p><xref ref="FiveNumberSummary">5-number summary</xref></p>
<p><xref ref="Mean">Mean</xref></p>
<p><xref ref="Median">Median</xref></p>
<p><xref ref="Midrange">Midrange</xref></p>
<p><xref ref="Range">Range</xref></p>
<p><xref ref="IQR">Inter Quartile Range</xref></p>
<p><xref ref="Variance">Variance</xref></p>
<p><xref ref="Skewness">Skewness</xref></p>
<p><xref ref="Kurtosis">Kurtosis</xref></p>
</section>


<section><title>Exercises</title>

<p>Complete the online WebWorK homework set "Computational Measures".</p>

<exercise>
	<statement>
	<p>Create a data set with about 10 elements. For your data set, compute each of the measures from this chapter and present your data using a frequency histogram.</p>
	</statement>
</exercise>

<exercise>
	<statement>
	<p>Find a "real-world" data set (similar perhaps to the Census data presented above.) Compute each of the measures from this chapter. Interpret and present your conclusions in an electronic report which can include an excel spreadsheet.</p>
	</statement>
</exercise>

</section>

</chapter>