<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="CurveEstimation" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Regression</title>

<section><title>Introduction</title>
<p>
Given a set of data points <m>(x_0,y_0), (x_1,y_1), ... (x_n,y_n)</m>, it is often desirable to have a nice continuous formula <m>y = f(x)</m> that expresses the general nature of those data points. Such a formula "interpolates" the data points if <m>y_k = f(x_k)</m>, that is the formula gives a graph that exactly passes through each of the given data points.</p>
<p>
On the other hand, sometimes the data points are known to be only approximate or the complexity of the formula needed to interpolate all of the data points exactly is too large. In this case, the formula may only be required to return values that are relatively close to the data points. Such a formula is said to "approximate" and gives <m>y_k \approx f(x_k)</m>. Below, we consider ways to create useful models that approximate the data points.
</p>
<p>
From basic algebra, if you are given two distinct points then there is one line which passes exactly through (i.e. interpolates) both. There are many ways to create this linear model but for points <m>(x_0,y_0), (x_1,y_1)</m>,

<me> y = \frac{y_1 - y_0}{x_1 - x_0}(x - x_0) + y_0</me>

is the linear function which passes through both points if the x-values are distinct.  If the x's are equal then 
<me> x = x_0</me>
is linear and interpolates both data points. 
However, once you collect three or more points it is likely that there is no line which exactly "interpolates" all of the points. So, if we desire a linear model then we must settle for a model that approximates. In this chapter, you will investigate how to create polynomial functions which in some manner approximate a collection of data point in some "best" manner.
</p>
<exercise>
<p>Verify that each of the formulas presented above interpolates when given exactly two data points.
</p>
</exercise>
</section>

<section><title>Linear Regression</title>
<p>
In this section, we will presume only one independent variable x and one dependent variable y.
</p>
<p>
Consider a collection of data points 
<me>(x_0,y_0), (x_1,y_1), ... , (x_n,y_n)</me>
and a general linear function 
<me>f(x) = mx + b.</me>
It is possible but generally unlikely that each of the given data points are exactly interpolated by the linear function. However, you may notice that the data points exhibit a linear tendency or that the underlying physics might suggest a linear model. If so, you may find it easier to predict values of y for given values of x using a linear approximation. Here you will investigate a method for doing so called "linear regression", "least-squares", or "best-fit line". 
</p>
<p>
But why even bother creating a formula (a line here) to approximate data that does not satisfy that formula? Remember that you would expect collected data to vary slightly as one repeatedly collects that data in the same way that you would expect to make a slightly different score on repeated attempts at exams on the same material. Creating a formula that is close to your data gives a well-defined way to predict a y value for a given x value. This predictive behavior is illustrated in the exercise below.
</p>

<exercise>
		<introduction>
		<p>
		Consider using the following best-fit line for approximation.
		</p>
		</introduction>
		<webwork source="Library/NAU/setStatistics/linear_regression1.pg">
		</webwork>
		<conclusion>
		<p>
		So, these are simple calculations.
		</p>
		</conclusion>
</exercise>

<p>
To determine the best-fit line, you need to determine what is meant by the word "best". Here, we will derive the standard approach which interprets this to mean that the total vertical error between the line and the provided data points is minimized in some fashion.  Indeed, this vertical error would be of the form
<me>e_k = f(x_k) - y_k</me>
and would be zero if f(x) exactly interpolated at the given data point.  Note, some of these errors will be positive and some will be negative. To avoid any possible cancellation of errors, you can look at taking absolute values (which is tough to deal with algebraically) or by squaring the errors. This second option will be the approach taken here. This is similar to the approach taken earlier when developing formulas for the variance.
</p>
<p>The best-fit line therefore will be the line <m>f(x) = mx+b</m> so that the "total squared error" is minimized. This total squared error is given by
<me>TSE(m,b) = \sum_{k=1}^n e_k^2 = \sum_{k=1}^n (f(x_k) - y_k)^2 = \sum_{k=1}^n (m x_k + b - y_k)^2.</me>
</p>
<p>
For the following interactive cell, consider for the given data points various values for the slope and y-intercept and see if you can make the total squared error as small as possible. In doing so, notice the vertical distances from the line to the given data points generally decreases as this error measure gets smaller. 
</p>
<sage>
<input>
var('x')
@interact
def _(Points = input_box([(-1,1),(3,1),(4,3),(6,4)]), m = slider(-4,4,0.05,1),b = slider(-2,2,0.05,1)):   
    G = points(Points,size=20)
    xpt = []
    ypt = []
    f = m*x + b
    TSE = 0
    for k in range(len(Points)):
        x0 = Points[k][0]
        xpt.append(x0)
        y0 = Points[k][1]
        ypt.append(y0)
        TSE += (f(x=x0) - y0)^2
        G += line([(x0,f(x=x0)),(x0,y0)],color='orange')
    G += plot(f,x,min(xpt),max(xpt),color='gray')
    T = 'Total Squared Error = $%s$'%str(TSE)
    G.show(title = T)
</input>    
</sage>
<p>
So that we don't have to guess the best values for slope and intercept, we can appeal to calculus. Indeed, to minimize this function of the two variables m and b take partial derivatives and set them equal to zero to get the critical values:
<me>TSE_m = \sum_{k=1}^n 2(m x_k + b - y_k) \cdot x_k</me>
and
<me>TSE_b = \sum_{k=1}^n 2(m x_k + b - y_k) \cdot 1 .</me>
Setting equal to zero and solving gives what is known as the "normal equations":
<me>m \sum_{k=1}^n x_k^2 + b \sum_{k=1}^n x_k = \sum_{k=1}^n x_k y_k</me>
and
<me>m \sum_{k=1}^n x_k + b \sum_{k=1}^n 1 = \sum_{k=1}^n y_k.</me>
Solving these for m and b gives the best fit line.
</p>
<p>So, let's see how to graph points against the best-fit line using R
</p>
<sage language='r'>
<input>
x &lt;- c(1, 2, 3, 5, 5, 6)
y &lt;- c(5, 4, 2, 2, 3, 1)
cor(x,y)   # correlation coerfficient
pts = data.frame(x, y)
plot(pts,pch = 16, cex = 1.0, col = "blue", main = "Scatter Plot vs Best-Fit Line", xlab = "x", ylab = "y")
# pch = 16 creates solid dots, while cex = 1.5 creates dots that are 1.5 times bigger than the default.
lm(x ~ y)
abline(lm(x ~ y))
</input>
</sage>

<exercise>
		<introduction>
		<p>
		Ok.  Let's see if you can apply this to get a best fit line.
		</p>
		</introduction>
		<webwork source="Library/ASU-topics/setStat/dueck4_2_4.pg">
		</webwork>
		<conclusion>
		<p>
		So, these are simple calculations.
		</p>
		</conclusion>
</exercise>

</section>

<section><title>Correlation</title>
<p>
You can plot points and plot the resulting best-fit line determined in the previous section but the question remains whether the line is any good. In particular, the real use of the line often is to subsequently predict y-values for a given x-value. However, it is very likely that the best-fit line does not even pass through any of the provided data points.  So, how can something that misses every marker still be considered a good fit. To quantify this, we first need to discuss a way to measure how well two variables vary with each other.
</p>
<definition><title>Covariance</title>
<p>Given paired data 
<me>(x_0,y_0), (x_1,y_1), ... , (x_n,y_n)</me>
with corresponding means <m>\overline{x}</m> and <m>\overline{y}</m>, the covariance is given by
<me>Cov(X,Y) = \sum_{k=0}^n (x_k-\overline{x})(y_k-\overline{y})/n.</me>
</p>
</definition>
<p>This general definition provides a general measure which is a second order term (like variance) but also maintains "units". To provide a unit-less metric, consider the following measure.
<definition><title>Correlation Coefficient</title>
<p>Given a collection of data points, the correlation coefficient is given by
<me>r = \frac{Cov(X,Y)}{\sigma_x \sigma_y}</me>
where <m>\sigma_x</m> is the standard deviation of the x-values only and <m>\sigma_y</m> is the standard deviation of the y-values only.
</p>
</definition>
<theorem><title>Interpretation of the Correlation Coefficient</title>
<statement>If the points are colinear with a positive slope then r=1 and if the points are collinear with a negative slope then r=-1.</statement>
<proof>
<p>
Assume the data points are colinear with a positive slope. Then the 
<m>TSE(m_0,b_0) = 0</m> for some <m>m_0</m> and <m>b_0</m>. For this line notice that <m>f(x_k) = y_k</m> exactly for all data points. It is easy to show then that <m>\overline{y} = m_0 \overline{x} + b_0</m> and <m>\sigma_y = | m_0 | \sigma_x</m>.

Therefore,
<me>Cov(X,Y) = \sum_{k=0}^n (x_k-\overline{x})(m_0 x_k + b_0 - (m_0 \overline{x} + b_0))/n = m_0 \sigma_x^2</me>
Putting these together gives correlation coefficient
<me>r = \frac{m_0 \sigma_x^2}{\sigma_x m_0 \sigma_x} = 1.</me> 
A similar proof follows in the second case by noting that <m>m_0 / | m_0 | = -1</m>.
</p>
</proof>
</theorem>

<exercise>
		<introduction>
		<p>
		Interpreting correlation coefficients.
		</p>
		</introduction>
		<webwork source="Library/CollegeOfIdaho/setStatistics_Ch04ScatterplotsAndCorrelation/04Stats_10_ScatterCorrelation.pg">
		</webwork>
		<conclusion>
		<p>
		So, these are simple calculations.
		</p>
		</conclusion>
</exercise>

<exercise>
		<introduction>
		<p>
		Interpreting correlation coefficients.
		</p>
		</introduction>
		<webwork source="Library/CollegeOfIdaho/setStatistics_Ch04ScatterplotsAndCorrelation/04Stats_07_ScatterCorrelation.pg">
		</webwork>
		<conclusion>
		<p>
		So, these are simple calculations.
		</p>
		</conclusion>
</exercise>


</p>
</section>

<section><title>Higher Degree Regression</title>
<p>
Continuing in a similar fashion to the previous section, consider now an approximation using a quadratic function <m>f(x) = a x^2 + b x + c</m>.  In this case, the total squared error would be of the form
<me>TSE(a,b,c) = \sum_{k=0}^n (a x_k^2 + b x_k + c - y_k)^2.</me>
Taking all three partials gives
<me>TSE_a = \sum_{k=0}^n 2(a x_k^2 + b x_k + c - y_k) \cdot x_k^2</me>
<me>TSE_b = \sum_{k=0}^n 2(a x_k^2 + b x_k + c - y_k) \cdot x_k</me>
<me>TSE_c = \sum_{k=0}^n 2(a x_k^2 + b x_k + c - y_k) \cdot 1 .</me>
Once again, setting equal to zero and solving gives the normal equations for the best-fit quadratic
<me>a \sum_{k=0}^n x_k^4 + b \sum_{k=0}^n x_k^3 + c \sum_{k=0}^n x_k^2 = \sum_{k=0}^n x_k^2 y_k</me>
<me>a \sum_{k=0}^n x_k^3 + b \sum_{k=0}^n x_k^2 + c \sum_{k=0}^n x_k = \sum_{k=0}^n x_k y_k</me>
<me>a \sum_{k=0}^n x_k^2 + b \sum_{k=0}^n x_k + c \sum_{k=0}^n 1 = \sum_{k=0}^n y_k.</me>
</p>

<p>We can also approach the derivation of regression forumulas using a linear algebra approach. To do this, consider the equations generated by plugging in the data points <m>(x_0,y_0), (x_1, y_1), ... , (x_n, y_n)</m> into the quadratic model. This yields a (likely overdetermined) system of equations.  Appending an error term <m>\epsilon_k</m> for each equation gives the following matrix form: 
</p>
<me>
\begin{bmatrix}
y_0
\\y_1
\\ y_2
\\ ...
\\ y_n
\end{bmatrix}
=

\begin{bmatrix}
 x_0^2 \amp x_0 \amp 1 \\ 
 x_1^2 \amp x_1 \amp 1 \\ 
 x_2^2 \amp x_2 \amp 1 \\ 
 ... \amp ... \amp ... \\ 
 x_n^2 \amp x_n \amp 1 
\end{bmatrix} 

\cdot 

\begin{bmatrix}
a
\\ 
b
\\
c
\end{bmatrix}
+
\begin{bmatrix}
 \epsilon_0 \\ 
 \epsilon_1 \\ 
 \epsilon_2 \\ 
 ... \\ 
 \epsilon_n 
\end{bmatrix}
</me>
<p>
which in matrix form looks something like
<me>Y = XA + \epsilon.</me>
Solving for <m>\epsilon</m> and then minimizing <m>\epsilon^t \epsilon</m> yields the same solution as above. In matrix form, after some work this becomes
<me>A = (X^t X)^{-1} X^t Y</me>
with the matrix A containing the three unknowns a, b, and c.
</p>
<p> We can also use Matlab (or the opensource alternative "octave") to compute this linear algebra for us. The graph here using the sagecell is a text graph and is very rudimentary but plugging this code into Matlab or a desktop version of octave should present a very nice graph.
</p>

<sage language ='octave'>
<input>
x = [-1 0 1 3 5 5]
y = [5 3 0 -1 3 6]
n = max(size(x));
for k = 1:n
  X(k,1) = x(k)^2;
  X(k,2) = x(k);
  X(k,3) = 1;
end
Y = y';  # transpose the set of y-values to be a column vector
A = inv(X'*X)*X'*Y;
a = A(1)
b = A(2)
c = A(3)
u = min(x):0.1:max(x);  # create a set of input values for plotting
v = a * u.^2 + b * u + c;
plot(x,y,'o',u,v,'.')
</input>
</sage>

<p>
Cutting and pasting this code into perhaps http://octave-online.net gives a nice, non ASCII graph. Below, we do the same thing but using Sage.</p>

<sage>
<input>
var('x')
xpts = vector(RR,(-1, 0, 1, 3, 5, 5))
ypts = vector(RR,(5, 3, 0, -1, 3, 6))
ones = vector(RR,(1, 1, 1, 1, 1, 1))
xpts2 = []     # accumulate the squares
pts = []       # accumulate the (x,y) pairs for plotting purposes
for k in range(len(xpts)):
    xpts2.append(xpts[k]^2)
    pts.append((xpts[k],ypts[k]))
xpts2 = vector(xpts2)

X = matrix(RR,[xpts2]).stack(xpts).stack(ones).transpose()  # create X
Y = matrix(RR,ypts).transpose()
Xt = X.transpose()
A = (Xt*X).inverse()*Xt*Y
[a,b,c] = [A[0][0], A[1][0], A[2][0]]
f = a*x^2+b*x+c
banner = "The quadratic interpolant is given by $%s$"%str(latex(f))
G = points(pts,size=20)
H = plot(f,x,min(xpts),max(xpts),title=banner)
show(G+H)
</input>
</sage>

<exercise><title>Cubic Interpolant</title>
<statement>
Modify the Sage code above to give a best-fit cubic interpolant.
</statement>
<solution>

<pre>
var('x')
xpts = vector((-1, 0, 1, 3, 5, 5))
ypts = vector((5, 3, 0, 4, 3, -1))
ones = vector((1, 1, 1, 1, 1, 1))
xpts3 = []
xpts2 = []
pts = []
for k in range(len(xpts)):
    xpts3.append(xpts[k]^3)
    xpts2.append(xpts[k]^2)
    pts.append((xpts[k],ypts[k]))
xpts3 = vector(xpts3)
xpts2 = vector(xpts2)

X = matrix([xpts3]).stack(xpts2).stack(xpts).stack(ones).transpose()
Y = matrix(ypts).transpose()
Xt = X.transpose()

A = (Xt*X).inverse()*Xt*Y
[a,b,c,d] = [A[0][0],A[1][0],A[2][0],A[3][0]]


f = a*x^3 + b*x^2 + c*x + d
banner = "The cubic interpolant is given by $%s$"%str(latex(f))
G = points(pts,size=20)
H = plot(f,x,min(xpts),max(xpts),title=banner)
show(G+H)
</pre>


</solution>
</exercise>

</section>




<section><title>Multi-variable Regression</title>
<p>
The regression models that we have looked at presumed a single independent variable. It is much more likely when investigating cause and effect relationships that there are perhaps many independent variables that contribute.  Let's briefly consider a model with two independent variables and then an interesting example that illustrates the limits when utilizing models.
</p>
<p>
Toward that end, a basic two-variable linear model is of the form
<me>z = \alpha_1 x + \alpha_2 y + \beta .</me>
For data points <me>(x_0,y_0,z_0), (x_1,y_1,z_1), (x_2,y_2,z_2), ... , (x_n,y_n,z_n) , </me> 
using a linear systems approach similar to the previous section, evaluating at these data points and appending an error term to each equation gives, in matrix form:

<me>
\begin{bmatrix}
z_0
\\z_1
\\ z_2
\\ ...
\\ z_n
\end{bmatrix}
=

\begin{bmatrix}
 x_0 \amp y_0 \amp 1 \\ 
 x_1 \amp y_1 \amp 1 \\ 
 x_2 \amp y_2 \amp 1 \\ 
 ... \amp ... \amp ... \\ 
 x_n \amp y_n \amp 1 
\end{bmatrix} 

\cdot 

\begin{bmatrix}
\alpha_1
\\ 
\alpha_2
\\
\beta
\end{bmatrix}
+
\begin{bmatrix}
 \epsilon_0 \\ 
 \epsilon_1 \\ 
 \epsilon_2 \\ 
 ... \\ 
 \epsilon_n 
\end{bmatrix}
</me>
where the <m>\epsilon_k</m> terms are the deviation between the exact data point and the approximation of that point on some plane. Symbolically
<me>Z = XA + \epsilon.</me>
If all of the points lie on the same plane (unlikely), then <m>\epsilon = 0</m>). Otherwise, once again applying a least squares solution approach is the same as minimizing <m> \epsilon^t \epsilon</m> and eventually gives
<me>A = (X^t X)^{-1} X^t Z</me>
in general.  Evaluating this with X and Z as above gives the matrix  
 
<me> A = \begin{bmatrix}
 \alpha_1 \\ 
 \alpha_2 \\  
 \beta 
\end{bmatrix}
</me>

</p>

<p>
A good example of the usefulness and limitations of multi-variate regression is the calculation of the "Heat Index". This measure determines a measure of discomfort relative to the ambient temperature and the relative humidity. Indeed, in warm climates a high temperature is more difficult to bear if the humidity is also high. One reason is that with high humidity the body is less effective in shedding heat through evaporation of body sweat.
</p>
<p>
The National Weather Service in 1990 published the following multiple regression equation for Heat Index (HI) relative to the ambient temperature (T) and the relative humidity (RH)
<me>H = -42.379 + 2.04901523 \cdot T + 10.14333127 \cdot R - 0.22475541 \cdot T \cdot R  \\
- 6.83783 \cdot 10^{-3} \cdot T^2 - 5.481717 \cdot 10^{-2} \cdot R^2 + 1.22874 \cdot 10^{-3} \cdot T^2 \cdot R \\
+ 8.5282 \cdot 10^{-4} \cdot T \cdot R^2-1.99 \cdot 10^{-6} \cdot T^2 \cdot R^2.</me>
Notice, their model utilizes quadratic terms and therefore uses a generalization of the linear result presented above. Details on how this equation was determined and other details are available at https://www.wpc.ncep.noaa.gov/html/heatindex_equation.shtml .
</p>

<sage><title> Computing Heat Index Values</title>
<input>
@interact
def _(T = (90),R = (95)):
    H = -42.379+2.04901523*T+10.14333127*R-0.22475541*T*R-6.83783*10^(-3)*T^2-5.481717*10^(-2)*R^2+1.22874*10^(-3)*T^2*R+8.5282*10^(-4)*T*R^2-1.99*10^(-6)*T^2*R^2
    print "At temperature ",T," with ",R,"% relative humidity, the Heat Index =",H
</input>
</sage>
<p>
Below one can compute a table for various ambient Temperature readings given one value for relative humidity.  Notice what happens for a relatively high humidity and relatively high temperature.
</p>
<sage><title> Computing Heat Index Table</title>
<input>
R = 95
for T in range(80,121):
    H = -42.379+2.04901523*T+10.14333127*R-0.22475541*T*R-6.83783*10^(-3)*T^2-5.481717*10^(-2)*R^2+1.22874*10^(-3)*T^2*R+8.5282*10^(-4)*T*R^2-1.99*10^(-6)*T^2*R^2
    print "At temperature ",T," with ",R,"% relative humidity, the Heat Index =",H
	
</input>
</sage>
<p>
Indeed, you cannot roast a turkey by simply turning the oven on 120 and pumping in a lot of humidity since the turkey is not trying to cool itself anymore. Any discomfort measured on the turkey's behalf would certainly be matched by the human since the bird would remain very much uncooked. The issue is that this model doesn't presume the possibility of 120F and 95% humidity. Often, in situations where the temperature is able to reach that level, such as a desert, then the relative humidity is correspondingly low. This idea of using a model to predict extreme values beyond the measured data is called extrapolation and should be utilized with care. Interpolation to estimate values within the confines of the measured data is however generally a safe bet.

</p>
</section>

</chapter>