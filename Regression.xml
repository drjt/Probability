<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="Regression" xmlns:xi="http://www.w3.org/2001/XInclude">
<title>Regression</title>

<section xml:id="RegressionIntroduction"><title>Introduction</title>

<p>
Given two distinct points, there is one line which passes exactly through both.  Indeed, if the points are <m>(x_1,y_1), (x_2,y_2)</m> then presuming the x-values are different gives the equation
	<me> y = \frac{y_2 - y_1}{x_2 - x_1}(x - x_1) + y_1</me>
is the linear function which passes through both points.  If the x's are equal then 
	<me> x = x_1</me>
is your linear equation. 
However, once you collect three or more points it is likely that there is no line which exactly "interpolates" all of the points.
</p>

<p>
In this chapter, you will investigate how to create polynomial functions which in some manner approximate a collection of data point in some "best" manner.
</p>
</section>





<section xml:id="BestFitLine"><title>Best-fit Line</title>

<p>
In this section, we will presume only one independent variable x and one dependent variable y.
</p>

<p>
Consider a collection of data points 
	<me>(x_1,y_1), (x_2,y_2), ... , (x_n,y_n)</me>
and a general linear function 
	<me>f(x) = mx + b.</me>
It is possible that each of the given data points are exactly "interpolated" by the linear function so that
	<me>f(x_k) = y_k</me>
for k = 1, 2, ... , n.  However, in general this is unlikely since even three points are not likely to be colinear. However, you may notice that the data points exhibit a linear tendency or that the underlying physics might suggest a linear model. If so, you may find it easier to predict values of y for given values of x using a linear approximation. Here you will investigate a method for doing so called "linear regression", "least-squares", or "best-fit line". 
</p>

<p>
But why even bother creating a formula (a line here) to approximate data that does not satisfy that formula? Remember that you would expect collected data to vary slightly as one repeatedly collects that data in the same way that you would expect to make a slightly different score on repeated attempts at exams on the same material. Creating a forumla that is close to your data gives a well-defined way to predict a y value for a given x value. This predictive behavior is illustrated in the exercise below.
</p>

<exercise><title>WeBWorK - best-fit line for approximation</title>
		<webwork source="Library/NAU/setStatistics/linear_regression1.pg">
		</webwork>
</exercise>

<p>
To determine the best-fit line, you need to determine what is meant by the word "best". Here, we will derive the standard approach which interprets this to mean that the total vertical error between the line and the provided data points is minimized in some fashion.  Indeed, this vertical error would be of the form
	<me>e_k = f(x_k) - y_k</me>
and would be zero if f(x) exactly interpolated at the given data point.  Note, some of these errors will be positive and some will be negative. To avoid any possible cancellation of errors, you can look at taking absolute values (which is tough to deal with algebraically) or by squaring the errors. This second option will be the approach taken here. This is similar to the approach taken earlier when developing formulas for the variance.
</p>

<p>The best-fit line therefore will be the line <m>f(x) = mx+b</m> so that the "total squared error" is minimized. This total squared error is given by
	<me>TSE(m,b) = \sum_{k=1}^n e_k^2 = \sum_{k=1}^n (f(x_k) - y_k)^2 = \sum_{k=1}^n (m x_k + b - y_k)^2.</me>
</p>

<p>
For the following interactive cell, consider for the given data points various values for the slope and y-intercept and see if you can make the total squared error as small as possible. In doing so, notice the vertical distances from the line to the given data points generally decreases as this error measure gets smaller. 
</p>


<sage>
<input>
var('x')
@interact
def _(Points = input_box([(-1,1),(3,1),(4,3),(6,4)]), 
             m = slider(-4,4,0.05,1),b = slider(-2,2,0.05,1)):   
    G = points(Points,size=20)
    xpt = []
    ypt = []
    f = m*x + b
    TSE = 0
    for k in range(len(Points)):
        x0 = Points[k][0]
        xpt.append(x0)
        y0 = Points[k][1]
        ypt.append(y0)
        TSE += (f(x=x0) - y0)^2
        G += line([(x0,f(x=x0)),(x0,y0)],color='orange')
    G += plot(f,x,min(xpt),max(xpt),color='gray')
    T = 'Total Squared Error = $%s$'%str(TSE)
    G.show(title = T)
</input>    
</sage>

<p>
So that we don't have to guess the best values for slope and intercept, we can appeal to calculus. Indeed, to minimize this function of the two variables m and b take partial derivatives and set them equal to zero to get the critical values:
	<me>TSE_m = \sum_{k=1}^n 2(m x_k + b - y_k) \cdot x_k</me>
and
	<me>TSE_b = \sum_{k=1}^n 2(m x_k + b - y_k) \cdot 1 .</me>
Setting equal to zero and solving gives what is known as the "normal equations":
	<me>m \sum_{k=1}^n x_k^2 + b \sum_{k=1}^n x_k = \sum_{k=1}^n x_k y_k</me>
and
	<me>m \sum_{k=1}^n x_k + b \sum_{k=1}^n 1 = \sum_{k=1}^n y_k.</me>
Solving these for m and b gives the best fit line.
</p>

<exercise><title>WeBWorK - Best fit line</title>
		<webwork source="Library/ASU-topics/setStat/dueck4_2_4.pg">
		</webwork>
</exercise>

</section>




<section xml:id="Correlation"><title>Correlation</title>
<p>
You can plot points and plot the resulting best-fit line determined in the previous section but the question remains whether the line is any good. In particular, the real use of the line often is to subsequently predict y-values for a given x-value. However, it is very likely that the best-fit line does not even pass through any of the provided data points.  So, how can something that misses every marker still be considered a good fit. To quantify this, we first need to discuss a way to measure how well two variables vary with each other.
</p>


<definition xml:id="DefnSampleCovariance"><title>Sample Covariance</title>
<p>Given paired data 
	<me>(x_1,y_1), (x_2,y_2), ... , (x_n,y_n)</me>
with corresponding means <m>\overline{x}</m> and <m>\overline{y}</m>, the sample covariance is given by
	<me> s_{xy} = \sum_{k=1}^n \frac{(x_k-\overline{x})(y_k-\overline{y})}{n-1}.</me>
</p>
</definition>


<p>
This general definition provides a general measure which is a second order term (like variance) but also maintains "units". To provide a unit-less metric, consider the following measure.
</p>

<definition xml:id="DefnCorrelationCoefficient"><title>Spearman's Rho (Correlation Coefficient)</title>
<statement>
<p>
Given a collection of data points, the Spearman's Rho correlation coefficient is given by
<me>r = \frac{s_{xy}}{s_x s_y}</me>
where <m>s_x</m> is the standard deviation of the x-values only and <m>s_y</m> is the standard deviation of the y-values only.
</p>
</statement>
</definition>



<theorem><title>Interpretation of the Spearman's Rho Correlation Coefficient</title>
<statement>
If the points are co-linear with a positive slope then r = 1 and if the points are co-linear with a negative slope then r = -1.
</statement>
<proof>
<p>
Assume the data points are co-linear with a positive slope. Then the 
	<me>TSE(m_0,b_0) = 0</me> 
for some <m>m_0</m> and <m>b_0</m>. For this line notice that <m>f(x_k) = y_k</m> exactly for all data points. 
It is easy to show then that 
	<me>\overline{y} = m_0 \overline{x} + b_0</me> 
and 
	<me>s_y = | m_0 | s_x</me>.

Therefore,
	<me>s_{xy} = \sum_{k=1}^n \frac{(x_k - \overline{x})(m_0 x_k + b_0 - (m_0 \overline{x} + b_0))}{n-1} = m_0 s_x^2</me>
Putting these together gives correlation coefficient
	<me>r = \frac{m_0 s_x^2}{s_x m_0 s_x} = 1.</me> 
A similar proof follows in the second case by noting that 
	<me>m_0 / | m_0 | = -1.</me>
</p>
</proof>
</theorem>



<theorem><title>Alternate formula for r</title>
<statement>
<p>
	<me>s_{xy} = \frac{n}{n-1} \left [ \frac{\sum_{k=1}^n x_k y_k}{n} - \overline{x} \overline{y} \right ].</me>
</p>
</statement>
<proof>
<p>
Expand <m>(x_k-\overline{x})(y_k-\overline{y})</m> and notice that the sample means are actually constants with respect to the summation.
</p>
</proof>
</theorem>


<p>
To compute the best-fit line and correlation coefficient by hand, it is often easiest to construct a table in a manner similar to how you might have computed the mean, variance, skewness and kurtosis in the previous chapter.  Indeed, 


<table halign="left">
      <tabular halign="right">
      
<row>
<cell bottom="medium"><m>x_k^2</m></cell>
<cell bottom="medium"><m>x_k</m></cell>
<cell bottom="medium"><m>y_k</m></cell>
<cell bottom="medium"><m>y_k^2</m></cell>
<cell bottom="medium"><m>x_k \cdot y_k</m></cell></row>      
<row><cell>4</cell><cell>-2</cell><cell>3</cell><cell>9</cell><cell>-6</cell></row>		
<row><cell>0</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>0</cell></row>		
<row><cell>0</cell><cell>0</cell><cell>-1</cell><cell>1</cell><cell>0</cell></row>		
<row><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row>		
<row><cell>9</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>3</cell></row>		
<row><cell>9</cell><cell>3</cell><cell>-1</cell><cell>1</cell><cell>-3</cell></row>		
<row>
<cell bottom="medium">16</cell>
<cell bottom="medium">4</cell>
<cell bottom="medium">0</cell>
<cell bottom="medium">0</cell>
<cell bottom="medium">0</cell>
</row>		
<row><cell>39</cell><cell>9</cell><cell>4</cell><cell>14</cell><cell>7</cell></row>		

      </tabular>
      <title>Computing best fit line and correlation coefficient by hand</title>
</table>

Therefore
<me>\overline{x} = \frac{9}{7}</me>
<me>\overline{y} = \frac{4}{7}</me>
<me>s_x^2 = \frac{7}{6} \cdot \left [ \frac{39}{7} - \left ( \frac{9}{7} \right )^2 \right ] = \frac{32}{7}</me>
<me>s_y^2 = \frac{7}{6} \cdot \left [ \frac{14}{7} - \left ( \frac{4}{7} \right )^2 \right ] = \frac{41}{21}</me>
<me>s_{xy} = \frac{7}{6} \cdot \left [ \frac{7}{7} - \frac{9}{7} \cdot \frac{4}{7} \right ] = \frac{13}{42} </me>
<me>r = \frac{\frac{13}{42}}{\sqrt{\frac{32}{7}} \sqrt{\frac{41}{21}}}</me>

and the normal equations to find the actual line are

<me> 39m + 9 b = 7 \text{  and  } 9m + 7b = 4.</me>
You should go ahead and simplify these values, solve the system of equations, plot the points and draw the line to complete the creation and evaluation of this best-fit line.
</p>


<exercise><title>WeBWorK - Interpreting correlation coefficients.</title>
		<webwork source="Library/CollegeOfIdaho/setStatistics_Ch04ScatterplotsAndCorrelation/04Stats_10_ScatterCorrelation.pg">
		</webwork>

</exercise>

<exercise><title>WeBWorK - Interpreting correlation coefficients again.</title>
		<webwork source="Library/CollegeOfIdaho/setStatistics_Ch04ScatterplotsAndCorrelation/04Stats_07_ScatterCorrelation.pg">
		</webwork>
</exercise>


<p>
Once again, we can use R to determine and display the data points and best-fit line with one of the built-in data sets. To do this, that data set needs to have the opportunity to utilize paired data. Below, we use the 'cars' data set since that set has speed paired with a corresponding stopping distance from some experiment. See if you can use one of the other data sets to create a best-fit line and determine if the correlation coefficient is sufficiently 'good' or not.
</p>


<sage language = 'r'>
<input>
data(cars)
x &lt; cars[,1]
y &lt;- cars[,2]
head = paste("Scatter Plot and Best-Fit Line for
       Speed vs Stoping Distance \n Correlation Coefficient = ",cor(x,y))   
pts = data.frame(x, y)
plot(pts,pch = 16, cex = 1.0, col = "blue", main = head, xlab = "x", ylab = "y")
lm(y ~ x)
abline(lm(y ~ x))
</input>
</sage>


<p>
Repeat the above R code but instead use two of the columns from EuStockMarkets for the data. Consider why you might get a relatively high correlation coefficient with this data. 
</p>

<p>
Again, repeat the above R code by instead use columns 2 and 4 from USJudgeRatings. Notice the relatively large correlation coefficient but ponder if there is really any connection between these two columns. Repeat for columns 1 and 4 and interpret the results.
</p>

</section>




<section xml:id="HigherDegreeRegression"><title>Higher Degree Regression</title>

<p>
Continuing in a similar fashion to the previous section, consider now an approximation using a quadratic function <m>f(x) = ax^2 + bx + c</m>.  In this case, the total squared error would be of the form
<me>TSE(a,b,c) = \sum_{k=0}^n (a x_k^2 + b x_k + c - y_k)^2.</me>
	
Taking all three partials gives

<me>TSE_a = \sum_{k=1}^n 2(a x_k^2 + b x_k + c - y_k) \cdot x_k^2</me>
<me>TSE_b = \sum_{k=1}^n 2(a x_k^2 + b x_k + c - y_k) \cdot x_k</me>
<me>TSE_c = \sum_{k=1}^n 2(a x_k^2 + b x_k + c - y_k) \cdot 1 .</me>
Once again, setting equal to zero and solving gives the normal equations for the best-fit quadratic
<me>a \sum_{k=1}^n x_k^4 + b \sum_{k=1}^n x_k^3 + c \sum_{k=1}^n x_k^2 = \sum_{k=1}^n x_k^2 y_k</me>
<me>a \sum_{k=1}^n x_k^3 + b \sum_{k=1}^n x_k^2 + c \sum_{k=1}^n x_k = \sum_{k=1}^n x_k y_k</me>
<me>a \sum_{k=1}^n x_k^2 + b \sum_{k=1}^n x_k + c \sum_{k=1}^n 1 = \sum_{k=1}^n y_k.</me>
</p>

<p>
One can easily use software to perform these calculations of course. Further, you can extend the ideas from above and derivate formulas for a best-fit cubic. The interactive cell below determines the optimal quadratic polynomial for a given set of data points and by commenting and uncommenting as suggested will also determine the best fit cubic.
</p>

<sage>
<input>
var('x')
xpts = vector([-1, 0, 1, 3, 5, 5])
ypts = vector([5, 3, 0, -1, 3, 6])
ones = vector([1, 1, 1, 1, 1, 1])
xpts3 = []
xpts2 = []
pts = []
for k in range(len(xpts)):
#    xpts3.append(xpts[k]^3)   # uncomment this for best cubic
    xpts2.append(xpts[k]^2)
    pts.append((xpts[k],ypts[k]))
# xpts3 = vector(xpts3)        # uncomment this for best cubic
xpts2 = vector(xpts2)

# For best cubic, instead uncomment the first and comment the second
# X = matrix([xpts3]).stack(xpts2).stack(xpts).stack(ones).transpose()
X = matrix([xpts2]).stack(xpts).stack(ones).transpose()

Y = matrix(ypts).transpose()
Xt = X.transpose()
print(X)

A = (Xt*X).inverse()*Xt*Y

# For best cubic, instead uncomment the first and comment the second
# [a,b,c,d] = [A[0][0],A[1][0],A[2][0],A[3][0]]
[a,b,c] = [A[0][0],A[1][0],A[2][0]]

# For best cubic, instead uncomment the first and comment the second
# f = a*x^3 + b*x^2 + c*x + d
f = a*x^2 + b*x + c

banner = "The interpolant is given by $%s$"%str(latex(f))
G = points(pts,size=20)
H = plot(f,x,min(xpts),max(xpts),title=banner)
show(G+H)
</input>
</sage>


<!--
<p>
One can also approach least squares using a linear algebra approach.  For example, using the quadratic model from above and evaluating at each of the given data points <m>(x_0,y_0), (x_1, y_2), ... , (x_n, y_n)</m>
</p>


-->
</section>




<section xml:id="MultiVariableRegressionExample"><title>Multi-variable Regression</title>
<p>
The regression models that we have looked at till now have always presumed a single independent variable and with "linear" coefficients. It is much more likely when investigating cause and effect relationships that there are perhaps many variables that contribute or perhaps using a non-linear relationship between the unknown coefficients.  To tease you to consider taking another course that covers multi-variate regression, in this section we briefly consider a two-variable model. We also consider an interesting example that illustrates the danger in using models to estimate values well beyond the range of the relevant data that has been used to create the model.
</p>

<p>
Consider then a model of the form
	<me>z = \alpha_1 x + \alpha_2 y + \beta</me>
and the data points 
	<me>(x_1,y_1,z_1), (x_2,y_2,z_2), ... , (x_n,y_n,z_n). </me>  

Evaluating at these data points gives, in matrix form

<me>
\begin{bmatrix}
z_1
\\ z_2
\\ ...
\\ z_n
\end{bmatrix}
=

\begin{bmatrix}
 x_1 \amp y_1 \amp 1 \\ 
 x_2 \amp y_2 \amp 1 \\ 
 ... \amp ... \amp ... \\ 
 x_n \amp y_n \amp 1 
\end{bmatrix} 

\cdot 

\begin{bmatrix}
\alpha_1
\\ 
\alpha_2
\\
\beta
\end{bmatrix}
+
\begin{bmatrix}
 \epsilon_1 \\ 
 \epsilon_2 \\ 
 ... \\ 
 \epsilon_n 
\end{bmatrix}
</me>
where the <m>\epsilon_k</m> terms are the deviation between the exact data point and the approximation of that point on some plane. Symbolically
<me>Z = XA + \epsilon.</me>
Unless all of the points lie on the same plane (unlikely) then when the vector <m>\epsilon = 0</m>, this system will overdetermined with more independent equations than unknowns. Applying a least squares solution approach is the same as minimizing <m> \epsilon^t \epsilon</m> and eventually gives
<me>A = (X^t X)^{-1} X^t Z</me>
in general.  Evaluating this with X and Z as above gives the matrix  
 
<me> A = \begin{bmatrix}
 \alpha_1 \\ 
 \alpha_2 \\  
 \beta 
\end{bmatrix}
</me>

</p>

<p>
A good example of the usefulness and limitations of multi-variate regression is the calculation of the "Heat Index". This measure determines a measure of discomfort relative to the ambient temperature and the relative humidity. Indeed, in warm climates a high temperature is more difficult to bear if the humidity is also high. One reason is that with high humidity the body is less effective in shedding heat through evaporation of body sweat.
</p>

<p>
The National Weather Service in 1990 published the following multiple regression equation for Heat Index (HI) relative to the ambient temperature (T) and the relative humidity (RH)
	<me>H = -42.379 + 2.04901523 \cdot T + 10.14333127 \cdot R - 0.22475541 \cdot T \cdot R \\ - 6.83783 \cdot 10^{-3} \cdot T^2 - 5.481717 \cdot 10^{-2} \cdot R^2 + 1.22874 \cdot 10^{-3} \cdot T^2 \cdot R \\ + 8.5282 \cdot 10^{-4} \cdot T \cdot R^2-1.99 \cdot 10^{-6} \cdot T^2 \cdot R^2.</me>
Notice, their model utilizes quadratic terms and therefore uses a generalization of the linear result presented above. Details on how this equation was determined and other details are available at https://www.wpc.ncep.noaa.gov/html/heatindex_equation.shtml .
</p>


<sage><title> Computing Heat Index Values</title>
<input>
@interact
def _(T = (90),R = (95)):
    H = (-42.379+2.04901523*T+10.14333127*R-0.22475541*T*R  
         -6.83783*10^(-3)*T^2-5.481717*10^(-2)*R^2+1.22874*10^(-3)*T^2*R 
         +8.5282*10^(-4)*T*R^2-1.99*10^(-6)*T^2*R^2)
    pretty_print(html("At %s"% str(T)+"$ ^o $ with %s"%str(R)
                          +" percent relative humidity,"))
    pretty_print(html("Heat Index =%s"%str(H)))
</input>
</sage>


<p>
Below one can compute a table for various ambient Temperature readings given one value for relative humidity.  Notice what happens for a relatively high humidity and relatively high temperature.
</p>


<sage><title> Computing Heat Index Table</title>
<input>
@interact
def _(R = input_box(default=95,width=10,label="Relative Humidity")):
    for T in range(80,121):
        H = (-42.379+2.04901523*T+10.14333127*R-0.22475541*T*R
              -6.83783*10^(-3)*T^2-5.481717*10^(-2)*R^2+1.22874*10^(-3)*T^2*R
              +8.5282*10^(-4)*T*R^2-1.99*10^(-6)*T^2*R^2)
        H = H.n(digits=4)
        pretty_print(html("At %s"%str(T)+"$ ^o $ with relative humidity"+ 
              " %s "%str(R)+"the Heat Index = %s "%str(H)+" degrees."))
</input>
</sage>


<p>
Indeed, you cannot roast a turkey by simply turning the oven on 120 and pumping in a lot of humidity since the turkey is not trying to cool itself anymore. Any discomfort measured on the turkey's behalf would certainly be matched by the human since the bird would remain very much uncooked. The issue is that this model doesn't presume the possibility of 120F and 95% humidity. Often, in situations where the temperature is able to reach that level, such as a desert, then the relative humidity is correspondingly low. This idea of using a model to predict extreme values beyond the measured data is called extrapolation and should be utilized with care. Interpolation to estimate values within the confines of the measured data is however generally a safe bet.
</p>
</section>



</chapter>
