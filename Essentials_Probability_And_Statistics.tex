%**************************************%
%*    Generated from PreTeXt source   *%
%*    on 2018-10-18T11:32:18-05:00    *%
%*                                    *%
%*   http://mathbook.pugetsound.edu   *%
%*                                    *%
%**************************************%
\documentclass[10pt,]{book}
%% Custom Preamble Entries, early (use latex.preamble.early)
%% Default LaTeX packages
%%   1.  always employed (or nearly so) for some purpose, or
%%   2.  a stylewriter may assume their presence
\usepackage{geometry}
%% Some aspects of the preamble are conditional,
%% the LaTeX engine is one such determinant
\usepackage{ifthen}
%% etoolbox has a variety of modern conveniences
\usepackage{etoolbox}
\usepackage{ifxetex,ifluatex}
%% Raster graphics inclusion
\usepackage{graphicx}
%% Color support, xcolor package
%% Always loaded, for: add/delete text, author tools
%% Here, since tcolorbox loads tikz, and tikz loads xcolor
\PassOptionsToPackage{usenames,dvipsnames,svgnames,table}{xcolor}
\usepackage{xcolor}
%% Colored boxes, and much more, though mostly styling
%% skins library provides "enhanced" skin, employing tikzpicture
%% boxes may be configured as "breakable" or "unbreakable"
%% "raster" controls grids of boxes, aka side-by-side
\usepackage{tcolorbox}
\tcbuselibrary{skins}
\tcbuselibrary{breakable}
\tcbuselibrary{raster}
%% xparse allows the construction of more robust commands,
%% this is a necessity for isolating styling and behavior
%% The tcolorbox library of the same name loads the base library
\tcbuselibrary{xparse}
%% Hyperref should be here, but likes to be loaded late
%%
%% Inline math delimiters, \(, \), need to be robust
%% 2016-01-31:  latexrelease.sty  supersedes  fixltx2e.sty
%% If  latexrelease.sty  exists, bugfix is in kernel
%% If not, bugfix is in  fixltx2e.sty
%% See:  https://tug.org/TUGboat/tb36-3/tb114ltnews22.pdf
%% and read "Fewer fragile commands" in distribution's  latexchanges.pdf
\IfFileExists{latexrelease.sty}{}{\usepackage{fixltx2e}}
%% Text height identically 9 inches, text width varies on point size
%% See Bringhurst 2.1.1 on measure for recommendations
%% 75 characters per line (count spaces, punctuation) is target
%% which is the upper limit of Bringhurst's recommendations
\geometry{letterpaper,total={340pt,9.0in}}
%% Custom Page Layout Adjustments (use latex.geometry)
%% This LaTeX file may be compiled with pdflatex, xelatex, or lualatex
%% The following provides engine-specific capabilities
%% Generally, xelatex and lualatex will do better languages other than US English
%% You can pick from the conditional if you will only ever use one engine
\ifthenelse{\boolean{xetex} \or \boolean{luatex}}{%
%% begin: xelatex and lualatex-specific configuration
%% fontspec package will make Latin Modern (lmodern) the default font
\ifxetex\usepackage{xltxtra}\fi
\usepackage{fontspec}
%% realscripts is the only part of xltxtra relevant to lualatex 
\ifluatex\usepackage{realscripts}\fi
%% 
%% Extensive support for other languages
\usepackage{polyglossia}
%% Main document language is US English
\setdefaultlanguage{english}
%% Spanish
\setotherlanguage{spanish}
%% Vietnamese
\setotherlanguage{vietnamese}
%% end: xelatex and lualatex-specific configuration
}{%
%% begin: pdflatex-specific configuration
%% translate common Unicode to their LaTeX equivalents
%% Also, fontenc with T1 makes CM-Super the default font
%% (\input{ix-utf8enc.dfu} from the "inputenx" package is possible addition (broken?)
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%% end: pdflatex-specific configuration
}
%% Monospace font: Inconsolata (zi4)
%% Sponsored by TUG: http://levien.com/type/myfonts/inconsolata.html
%% See package documentation for excellent instructions
%% One caveat, seem to need full file name to locate OTF files
%% Loads the "upquote" package as needed, so we don't have to
%% Upright quotes might come from the  textcomp  package, which we also use
%% We employ the shapely \ell to match Google Font version
%% pdflatex: "varqu" option produces best upright quotes
%% xelatex,lualatex: add StylisticSet 1 for shapely \ell
%% xelatex,lualatex: add StylisticSet 2 for plain zero
%% xelatex,lualatex: we add StylisticSet 3 for upright quotes
%% 
\ifthenelse{\boolean{xetex} \or \boolean{luatex}}{%
%% begin: xelatex and lualatex-specific monospace font
\usepackage{zi4}
\setmonofont[BoldFont=Inconsolatazi4-Bold.otf,StylisticSet={1,3}]{Inconsolatazi4-Regular.otf}
%% end: xelatex and lualatex-specific monospace font
}{%
%% begin: pdflatex-specific monospace font
%% "varqu" option provides textcomp \textquotedbl glyph
%% "varl"  option provides shapely "ell"
\usepackage[varqu,varl]{zi4}
%% end: pdflatex-specific monospace font
}
%% Symbols, align environment, bracket-matrix
\usepackage{amsmath}
\usepackage{amssymb}
%% allow page breaks within display mathematics anywhere
%% level 4 is maximally permissive
%% this is exactly the opposite of AMSmath package philosophy
%% there are per-display, and per-equation options to control this
%% split, aligned, gathered, and alignedat are not affected
\allowdisplaybreaks[4]
%% allow more columns to a matrix
%% can make this even bigger by overriding with  latex.preamble.late  processing option
\setcounter{MaxMatrixCols}{30}
%%
%% Semantic Macros
%% To preserve meaning in a LaTeX file
%% Only defined here if required in this document
%% Used for fillin answer blank
%% Argument is length in em
%% Length may compress for output to fit in one line
\newcommand{\fillin}[1]{\leavevmode\leaders\vrule height -1.2pt depth 1.5pt \hskip #1em minus #1em \null}
%% Subdivision Numbering, Chapters, Sections, Subsections, etc
%% Subdivision numbers may be turned off at some level ("depth")
%% A section *always* has depth 1, contrary to us counting from the document root
%% The latex default is 3.  If a larger number is present here, then
%% removing this command may make some cross-references ambiguous
%% The precursor variable $numbering-maxlevel is checked for consistency in the common XSL file
\setcounter{secnumdepth}{3}
%% begin: General AMS environment setup
%% Environments built with amsthm package
\usepackage{amsthm}
%% Numbering for Theorems, Conjectures, Examples, Figures, etc
%% Controlled by  numbering.theorems.level  processing parameter
%% Numbering: all theorem-like numbered consecutively
%% i.e. Corollary 4.3 follows Theorem 4.2
%% Always need some theorem environment to set base numbering scheme
%% even if document has no theorems (but has other environments)
%% Create a never-used style first, always
%% simply to provide a global counter to use, namely "cthm"
\newtheorem{cthm}{BadTheoremStringName}[section]
%% end: General AMS environment setup
%% AMS proof environment is basically fine as-is and special treatment
%% would certainly interfere with the functioning of \qed, etc.
%% So we simply localize the default heading
%% Redefinition of the "proof" environment is to cause a long alternate
%% title to line-break appropriately.  Code is cut verbatim, by suggestion,
%% from "Using the amsthm Package" Version 2.20.3, September 2017
\renewcommand*{\proofname}{Proof}
\makeatletter
\renewenvironment{proof}[1][\proofname]{\par
  \pushQED{\qed}%
  \normalfont \topsep6\p@\@plus6\p@\relax
  \trivlist
  \item\relax
    {\itshape
    #1\@addpunct{.}}\hspace\labelsep\ignorespaces
}{%
  \popQED\endtrivlist\@endpefalse
}
\makeatother
%% And a filled-in Halmos, making hollow available for examples
\renewcommand{\qedsymbol}{\(\blacksquare\)}
%%
%% tcolorbox, with styles, for THEOREM-LIKE
%%
%% theorem: fairly simple numbered block/structure
\tcbset{ theoremstyle/.style={size=minimal, boxrule=-0.3pt, frame empty, colback=white, colbacktitle=white, coltitle=black, fonttitle=\normalfont\bfseries, attach title to upper, after title={\space}, } }
\newtcolorbox[use counter=cthm, number within=section]{theorem}[3]{title={{Theorem~\thetcbcounter\notblank{#1#2}{\space}{}\notblank{#1}{\space#1}{}\notblank{#2}{\space(#2)}{}}}, label=#3, breakable, fontupper=\itshape, theoremstyle, }
%% corollary: fairly simple numbered block/structure
\tcbset{ corollarystyle/.style={size=minimal, boxrule=-0.3pt, frame empty, colback=white, colbacktitle=white, coltitle=black, fonttitle=\normalfont\bfseries, attach title to upper, after title={\space}, } }
\newtcolorbox[use counter=cthm, number within=section]{corollary}[3]{title={{Corollary~\thetcbcounter\notblank{#1#2}{\space}{}\notblank{#1}{\space#1}{}\notblank{#2}{\space(#2)}{}}}, label=#3, breakable, fontupper=\itshape, corollarystyle, }
%%
%% tcolorbox, with styles, for DEFINITION-LIKE
%%
%% definition: fairly simple numbered block/structure
\tcbset{ definitionstyle/.style={size=minimal, boxrule=-0.3pt, frame empty, colback=white, colbacktitle=white, coltitle=black, fonttitle=\normalfont\bfseries, attach title to upper, after title={\space}, after upper={\hfill{}\(\lozenge\)}, } }
\newtcolorbox[use counter=cthm, number within=section]{definition}[2]{title={{Definition~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, label=#2, breakable, definitionstyle, }
%%
%% tcolorbox, with styles, for EXAMPLE-LIKE
%%
%% example: fairly simple numbered block/structure
\tcbset{ examplestyle/.style={size=minimal, boxrule=-0.3pt, frame empty, colback=white, colbacktitle=white, coltitle=black, fonttitle=\normalfont\bfseries, attach title to upper, after title={\space}, after upper={\hfill{}\(\square\)}, } }
\newtcolorbox[use counter=cthm, number within=section]{example}[2]{title={{Example~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, label=#2, breakable, examplestyle, }
%%
%% tcolorbox, with styles, for inline exercises
%%
%% inlineexercise: fairly simple numbered block/structure
\tcbset{ inlineexercisestyle/.style={size=minimal, boxrule=-0.3pt, frame empty, colback=white, colbacktitle=white, coltitle=black, fonttitle=\normalfont\bfseries, attach title to upper, after title={\space}, } }
\newtcolorbox[use counter=cthm, number within=section]{inlineexercise}[2]{title={{Checkpoint~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, label=#2, breakable, inlineexercisestyle, }
%% Localize LaTeX supplied names (possibly none)
\renewcommand*{\chaptername}{Chapter}
%% Equation Numbering
%% Controlled by  numbering.equations.level  processing parameter
\numberwithin{equation}{section}
%% For improved tables
\usepackage{array}
%% Some extra height on each row is desirable, especially with horizontal rules
%% Increment determined experimentally
\setlength{\extrarowheight}{0.2ex}
%% Define variable thickness horizontal rules, full and partial
%% Thicknesses are 0.03, 0.05, 0.08 in the  booktabs  package
\makeatletter
\newcommand{\hrulethin}  {\noalign{\hrule height 0.04em}}
\newcommand{\hrulemedium}{\noalign{\hrule height 0.07em}}
\newcommand{\hrulethick} {\noalign{\hrule height 0.11em}}
%% We preserve a copy of the \setlength package before other
%% packages (extpfeil) get a chance to load packages that redefine it
\let\oldsetlength\setlength
\newlength{\Oldarrayrulewidth}
\newcommand{\crulethin}[1]%
{\noalign{\global\oldsetlength{\Oldarrayrulewidth}{\arrayrulewidth}}%
\noalign{\global\oldsetlength{\arrayrulewidth}{0.04em}}\cline{#1}%
\noalign{\global\oldsetlength{\arrayrulewidth}{\Oldarrayrulewidth}}}%
\newcommand{\crulemedium}[1]%
{\noalign{\global\oldsetlength{\Oldarrayrulewidth}{\arrayrulewidth}}%
\noalign{\global\oldsetlength{\arrayrulewidth}{0.07em}}\cline{#1}%
\noalign{\global\oldsetlength{\arrayrulewidth}{\Oldarrayrulewidth}}}
\newcommand{\crulethick}[1]%
{\noalign{\global\oldsetlength{\Oldarrayrulewidth}{\arrayrulewidth}}%
\noalign{\global\oldsetlength{\arrayrulewidth}{0.11em}}\cline{#1}%
\noalign{\global\oldsetlength{\arrayrulewidth}{\Oldarrayrulewidth}}}
%% Single letter column specifiers defined via array package
\newcolumntype{A}{!{\vrule width 0.04em}}
\newcolumntype{B}{!{\vrule width 0.07em}}
\newcolumntype{C}{!{\vrule width 0.11em}}
\makeatother
%% Figures, Tables, Listings, Named Lists, Floats
%% The [H]ere option of the float package fixes floats in-place,
%% in deference to web usage, where floats are totally irrelevant
%% You can remove some of this setup, to restore standard LaTeX behavior
%% HOWEVER, numbering of figures/tables AND theorems/examples/remarks, etc
%% may de-synchronize with the numbering in the HTML version
%% You can remove the "placement={H}" option to allow flotation and
%% preserve numbering, BUT the numbering may then appear "out-of-order"
%% Floating environments: http://tex.stackexchange.com/questions/95631/
\usepackage{float}
\usepackage{newfloat}
\usepackage{caption}%% Adjust stock table environment so that it no longer floats
\SetupFloatingEnvironment{table}{fileext=lot,placement={H},within=section,name=Table}
\captionsetup[table]{labelfont=bf}
%% http://tex.stackexchange.com/questions/16195
\makeatletter
\let\c@table\c@cthm
\makeatother
%% Program listing support: for listings, programs, and Sage code
\usepackage{listings}
%% We define the listings font style to be the default "ttfamily"
%% To fix hyphens/dashes rendered in PDF as fancy minus signs by listing
%% http://tex.stackexchange.com/questions/33185/listings-package-changes-hyphens-to-minus-signs
\makeatletter
\lst@CCPutMacro\lst@ProcessOther {"2D}{\lst@ttfamily{-{}}{-{}}}
\@empty\z@\@empty
\makeatother
\ifthenelse{\boolean{xetex}}{}{%
%% begin: pdflatex-specific listings configuration
%% translate U+0080 - U+00F0 to their textmode LaTeX equivalents
%% Data originally from https://www.w3.org/Math/characters/unicode.xml, 2016-07-23
%% Lines marked in XSL with "$" were converted from mathmode to textmode
\lstset{extendedchars=true}
\lstset{literate={ }{{~}}{1}{¡}{{\textexclamdown }}{1}{¢}{{\textcent }}{1}{£}{{\textsterling }}{1}{¤}{{\textcurrency }}{1}{¥}{{\textyen }}{1}{¦}{{\textbrokenbar }}{1}{§}{{\textsection }}{1}{¨}{{\textasciidieresis }}{1}{©}{{\textcopyright }}{1}{ª}{{\textordfeminine }}{1}{«}{{\guillemotleft }}{1}{¬}{{\textlnot }}{1}{­}{{\-}}{1}{®}{{\textregistered }}{1}{¯}{{\textasciimacron }}{1}{°}{{\textdegree }}{1}{±}{{\textpm }}{1}{²}{{\texttwosuperior }}{1}{³}{{\textthreesuperior }}{1}{´}{{\textasciiacute }}{1}{µ}{{\textmu }}{1}{¶}{{\textparagraph }}{1}{·}{{\textperiodcentered }}{1}{¸}{{\c{}}}{1}{¹}{{\textonesuperior }}{1}{º}{{\textordmasculine }}{1}{»}{{\guillemotright }}{1}{¼}{{\textonequarter }}{1}{½}{{\textonehalf }}{1}{¾}{{\textthreequarters }}{1}{¿}{{\textquestiondown }}{1}{À}{{\`{A}}}{1}{Á}{{\'{A}}}{1}{Â}{{\^{A}}}{1}{Ã}{{\~{A}}}{1}{Ä}{{\"{A}}}{1}{Å}{{\AA }}{1}{Æ}{{\AE }}{1}{Ç}{{\c{C}}}{1}{È}{{\`{E}}}{1}{É}{{\'{E}}}{1}{Ê}{{\^{E}}}{1}{Ë}{{\"{E}}}{1}{Ì}{{\`{I}}}{1}{Í}{{\'{I}}}{1}{Î}{{\^{I}}}{1}{Ï}{{\"{I}}}{1}{Ð}{{\DH }}{1}{Ñ}{{\~{N}}}{1}{Ò}{{\`{O}}}{1}{Ó}{{\'{O}}}{1}{Ô}{{\^{O}}}{1}{Õ}{{\~{O}}}{1}{Ö}{{\"{O}}}{1}{×}{{\texttimes }}{1}{Ø}{{\O }}{1}{Ù}{{\`{U}}}{1}{Ú}{{\'{U}}}{1}{Û}{{\^{U}}}{1}{Ü}{{\"{U}}}{1}{Ý}{{\'{Y}}}{1}{Þ}{{\TH }}{1}{ß}{{\ss }}{1}{à}{{\`{a}}}{1}{á}{{\'{a}}}{1}{â}{{\^{a}}}{1}{ã}{{\~{a}}}{1}{ä}{{\"{a}}}{1}{å}{{\aa }}{1}{æ}{{\ae }}{1}{ç}{{\c{c}}}{1}{è}{{\`{e}}}{1}{é}{{\'{e}}}{1}{ê}{{\^{e}}}{1}{ë}{{\"{e}}}{1}{ì}{{\`{\i}}}{1}{í}{{\'{\i}}}{1}{î}{{\^{\i}}}{1}{ï}{{\"{\i}}}{1}{ð}{{\dh }}{1}{ñ}{{\~{n}}}{1}{ò}{{\`{o}}}{1}{ó}{{\'{o}}}{1}{ô}{{\^{o}}}{1}{õ}{{\~{o}}}{1}{ö}{{\"{o}}}{1}{÷}{{\textdiv }}{1}{ø}{{\o }}{1}{ù}{{\`{u}}}{1}{ú}{{\'{u}}}{1}{û}{{\^{u}}}{1}{ü}{{\"{u}}}{1}{ý}{{\'{y}}}{1}{þ}{{\th }}{1}{ÿ}{{\"{y}}}{1}}
%% end: pdflatex-specific listings configuration
}
%% End of generic listing adjustments
%% Sage's blue is 50%, we go way lighter (blue!05 would work)
\definecolor{sageblue}{rgb}{0.95,0.95,1}
%% Sage input, listings package: Python syntax, boxed, colored, line breaking
%% To be flush with surrounding text's margins, set
%% xmargins to be sum of framerule, framesep, and epsilon (~0.25pt)
%% space between input/output comes from input style "belowskip",
%% by giving output an aboveskip of zero
\lstdefinestyle{sageinputstyle}{language=Python,breaklines=true,breakatwhitespace=true,%
basicstyle=\small\ttfamily,columns=fixed,frame=single,backgroundcolor=\color{sageblue},%
framerule=0.5pt,framesep=4pt,xleftmargin=4.75pt,xrightmargin=4.75pt}
%% Sage output, similar, but not boxed, not colored
\lstdefinestyle{sageoutputstyle}{language=Python,breaklines=true,%
breakatwhitespace=true,basicstyle=\small\ttfamily,columns=fixed,aboveskip=0pt}
%% The environments manufactured by the listings package
\lstnewenvironment{sageinput}
  {\lstset{style=sageinputstyle}}
  {}
\lstnewenvironment{sageoutput}
  {\lstset{style=sageoutputstyle}}
  {}
%% Fancy Verbatim for consoles, preformatted, code display
\usepackage{fancyvrb}
%% Pre-formatted text, a peer of paragraphs
\DefineVerbatimEnvironment{preformatted}{Verbatim}{}
%% More flexible list management, esp. for references
%% But also for specifying labels (i.e. custom order) on nested lists
\usepackage{enumitem}
%% hyperref driver does not need to be specified, it will be detected
\usepackage{hyperref}
%% configure hyperref's  \url  to match listings' inline verbatim
\renewcommand\UrlFont{\small\ttfamily}
%% Hyperlinking active in PDFs, all links solid and blue
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,filecolor=blue,urlcolor=blue}
\hypersetup{pdftitle={Essentials of Mathematical Probability and Statistics}}
%% If you manually remove hyperref, leave in this next command
\providecommand\phantomsection{}
%% Graphics Preamble Entries
\usepackage{tikz}
\usetikzlibrary{backgrounds}
\usetikzlibrary{arrows,matrix}
\usetikzlibrary{snakes}
%% If tikz has been loaded, replace ampersand with \amp macro
%% tcolorbox styles for sidebyside layout
\tcbset{ sbsstyle/.style={raster equal height=rows,raster force size=false} }
\tcbset{ sbsheadingstyle/.style={size=minimal,halign=center,fontupper=\bfseries,colback=white,frame empty} }
\tcbset{ sbspanelstyle/.style={size=minimal,colback=white,frame empty} }
\tcbset{ sbscaptionstyle/.style={size=minimal,halign=center,colback=white,frame empty} }
%% Enviroments for side-by-side and components
%% Necessary to use \NewTColorBox for boxes of the panels
%% "newfloat" environment to squash page-breaks within a single sidebyside
%% \leavevmode necessary when a side-by-side comes first, right after a heading
%% "xparse" environment for entire sidebyside
\NewDocumentEnvironment{sidebyside}{mmmm}
  {\begin{tcbraster}
    [sbsstyle,raster columns=#1,
    raster left skip=#2\linewidth,raster right skip=#3\linewidth,raster column skip=#4\linewidth]}
  {\end{tcbraster}}
%% "tcolorbox" environments for three components of a panel
\NewTColorBox{sbsheading}{m}{sbsheadingstyle,width=#1\linewidth}
\NewTColorBox{sbspanel}{mO{top}}{sbspanelstyle,width=#1\linewidth,valign=#2}
\NewTColorBox{sbscaption}{m}{sbscaptionstyle,width=#1\linewidth}
%% extpfeil package for certain extensible arrows,
%% as also provided by MathJax extension of the same name
%% NB: this package loads mtools, which loads calc, which redefines
%%     \setlength, so it can be removed if it seems to be in the 
%%     way and your math does not use:
%%     
%%     \xtwoheadrightarrow, \xtwoheadleftarrow, \xmapsto, \xlongequal, \xtofrom
%%     
%%     we have had to be extra careful with variable thickness
%%     lines in tables, and so also load this package late
\usepackage{extpfeil}
%% Custom Preamble Entries, late (use latex.preamble.late)
%% Begin: Author-provided packages
%% (From  docinfo/latex-preamble/package  elements)
%% End: Author-provided packages
%% Begin: Author-provided macros
%% (From  docinfo/macros  element)
%% Plus three from MBX for XML characters

\newcommand{\lt}{<}
\newcommand{\gt}{>}
\newcommand{\amp}{&}
%% End: Author-provided macros
\begin{document}
\frontmatter
%% begin: half-title
\thispagestyle{empty}
{\centering
\vspace*{0.28\textheight}
{\Huge Essentials of Mathematical Probability and Statistics}\\[2\baselineskip]
{\LARGE A First Course For the Mathematically Inclined}\\
}
\clearpage
%% end:   half-title
%% begin: adcard
\thispagestyle{empty}
\null%
\clearpage
%% end:   adcard
%% begin: title page
%% Inspired by Peter Wilson's "titleDB" in "titlepages" CTAN package
\thispagestyle{empty}
{\centering
\vspace*{0.14\textheight}
%% Target for xref to top-level element is ToC
\addtocontents{toc}{\protect\hypertarget{Essentials_Probability_And_Statistics}{}}
{\Huge Essentials of Mathematical Probability and Statistics}\\[\baselineskip]
{\LARGE A First Course For the Mathematically Inclined}\\[3\baselineskip]
{\Large John Travis}\\[0.5\baselineskip]
{\Large Mississippi College}\\[3\baselineskip]
{\Large }\\[0.5\baselineskip]
[3\baselineskip]
{\Large October 18, 2018}\\}
\clearpage
%% end:   title page
%% begin: copyright-page
\thispagestyle{empty}
\hypertarget{colophon-1}{}\noindent
\hypertarget{p-1}{}%
John Travis grew up in Mississippi and had his graduate work at the University of Tennessee and Mississippi State University. As a numerical analyst, since 1988 he has been a professor of mathematics at his undergraduate alma mater Mississippi College where he currently serves as Professor and Chair of Mathematics.%
\par
\hypertarget{p-2}{}%
You can find him playing racquetball or guitar but not generally at the same time. He is also an active supporter and organizer for the opensouce online homework system WeBWorK.%
\par
\vspace*{\stretch{2}}
\noindent\textcopyright\ 2016\textendash{}today\quad{}John Travis\\[0.5\baselineskip]
Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or any later version published by the Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.  A copy of the license is included in the appendix entitled ``GNU Free Documentation License.''\par\medskip
\vspace*{\stretch{1}}
\null\clearpage
%% end:   copyright-page
%% begin: preface
\chapter*{Preface}\label{preface-1}
\addcontentsline{toc}{chapter}{Preface}
\hypertarget{p-3}{}%
This text is intended for a one-semester calculus-based undergraduate course in probability and statistics .%
\par
\hypertarget{p-4}{}%
A collection of WeBWorK online homework problems are available to correlate with the material in this text. Copies of these sets of problems are available by contacting the author.%
\par
\hypertarget{p-5}{}%
WeBWorK (\href{http://webwork.maa.org}{webwork.maa.org}) is an open-source online homework system for math and science courses. WeBWorK is supported by the MAA and the NSF and comes with a Open Problem Library (OPL) of over 35,000 homework problems. Problems in the OPL target most lower division undergraduate math courses and some advanced courses. Supported courses include college algebra, discrete mathematics, probability and statistics, single and multivariable calculus, differential equations, linear algebra and complex analysis.%
\par
\hypertarget{p-6}{}%
Sage (\href{http://sagemath.org}{sagemath.org}) is a free, open source, software system for advanced mathematics, which is ideal for assisting with a study of abstract algebra. Sage can be used either on your own computer, a local server, or on SageMathCloud (\href{https://cloud.sagemath.com}{https://cloud.sagemath.com}).%
\par\hfill\begin{tabular}{l@{}}
John Travis\\
Clinton, Mississippi  2015
\end{tabular}\\\par
%% end:   preface
%% begin: table of contents
%% Adjust Table of Contents
\setcounter{tocdepth}{1}
\renewcommand*\contentsname{Contents}
\tableofcontents
%% end:   table of contents
\mainmatter
%
%
\typeout{************************************************}
\typeout{Chapter 1 Statistical Measures}
\typeout{************************************************}
%
\chapter[{Statistical Measures}]{Statistical Measures}\label{RepresentingData}
%
%
\typeout{************************************************}
\typeout{Section 1.1 Introduction}
\typeout{************************************************}
%
\section[{Introduction}]{Introduction}\label{section-1}
\hypertarget{p-7}{}%
To compute your final grade in a class your teacher will likely consider the scores you have earned on various assignments and examinations completed during the duration of the course. However, she ultimately will likely be required to assign some numerical score indicating your level of success in the course. One grade to rule them all. This final grade can only be one value and it would make sense that the grade be a reflection of your work on these tasks. So, what is a fair way for your teacher to complete this task?%
\par
\hypertarget{p-8}{}%
Through this process, you will also often need to take into account whether that data set is the entire list of possibilities--known as the population--or just a subset of that population perhaps obtained by taking repeated measurements --that is, a sample.%
\par
\hypertarget{p-9}{}%
In general, it is often useful to make decisions using quantitative data but making those decisions can be somewhat arbitrary without a mathematical basis supporting those decisions. In this chapter, you will consider a number of ways to use point values to represent a given set of data. Each of these quantitative metrics will be called a "statistical measure" and will, in some fashion, describe using one number some property of the entire data set. Such measures are part of what is known as "descriptive statistics".  Later, you will learn about how other metrics can be used to predict properties of the underlying situation. Doing this is part of what is known as "inferential statistics".%
\par
\hypertarget{p-10}{}%
So, let's go and hopefully you will in some measure enjoy the ride!%
%
%
\typeout{************************************************}
\typeout{Section 1.2 Measurement Scales}
\typeout{************************************************}
%
\section[{Measurement Scales}]{Measurement Scales}\label{section-2}
\hypertarget{p-11}{}%
In creating statistical measures, you might want to consider one of the following general types.%
\par
\hypertarget{p-12}{}%
\leavevmode%
\begin{itemize}[label=\textbullet]
\item{}\hypertarget{p-13}{}%
Nominal measures - In this case, data falls into mutually exclusive and exhaustive categories for which the numerical value is only used for identification purposes. For example, assigning Male = 1, Female = -1.%
\item{}\hypertarget{p-14}{}%
Ordinal measures - In this case, data consists of discrete numerical values which can be ranked from lowest to highest or vice versa. For example, your grades in a number of classes are used to compute your GPA--which is a single number.%
\item{}\hypertarget{p-15}{}%
Interval measures - In this case, data possesses an order and where the distance between data values is of significance. For example, heights and weights.%
\item{}\hypertarget{p-16}{}%
Ratio measures - In this case, data can be expressed as a position in some interval and where ratios between observations have meaning. For example, percentile rankings%
\end{itemize}
%
\par
\hypertarget{p-17}{}%
In the subsequent sections of this chapter, you will see that a number of different measures are available for most data sets. Determining which "correct" measure to use for describing any given data set will depend the actual situation surrounding the collection of the data.%
%
%
\typeout{************************************************}
\typeout{Section 1.3 Statistical Measures of Position}
\typeout{************************************************}
%
\section[{Statistical Measures of Position}]{Statistical Measures of Position}\label{section-3}
\hypertarget{p-18}{}%
Given a collection of data, sorting the data may provide several useful descriptors. When sorting data, you can easily use something like a spreadsheet for larger data sets but in this section you will also see there are ways to perform a sort by hand. In either case, statistical measures of position generally involve very little computational work once the data is sorted and take into account only the order of the data from lowest to highest.  To assist with notation, we will generally use x-values to represent the original raw data and y-values to represent that same data when ordered with the subscript indicating the positional placement.%
\par
\hypertarget{p-19}{}%
\begin{definition}{Order Statistic.}{definition-1}%
\hypertarget{OrderStatistics}{}%
From the data set \(x_1, x_2, ... , x_n\), assume that when sorted it is denoted \(y_1, y_2, ..., y_n\) where%
\begin{equation*}
y_1 \le y_2 \le ... \le y_n.
\end{equation*}
Then, \(y_k\) is known as the kth order statistic.%
\end{definition}
%
\par
\hypertarget{p-21}{}%
\begin{example}{Age of Presidents - order statistics.}{PresidentAge}%
\hypertarget{p-22}{}%
The age at inauguration for presidents from 1981-2019 gives the data \(x_1 = 69, x_2 = 64, x_3 = 46, x_4 = 54, x_5 = 47, x_6 = 70\) (Reagan, Bush, Clinton, Bush, Obama, Trump). For this data, the order statistics are denoted \(y_1 = 46, y_2 = 47, y_3 = 54, y_4 = 64, y_5 = 69, y_6 = 71\).%
\end{example}
%
\par
\hypertarget{p-23}{}%
Once the data is sorted, it should be very easy for you to locate the smallest and largest values.%
\par
\hypertarget{p-24}{}%
\begin{definition}{Minimum/Maximum:.}{definition-2}%
\hypertarget{MaxMin}{}%
For a given data set, the smallest and largest values are known as the minimum and maximum, respectively. In our notation, minimum = \(y_1\) and the maximum = \(y_n\)%
\end{definition}
%
\par
\hypertarget{p-26}{}%
\begin{example}{Age of Presidents - Minimum/Maximum.}{example-2}%
\hypertarget{p-27}{}%
Using the \hyperref[PresidentAge]{President inauguration data~\ref{PresidentAge}}, minimum = \(y_1 = 46\) and maximum = \(y_6 = 70\).%
\end{example}
%
\par
\hypertarget{p-28}{}%
A value that separates ordered data into two groups with a desired percentage on each side is called a percentile. There are multiple ways that have been created that achieve this goal. In this text we present two and will consistently use the first one presented below. For each, in general, a given percentile is a numerical value at which approximately a given percentage of the data is smaller.%
\par
\hypertarget{p-29}{}%
The definition presented below provides for a unique measure for each unique value of p that corresponds to the PERCENTILE.EXC macro in Excel.  This version starts by computing \((n+1)p\) where \(0 < p < 1\) and using this to linearly interpolate between two adjacent entries in the sorted list.  Another option that corresponds to PERCENTILE.INC (and PERCENTILE) in Excel is to start with \((n-1)p+1\) for determining how to pick the two adjacent entries and then proceeding with linear interpolation. Again, the definition below utilizes the first approach.%
\par
\hypertarget{p-30}{}%
\begin{definition}{Percentiles.}{PercentileDefn}%
\hypertarget{Percentiles}{}%
For \(0 \lt s \lt 1\) and for order statistics \(y_1, y_2, ..., y_n\) define the 100s-th percentile to be%
\begin{equation*}
P^{s} = (1-r)y_m + ry_{m+1}
\end{equation*}
where m is the integer part of (n+1)s, namely%
\begin{equation*}
m = \left\lfloor (n+1)s \right\rfloor
\end{equation*}
and%
\begin{equation*}
r = (n+1)s - m,
\end{equation*}
the fractional part of (n+1)s.%
\end{definition}
%
\par
\hypertarget{p-32}{}%
 \begin{inlineexercise}{}{exercise-1}%
\hypertarget{p-33}{}%
Compute the following percentile values.%
\par\medskip
\hypertarget{p-34}{}%
Consider the following data set:%
\begin{equation*}
\begin{array}{ccccccccc} 
39 \amp  20 \amp  36 \amp  27 \amp  51 \amp  27 \amp  12 \amp  16 \amp  12\\
51 \amp  39 \amp  28 \amp  51 \amp  13 \amp  27 \amp  34 \amp  49 \amp  40  
\end{array}
\end{equation*}
%
\par
\hypertarget{p-35}{}%
Find the 15th and 88th percentiles for this data.%
\par
\hypertarget{p-36}{}%
15th percentile =  \fillin{15}%
\par
\hypertarget{p-37}{}%
88th percentile =  \fillin{15}%
\par\medskip\noindent \hypertarget{p-38}{}%
%
\par
\end{inlineexercise}
%
\par
\hypertarget{p-39}{}%
\begin{example}{Presidential Percentile.}{example-3}%
\hypertarget{p-40}{}%
To compute, say, the 42nd percentile for the \hyperref[PresidentAge]{President inauguration data presented earlier~\ref{PresidentAge}} consider s = 0.42. Since there are 6 numbers in our data set, then%
\begin{equation*}
(n+1)s = 7 \cdot 0.42 = 2.94
\end{equation*}
and so m = 2 and r = 0.94.  Thus, the percentile will lie between \(y_2 = 47\) and \(y_3 = 54\) and much closer to 54 than 47.  Numerically%
\begin{equation*}
P^{0.42} = 0.06 \cdot 47 + 0.94 \cdot 54 = 53.58.
\end{equation*}
%
\end{example}
%
\par
\hypertarget{p-41}{}%
The formula for percentiles determines a weighted average between \(y_m\) and \(y_{m+1}\) which is unique for distinct values of p provided each of the data values are distinct. Note that if some of the y-values are equal then some of these averages might be averages of equal numbers and will therefore be the common value.%
\par
\hypertarget{p-42}{}%
Some special percentiles are provided special names...%
\par
\hypertarget{p-43}{}%
\begin{definition}{Quartiles.}{definition-4}%
\hypertarget{Quartiles}{}%
Given a sorted data set, the first, second, and third quartiles are the values of \(Q_1 = P^{0.25}, Q_2 = P^{0.5}\) and \(Q_3 = P^{0.75}\).%
\end{definition}
%
\par
\hypertarget{p-45}{}%
It should be noted that many graphing calculators often compute quartiles using a straight average of two adjacent entries rather than by using the formula above. This causes some difficulty and especially so when n mod 4 = 2.%
\par
\hypertarget{p-46}{}%
\begin{example}{\(Q_1\) and \(Q_3\) when n mod 4 = 2.}{example-4}%
\hypertarget{p-47}{}%
Suppose n = 22 = 5(4) + 2. Computing the first quartile as defined above gives (n+1)p = 23(0.25) = 5.75 = 5 + 0.75 = m + r.  Therefore,%
\begin{equation*}
Q_1 = 0.25 \times y_5 + 0.75 \times y_6
\end{equation*}
which is a value closer to \(y_6\).  Many graphing calculators however quickly approximate this with%
\begin{equation*}
0.5 \times y_5 + 0.5 \times y_6
\end{equation*}
so you should be aware of this possible difference.  You should also notice that in this case p = 0.25 but r = 0.75 so these values are not required to be the same.%
\end{example}
%
\par
\hypertarget{p-48}{}%
\begin{definition}{Deciles:.}{definition-5}%
\hypertarget{Deciles}{}%
Given a sorted data set, the first, second, ..., ninth deciles are the value of \(D_1 = P^{0.1}, D_2 = P^{0.2}, ... , D_9 = P^{0.9}\)%
\end{definition}
%
\par
\hypertarget{p-50}{}%
\begin{example}{Small Example - Quartiles.}{example-5}%
\hypertarget{p-51}{}%
Consider the following data set: {2,5,8,10}. The 50th percentile should be a numerical value for which approximately 50% of the data is smaller. In this case, that would be some number between 5 and 8.  For now, let's just take 6.5 so that two numbers in the set lie below 6.5 and two lie above. This is a perfect 50% for the 50th percentile. In a similar manner, the 25th percentile would be some number between 2 and 5, say 2.75, so that one number lies below 2.75 and three numbers lie above.%
\par
\hypertarget{p-52}{}%
Using the \hyperref[PercentileDefn]{definition~\ref{PercentileDefn}}, the 25th percentile is computed by considering%
\begin{equation*}
(n+1)p = (4+1)0.25 = 5/4 = 1.25\text{.}
\end{equation*}
So, m = 1 and r = 0.25. Therefore%
\begin{equation*}
P^{0.25} = 0.75 \times 2 + 0.25 \times 5 = 2.75
\end{equation*}
as noted above.%
\par
\hypertarget{p-53}{}%
Similarly, the 75th percentile is given by%
\begin{equation*}
(n+1)p = (4+1)0.75 = 15/4 = 3.75\text{.}
\end{equation*}
So, m = 3 and r = 0.75. Therefore%
\begin{equation*}
P^{0.75} = 0.25 \times 8 + 0.75 \times 10 = 9.5
\end{equation*}
It is interesting to note that 3 also lies between 2 and 5 as does 2.75 and has the same percentages above (75 percent) and below (25 percent). However, it should designate a slightly larger percentile location. Indeed, going backward:%
\begin{gather*}
3 = (1-r) \times 2 + r \times 5\\
\Rightarrow r = \frac{1}{3}\\
\Rightarrow (n+1)p = 1 + \frac{1}{3} = \frac{4}{3}\\
\Rightarrow p = \frac{4}{15} \approx 0.267
\end{gather*}
and so 3 would actually be at approximately the 26.7th percentile.%
\end{example}
%
\par
\hypertarget{p-54}{}%
\begin{inlineexercise}{}{exercise-2}%
\hypertarget{p-55}{}%
In general, given a numerical value within the range of a given data set, one can determine the percentile ranking of that value by reversing the general formula for percentile and solving for p, given \(P^s\).  Determine such a formula/process for doing this in general.%
\end{inlineexercise}
%
\par
\hypertarget{p-56}{}%
For your data set {2,5,8,10}, \(Q_1 = 2.75, Q_2 = 6.5,\) and \(Q_3 = 9.5\).%
\par
\hypertarget{p-57}{}%
For a given data set, a summary of these statistics is often desired in order to give the user a quick overview of the more important order statistics.%
\par
\hypertarget{p-58}{}%
\begin{definition}{5-number summary.}{definition-6}%
\hypertarget{FiveNumberSummary}{}%
Given a set of data, the 5-number summary is a vector of the order statistics given by%
\begin{equation*}
\lt \text{minimum}, Q_1, Q_2, Q_3, \text{maximum} \gt .
\end{equation*}
%
\end{definition}
%
\par
\hypertarget{p-60}{}%
You can also compute these statistics automatically using the opensource statistical software known simply as "R".  The following interactive cell uses the opensource software "Sage" to perform this calculation using the freely available web portal at sagemath.sagecell.org. You can change the data list if you want to use this to compute values for a different collections of numbers. The five-number-summary is displayed graphically using a "Box-Plot". Graphical representations of data will be discussed later in this chapter. You should compare the answers found using R with the values produced by our \hyperref[PercentileDefn]{definition~\ref{PercentileDefn}}%
\par
\hypertarget{p-61}{}%
\leavevmode%
\begin{sageinput}
data <- c( 1, 2, 5, 7, 7, -1, 3, 2)   # concatenate the following items into a list
paste("Quartiles:")
quantile(data)
paste("Specific Percentiles:")
quantile(data, c(.32, .57, .98))   # find the 32nd, 57th and 98th percentiles
paste("Box and Whisker Diagram:")
boxplot(data, horizontal=TRUE)
\end{sageinput}
%
\par
\hypertarget{p-62}{}%
\begin{example}{Small example - 5 number summary.}{example-6}%
\hypertarget{p-63}{}%
Returning to our previous example, the five number summary would be%
\begin{equation*}
\lt 2, 2.75, 6.5, 9.5, 10 \gt .
\end{equation*}
%
\end{example}
%
%
%
\typeout{************************************************}
\typeout{Section 1.4 Statistical Measures of the Middle}
\typeout{************************************************}
%
\section[{Statistical Measures of the Middle}]{Statistical Measures of the Middle}\label{section-4}
\begin{definition}{Arithmetic Mean.}{definition-7}%
\hypertarget{Mean}{}%
Suppose X is a discrete random variable with range \(R = {x_1, x_2, ..., x_n}\). The arithmetic mean is given by%
\begin{equation*}
\frac{x_1 + ... + x_n}{n} = \frac{\sum_{k=1}^n x_k}{n}.
\end{equation*}
If this data comes from sample data then we call it a sample mean and denote this value by \(\overline{x}\). If this data comes from the entire universe of possibilities then we call it a population mean and denote this value by \(\mu\).  When presented with raw data, it might be good to generally presume that data comes from a sample and utilize \(\overline{x}\).%
\end{definition}
\hypertarget{p-65}{}%
To illustrate, consider the previous data set: {2,5,8,10}. The arithmetic mean is given by%
\begin{equation*}
\overline{x} = \frac{2+5+8+10}{4} = \frac{25}{4} = 6.25.
\end{equation*}
%
\par
\hypertarget{p-66}{}%
The mean is often called the centroid in the sense that if the x values were locations of objects of equal weight, then the centroid would be the point where this system of n equal masses would balance. Play around with the interactive cell below by entering your own data values into the first list.%
\par
\hypertarget{p-67}{}%
\leavevmode%
\begin{sageinput}
x = [2, 5, 8, 10, 11]     # Put your data values in this list
x.sort()

mu = mean(x)
n = len(x)
pts = [(x[0],0.05)]
M = 0.2
for k in range(1,n):
    if x[k]==x[k-1]:
        pts.append((x[k],pts[k-1][1]+0.1))
        M += 0.1
    else:
        pts.append((x[k],0.05))
G = points(pts,size=100,figsize=[10,2])
G += polygon([(mu,0), (mu+0.2,-0.5), (mu-0.2,-0.5)],color='brown')
G.show(ymin=-0.5, ymax = M)
\end{sageinput}
%
\par
\hypertarget{p-68}{}%
The values can all be provided with varying weights if desired and the result is called the weighted arithmetic mean and is given by%
\begin{equation*}
\frac{m_1 x_1 + ... + m_n x_n}{m_1 + ... + m_n} = \frac{\sum_{k=1}^n m_k x_k}{\sum_{k=1}^n m_k}.
\end{equation*}
This is often how your teacher will actually compute your final grade in a class where the \(m_k\) are the relative weights for each assignment grade.%
\par
\hypertarget{p-69}{}%
\leavevmode%
\begin{sageinput}
x = [2, 5, 8, 10]     # Put _unique_ data values in this list
w = [1, 2.5, 2.5, 4]  # Scale to be at most 10 and not tiny for good image
wsum = sum(w)

n = len(x)
pts = [(x[0],0.05)]
M = 0.2
mu = 0
for k in range(1,n):
    mu += x[k]*w[k]
    if x[k]==x[k-1]:
        pts.append((x[k],pts[k-1][1]+0.1))
        M += 0.2
    else:
        pts.append((x[k],0.05))
mu = mu/wsum
G = Graphics()
for k in range(n):
    G += point(pts[k],size=100*w[k]) 
P = polygon([(mu,0), (mu+0.2,-0.5), (mu-0.2,-0.5)],color='brown')
(G+P).show(ymin=-0.5, ymax = M)
\end{sageinput}
%
\par
\hypertarget{p-70}{}%
\begin{example}{Computing class final grade.}{example-7}%
\hypertarget{p-71}{}%
Suppose in a given class you have a daily grade of 92, exam 1 grade of 85, exam 2 grade of 87, and a final exam grade of 93.  IF the daily grade counts 10 percent, the first two exams count 25 percent each and the final counts 40 percent then your final grade would be%
\begin{equation*}
\frac{0.10 \cdot 92 + 0.25 \cdot 85 + 0.25 \cdot 87 + 0.40 \cdot 0.93}{0.10 + 0.25 + 0.25 + 0.40} = 89.4 .
\end{equation*}
It would then appear that you might want to do some bargaining with your teacher about how nice it would be to round that up.%
\end{example}
%
\par
\hypertarget{p-72}{}%
\begin{definition}{Median:.}{definition-8}%
\hypertarget{Median}{}%
A positional measure of the middle is often utilized by finding the location of the 50th percentile. This value is also called the median and indicates the value at which approximately half the sorted data lies below and half lies above.%
\end{definition}
%
\par
\hypertarget{p-74}{}%
For data sets with an odd number of values, this is the "middle" data value if one were to successively cross off pairs from the two ends of the sorted data. For data sets with an even number of values, this is a average of the two data values left after crossing off all other pairs.  Using the order statistics, the median equals%
\begin{equation*}
y_{\frac{n+1}{2}}
\end{equation*}
if n is odd and%
\begin{equation*}
\frac{y_\frac{n}{2} + y_{\frac{n}{2}+1}}{2}
\end{equation*}
if n is even.%
\par
\hypertarget{p-75}{}%
From the \hyperref[PresidentAge]{Presidential data~\ref{PresidentAge}}, note that you are considering an even number of data values and so the median is given by (54+64)/2 = 59.%
\par
\hypertarget{p-76}{}%
\begin{definition}{Midrange:.}{definition-9}%
\hypertarget{Midrange}{}%
The midrange is a mixture of the mean and median where one takes the simple average of the maximum and minimum values in the data set. Using the order statistics, this equals%
\begin{equation*}
\frac{y_1+y_n}{2}
\end{equation*}
%
\end{definition}
%
\par
\hypertarget{p-78}{}%
From the \hyperref[PresidentAge]{Presidential data~\ref{PresidentAge}}, the maximum is 70 and the minimum is 46 so the midrange is 58, the average of these two.%
\par
\hypertarget{p-79}{}%
There are several advantages and disadvantages associated with each of these measures. The mean utilizes all of the data values so each term is important. Utilizes them all even if some of the data values might suffer from collection errors. The median ignores outliers (which might be a result of collection errors) but does not account for the relative differences between terms. The midrange is very easy to compute but ignores the relative differences for all terms but the two extremes. A similar collection of features and drawbacks are associated with all descriptive statistics.%
\par
\hypertarget{p-80}{}%
You can again compute many statistics automatically using R... \leavevmode%
\begin{sageinput}
data <- c( 1, 2, 5, 7, 7, -1, 3, 2)   # concatenate the following items into a list
paste("Mean = ", mean(data))
paste("Median =", median(data))
\end{sageinput}
%
\par
\hypertarget{p-81}{}%
\begin{example}{USA State Population Measures of the Middle.}{StatePopulations}%
\hypertarget{p-82}{}%
The US Census Bureau reported the following state populations (in millions) for 2013: \href{Data/USA_States_Populations_2014.xlsx}{Spreadsheet}%
\par
\hypertarget{p-83}{}%
\leavevmode%
\begin{table}
\centering
\begin{tabular}{rr}
State&Population\tabularnewline\hrulemedium
Wyoming&0.6\tabularnewline[0pt]
Vermont&0.6\tabularnewline[0pt]
District of Columbia&0.6\tabularnewline[0pt]
North Dakota&0.7\tabularnewline[0pt]
Alaska&0.7\tabularnewline[0pt]
South Dakota&0.8\tabularnewline[0pt]
Delaware&0.9\tabularnewline[0pt]
Montana&1\tabularnewline[0pt]
Rhode Island&1.1\tabularnewline[0pt]
New Hampshire&1.3\tabularnewline[0pt]
Maine&1.3\tabularnewline[0pt]
Hawaii&1.4\tabularnewline[0pt]
Idaho&1.6\tabularnewline[0pt]
West Virginia&1.9\tabularnewline[0pt]
Nebraska&1.9\tabularnewline[0pt]
New Mexico&2.1\tabularnewline[0pt]
Nevada&2.8\tabularnewline[0pt]
Kansas&2.9\tabularnewline[0pt]
Utah&2.9\tabularnewline[0pt]
Arkansas&3\tabularnewline[0pt]
Mississippi&3\tabularnewline[0pt]
Iowa&3.1\tabularnewline[0pt]
Connecticut&3.6\tabularnewline[0pt]
Oklahoma&3.9\tabularnewline[0pt]
Oregon&3.9\tabularnewline[0pt]
Kentucky&4.4\tabularnewline[0pt]
Louisiana&4.6\tabularnewline[0pt]
South Carolina&4.8\tabularnewline[0pt]
Alabama&4.8\tabularnewline[0pt]
Colorado&5.3\tabularnewline[0pt]
Minnesota&5.4\tabularnewline[0pt]
Wisconsin&5.7\tabularnewline[0pt]
Maryland&5.9\tabularnewline[0pt]
Missouri&6\tabularnewline[0pt]
Tennessee&6.5\tabularnewline[0pt]
Indiana&6.6\tabularnewline[0pt]
Arizona&6.6\tabularnewline[0pt]
Massachusetts&6.7\tabularnewline[0pt]
Washington&7\tabularnewline[0pt]
Virginia&8.3\tabularnewline[0pt]
New Jersey&8.9\tabularnewline[0pt]
North Carolina&9.8\tabularnewline[0pt]
Michigan&9.9\tabularnewline[0pt]
Georgia&10\tabularnewline[0pt]
Ohio&11.6\tabularnewline[0pt]
Pennsylvania&12.8\tabularnewline[0pt]
Illinois&12.9\tabularnewline[0pt]
Florida&19.6\tabularnewline[0pt]
New York&19.7\tabularnewline[0pt]
Texas&26.4\tabularnewline[0pt]
California&38.3
\end{tabular}
\caption{USA State Populations - 2014\label{table-1}}
\end{table}
 Determine the minimum, maximim, midrange, and mean for this data.%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-1}{}\quad%
\hypertarget{p-84}{}%
Notice that these are already in order so you can presume \(y_1 = 0.6\) million is the minimum and \(y_{50} = 38.3\) million is the maximum. Therefore, the midrange is given by%
\begin{equation*}
\frac{0.6+38.3}{2} = \frac{38.9}{2} = 19.45  \text{million}.
\end{equation*}
In this collection of "states" data the District of Columbia is included so that the number of data items is n=51. The mean of this data takes a bit of arithmetic but gives%
\begin{equation*}
\overline{x} = \frac{\sum_{k=1}^{51} y_k }{51} = \frac{316.1}{51} \approx 6.20
\end{equation*}
million residents.%
\par
\hypertarget{p-85}{}%
Since the number of states is odd, the median is found by looking at the 26th order statistic. In this case, that is the 4.4 million residents of Kentucky, i.e. \(y_{26} = 4.4\).%
\end{example}
%
%
%
\typeout{************************************************}
\typeout{Section 1.5 Statistical Measures of Variation}
\typeout{************************************************}
%
\section[{Statistical Measures of Variation}]{Statistical Measures of Variation}\label{section-5}
\hypertarget{p-86}{}%
These measures provide some indication of how much the data set is "spread out". Indeed, note that the data sets {-2,-1,0,1,2} and {-200,-100,0,100,200} have the same mean but one is much more spread out than the other. Measures of variation should catch this difference.%
\par
\hypertarget{p-87}{}%
\begin{definition}{Range:.}{definition-10}%
\hypertarget{Range}{}%
Using the order statistics,%
\begin{equation*}
y_n - y_1.
\end{equation*}
%
\end{definition}
%
\par
\hypertarget{p-89}{}%
It is trivial to note that the range is very easy to compute but it completely gnores all data values but the two ends.%
\par
\hypertarget{p-90}{}%
From the \hyperref[PresidentAge]{Presidential data~\ref{PresidentAge}}, the maximum is 69 and the minimum is 46 so the range is 23, the difference of these two.%
\par
\hypertarget{p-91}{}%
\begin{definition}{Interquartile Range (IQR):.}{definition-11}%
\hypertarget{IQR}{}%
%
\begin{equation*}
IQR = P^{0.75} - P^{0.25}.
\end{equation*}
%
\end{definition}
%
\par
\hypertarget{p-93}{}%
For the data set {2, 5, 8, 10}, you have found that \(Q_1 = 2.75\) and \(Q_3 = 9.5\). Therefore,%
\begin{equation*}
IQR = 9.5 - 2.75 = 6.75.
\end{equation*}
%
\par
\hypertarget{p-94}{}%
Average Deviation from the Mean (Population):  Given a population data set \(x_1, x_2, ... , x_n\) with mean \(\mu\) each term deviates from the mean by the value \(x_k - \mu\). So, averaging these gives%
\begin{equation*}
\frac{\sum_{k=1}^n (x_k-\mu)}{n} = \frac{\sum_{k=1}^n x_k}{n} - \frac{\sum_{k=1}^n \mu}{n} = \mu - \mu = 0.
\end{equation*}
This metric is therefore always zero for any provided set of data since cancellation makes this not useful. So, we need to determine ways to avoid cancellation.%
\par
\hypertarget{p-95}{}%
Average Absolute Deviation from the Mean (Population):%
\begin{equation*}
\frac{\sum_{k=1}^n \left | x_k-\mu \right |}{n} 
\end{equation*}
which, although nicely stated, is difficult to deal with algebraically since the absolute values do not simplify well algebraically. To avoid this algebraic roadblock, we can look for another way to nearly accomplish the same goal by squaring and then square rooting.%
\par
\hypertarget{p-96}{}%
Average Squared Deviation from the Mean (Population):%
\begin{equation*}
\frac{\sum_{k=1}^n ( x_k-\mu )^2}{n} 
\end{equation*}
which will always be non-negative but can be easily expanded using algebra. Since this is a mouthful, this measure is generally called the "variance".%
\par
\hypertarget{p-97}{}%
Using the average squared deviation from the mean, differences have been squared. Thus all of the squared differences added are non-negative but very small ones have been made even smaller and larger ones have been made relatively larger. To undo this scaling issue, one must take a square root to get things back into the right ball park.%
\par
\hypertarget{p-98}{}%
\begin{definition}{Variance and Standard Deviation.}{definition-12}%
\hypertarget{Variance}{}%
The variance is the average squared deviation from the mean. If this data comes from the entire universe of possibilities then we call it a population variance and denote this value by \(\sigma^2\). Therefore%
\begin{equation*}
\sigma^2 = \frac{\sum_{k=1}^n ( x_k-\mu )^2}{n} 
\end{equation*}
%
\par
\hypertarget{p-100}{}%
The standard deviation is the square root of the variance. If this data comes from the entire universe of possibilities then we call it a population standard deviation and denote this value by \(\sigma\). Therefore%
\begin{equation*}
\sigma = \sqrt{\frac{\sum_{k=1}^n ( x_k-\mu )^2}{n}}.
\end{equation*}
%
\par
\hypertarget{p-101}{}%
If data comes from a sample of the population then we call it a sample variance and denote this value by v. Since sample data tends to reflect certain "biases" then we increase this value slightly by \(\frac{n}{n-1}\) to give the sample variance%
\begin{equation*}
s^2 = \frac{n}{n-1}\frac{\sum_{k=1}^n ( x_k-\overline{x} )^2}{n} = \frac{\sum_{k=1}^n ( x_k-\overline{x} )^2}{n-1}.
\end{equation*}
and the sample standard deviation similarly as the square root of the sample variance.%
\end{definition}
%
\par
\hypertarget{p-102}{}%
From the data {2,5,8,10}, you have found that the mean is 6.25. Computing the variance then involves accumulating and averaging the squared differences of each data value and this mean. Then%
\begin{align*}
& \frac{1}{4} \left ( (2-6.25)^2 + (5-6.25)^2 + (8-6.25)^2 + (10-6.25)^2 \right ) \\
& = \frac{18.0625 + 1.5625 + 3.0625 + 14.0625}{4} \\
& = \frac{36.75}{4}\\
& = 9.1875.
\end{align*}
%
\par
\hypertarget{p-103}{}%
\begin{theorem}{Alternate Forms for Variance.}{}{AlternateVariance}%
%
\begin{align*}
\sigma^2 & = \left ( \frac{\sum_{k=1}^n x_k^2 }{n} \right ) - \mu^2 \\
& = \left [ \frac{\sum_{k=1}^n x_k(x_k - 1)}{n} \right ] + \mu - \mu^2
\end{align*}
\end{theorem}
\begin{proof}\hypertarget{proof-1}{}
\hypertarget{p-104}{}%
%
\begin{align*}
\sigma^2 & = \frac{\sum_{k=1}^n ( x_k-\mu )^2}{n}\\
& = \frac{\sum_{k=1}^n ( x_k^2 - 2x_k \mu + \mu^2 )}{n}\\
& = \frac{\sum_{k=1}^n x_k^2 - 2\mu \sum_{k=1}^n x_k  + n \mu^2 )}{n}\\
& = \left ( \frac{\sum_{k=1}^n x_k^2 }{n} \right ) - \mu^2
\end{align*}
%
\par
\hypertarget{p-105}{}%
The second part is proved similarly. Using the first part of the proof above,%
\begin{align*}
\sigma^2 & = \frac{\sum_{k=1}^n ( x_k-\mu )^2}{n}\\
& = \left ( \frac{\sum_{k=1}^n x_k^2 }{n} \right ) - \mu^2\\
& = \left ( \frac{\sum_{k=1}^n x_k (x_k - 1) + x_k }{n} \right ) - \mu^2\\
& = \left ( \frac{\sum_{k=1}^n x_k (x_k - 1)}{n} \right ) + \mu - \mu^2\qedhere
\end{align*}
%
\end{proof}
%
\par
\hypertarget{p-106}{}%
\begin{example}{Computing means and variances by hand.}{example-9}%
\hypertarget{p-107}{}%
In the data table below, notice that the \(x_k\) column would be the given data values but the column for \(x_k^2\) you could easily compute. \begin{table}
\centering
\begin{tabular}{rr}
\(x_k\)&\(x_k^2\)\tabularnewline\hrulemedium
1&1\tabularnewline[0pt]
-1&1\tabularnewline[0pt]
0&0\tabularnewline[0pt]
2&4\tabularnewline[0pt]
2&4\tabularnewline[0pt]
5&25
\end{tabular}
\caption{Sample Grouped Data\label{table-2}}
\end{table}
 So, \(\Sigma x_k = 9\) and \(\Sigma x_k^2 = 35\).  Therefore \(\overline{x} = \frac{9}{6} = \frac{3}{2}\) and \(v = \frac{\Sigma x_k^2}{6} - (\overline{x})^2 = ( \frac{35}{6} - \frac{3}{2}^2 = \frac{70-18}{12} = \frac{26}{6}\).  Therefore, \(s^2 = \frac{6}{5} \times v = \frac{26}{5}\).%
\end{example}
%
\par
\hypertarget{p-108}{}%
Use R to compute these values... \leavevmode%
\begin{sageinput}
data <- c( 1, -1, 0, 2, 2, 5)   # concatenate the following items into a list
paste("Variance = ", var(data))
paste("Standard Dev = ", sd(data))
paste("Inter Quantile Range =",IQR(data))
paste("Box and Whisker Diagram:")
boxplot(data, horizontal=TRUE)
\end{sageinput}
%
\par
\hypertarget{p-109}{}%
Once again, the Population of the individual USA states according to the 2013 Census is considered below.%
\par
\hypertarget{p-110}{}%
\begin{inlineexercise}{USA State Population Measures of Variation.}{exercise-3}%
\hypertarget{p-111}{}%
Using the \hyperref[StatePopulations]{US Census Bureau state populations~\ref{StatePopulations}} (in millions) for 2014 provided earlier, determine the range, quartiles, and variance for this sample data.%
\par
\hypertarget{p-112}{}%
\textbf{Solution}.\quad%
\hypertarget{p-113}{}%
Again, you should note that these are already in order so the range is quickly found to be%
\begin{equation*}
y_n - y_1 = 38.3 - 0.6 = 37.7
\end{equation*}
million residents.%
\par
\hypertarget{p-114}{}%
For IQR, we first must determine the quartiles. The median (found earlier) already is the second quartile so we have \(Q_2 = 4.5\) million. For the other two, the formula for computing percentiles gives you the 25th percentiile%
\begin{gather*}
(n+1)p = 51(1/4) = 12.75\\
Q_1 = P^{0.25} = 0.25 \times 1.9 + 0.75 \times 2.1 = 2.05
\end{gather*}
and the 75th percentile%
\begin{gather*}
(n+1)p = 51(3/4) = 38.25\\
Q_3 = P^{0.75} = 0.75 \times 7 + 0.25 \times 8.3 = 7.325.
\end{gather*}
Hence, the IQR = 7.325 - 2.05 = 5.275 million residents.%
\par
\hypertarget{p-115}{}%
From the computation before, again note that n=51 since the District of Columbia is included. The mean of this data found before was found to be approximately 6.20 million residents. So, to determine the variance you may find it easier to compute using the the alternate variance formulas \hyperref[AlternateVariance]{Theorem~\ref{AlternateVariance}}.%
\begin{align*}
v & = \left ( \frac{\sum_{k=1}^n y_k^2 }{n} \right ) - \mu^2\\
& \approx \frac{4434.37}{51} - (6.20)^2\\
& = 48.51
\end{align*}
and so you get a sample variance of%
\begin{equation*}
s^2 \approx \frac{51}{50} \cdot 48.51 = 49.48
\end{equation*}
and a sample standard deviation of%
\begin{equation*}
s \approx \sqrt{49.48} \approx 7.03
\end{equation*}
million residents.%
%
\end{inlineexercise}
%
\par
\hypertarget{p-116}{}%
The state population data set has been entered for you in the R cell below... \leavevmode%
\begin{sageinput}
data <- c (0.6,0.6,0.6,0.7,0.7,0.8,0.9,1,1.1,1.3,1.3,1.4,1.6,
1.9,1.9,2.1,2.8,2.9,2.9,3,3,3.1,3.6,3.9,3.9,4.4,4.6,
4.8,4.8,5.3,5.4,5.7,5.9,6,6.5,6.6,6.6,6.7,7,8.3,
8.9,9.8,9.9,10,11.6,12.8,12.9,19.6,19.7,26.4,38.3)
paste("Variance = ", var(data))
paste("Standard Dev = ", sd(data))
paste("Inter Quantile Range =",IQR(data))
\end{sageinput}
%
%
%
\typeout{************************************************}
\typeout{Section 1.6 Adjusting Statistical Measures for Grouped Data}
\typeout{************************************************}
%
\section[{Adjusting Statistical Measures for Grouped Data}]{Adjusting Statistical Measures for Grouped Data}\label{section-6}
\hypertarget{p-117}{}%
As you considered the measures of the center and spread before, each data point was considered individually. Often, data may however be grouped into categories. The number of data items in each category is called the "frequency" of that outcome and the collection of these frequencies for all outcomes is called a "frequency distribution".%
%
%
\typeout{************************************************}
\typeout{Subsection 1.6.1 Data Grouped into Single-valued Categories}
\typeout{************************************************}
%
\subsection[{Data Grouped into Single-valued Categories}]{Data Grouped into Single-valued Categories}\label{subsection-1}
\hypertarget{p-118}{}%
In this case, rather than considering \(x_k\) to be the kth data value can take advantage of the grouping to perhaps save a bit on arithmetic.%
\par
\hypertarget{p-119}{}%
Indeed, let's assume that data is grouped into m categories \(x_1, x_2, ..., x_m\) with corresponding frequencies \(f_1, f_2, ..., f_m\). Then, for example, when computing the mean rather than adding \(x_1\) with itself \(f_1\) times just compute \(x_1 \times f_1\) for the first category and continuing through the remaining categories. This gives the following grouped data formula for the mean%
\begin{equation*}
\mu = \frac{x_1 f_1 + ... + x_m f_m}{f_1 + ... + f_m} = \frac{\sum_{k=1}^m x_k f_k}{\sum_{k=1}^m f_k}.
\end{equation*}
and the following grouped data formula for the variance (along with one equivalent form)%
\begin{equation*}
\sigma^2 = \frac{\sum_{k=1}^m ( x_k-\mu )^2 f_k}{\sum_{k=1}^m f_k} = \frac{\sum_{k=1}^m x_k^2 f_k}{\sum_{k=1}^m f_k} - \mu^2
\end{equation*}
%
\begin{inlineexercise}{}{exercise-4}%
\hypertarget{p-120}{}%
Consider the following data set%
\par
\hypertarget{p-121}{}%
{3, 1, 2, 2, 3, 1, 3, 4, 5, 5, 1, 4, 5, 1, 2, 4, 5, 3, 2, 5, 2, 1, 2, 2, 5}%
\par
\hypertarget{p-122}{}%
Create a frequency distribution and determine the sample mean and variance.%
\par
\hypertarget{p-123}{}%
\textbf{Solution}.\quad%
\hypertarget{p-124}{}%
Collecting this data into a frequency distribution gives \leavevmode%
\begin{table}
\centering
\begin{tabular}{rr}
\(x_k\)&\(f_k\)\tabularnewline\hrulemedium
1&5\tabularnewline[0pt]
2&7\tabularnewline[0pt]
3&4\tabularnewline[0pt]
4&3\tabularnewline[0pt]
5&6
\end{tabular}
\caption{Grouped Discrete Data\label{table-3}}
\end{table}
 Therefore,%
\begin{equation*}
\overline{x} = \frac{1 \times 5 + 2 \times 7 + 3 \times 4 + 4 \times 3 + 5 \times 6}{5+7+4+3+6} \\
= \frac{5 + 14 + 12 + 12 + 30}{25} = \frac{43}{25}
\end{equation*}
and%
\begin{align*}
v & = \frac{1^2 \times 5 + 2^2 \times 7 + 3^2 \times 4 + 4^2 \times 3 + 5^2 \times 6}{5+7+4+3+6} - \left ( \frac{43}{25} \right )^2 \\
& = \frac{5 + 28 + 36 + 48 + 150}{25} - \left ( \frac{43}{25} \right )^2 \\
& = \frac{4826}{625}\\
& \approx 7.7216
\end{align*}
and so \(s^2 = \frac{25}{24} \frac{4826}{625} \approx 8.043\).%
%
\end{inlineexercise}
%
%
\typeout{************************************************}
\typeout{Subsection 1.6.2 Data Grouped into Continuous Intervals}
\typeout{************************************************}
%
\subsection[{Data Grouped into Continuous Intervals}]{Data Grouped into Continuous Intervals}\label{subsection-2}
\hypertarget{p-125}{}%
For measures on data grouped into intervals, it is somewhat difficult to do calculations when the data no longer exists as individual values since all you know is the frequencies of each interval. You can use "class marks"...the midpoints of each interval...as representers for all of the items that fell into that interval for computing means and variances.  For positional measures, you want to approach this in the same manner as with percentiles before.  That is, my doing some sort of linear interpolation on the width of each interval.%
\par
\hypertarget{p-126}{}%
So, for medians, consider the following approach:%
\par
\hypertarget{p-127}{}%
\leavevmode%
\begin{enumerate}
\item\hypertarget{li-5}{}Compute frequencies \(f_k\) and cummulative frequencies \(F_k\) for each class%
\item\hypertarget{li-6}{}Set m = total cummulative frequency/2 = \(F_{last}/2\)%
\item\hypertarget{li-7}{}Determine the interval \(k\) where \(m \in [F_{k-1},F_k]\)%
\item\hypertarget{li-8}{}Set median = \((b_k-a_k)\frac{m - F_{k-1}}{f_k}+a_k\)%
\end{enumerate}
%
\par
\hypertarget{p-128}{}%
\begin{example}{Computing Median for Interval Grouped Data.}{example-10}%
\hypertarget{p-129}{}%
\leavevmode%
\begin{table}
\centering
\begin{tabular}{rr}
\([a_k,b_k]\)&\(f_k\)\tabularnewline\hrulemedium
[0,5)&5\tabularnewline[0pt]
[5,10)&7\tabularnewline[0pt]
[10,20)&4\tabularnewline[0pt]
[20,23)&3\tabularnewline[0pt]
[23,30)&6
\end{tabular}
\caption{Interval Frequency Distribution\label{table-4}}
\end{table}
 The total cummulative frequency is 25 and so \(m = \frac{25}{2} = 12.5\) which lies in the k = 3 interval [10,20) and \(F_2 = 12\).  Therefore%
\begin{equation*}
\text{median} = (20-10) \frac{12.5-12}{4} + 10 = 11.25
\end{equation*}
%
\end{example}
%
%
%
\typeout{************************************************}
\typeout{Section 1.7 Other Statistical Point Measures}
\typeout{************************************************}
%
\section[{Other Statistical Point Measures}]{Other Statistical Point Measures}\label{section-7}
\hypertarget{p-130}{}%
Above, we have investigated statistical measures that help determine the middle and the spread of a given data set. There are however other metrics available that help describe the distribution of that data.  Skewness is one of those metrics and describes any lack of symmetry of the data set's distribution and whether data is stretched out to one side or the other.%
\par
\hypertarget{p-131}{}%
\begin{definition}{Skewness.}{definition-13}%
\hypertarget{Skewness}{}%
For population data, the Skewness of \(x_1, x_2, ..., x_n\) is given by%
\begin{equation*}
\frac{1}{\sigma^3} \frac{\sum_{k=1}^n ( x_k-\mu )^3}{n}.
\end{equation*}
%
\par
\hypertarget{p-133}{}%
For sample data, the Skewness of \(x_1, x_2, ..., x_n\) is given by%
\begin{equation*}
\frac{1}{s^3} \frac{\sum_{k=1}^n ( x_k-\overline{x} )^3}{n}.
\end{equation*}
%
\end{definition}
%
\par
\hypertarget{p-134}{}%
A positive skewness indicates that the positive \((x_k - \mu)^3\) terms (likewise \((x_k - \overline{x})^3\) terms) overwhelm the negative terms. So, a positive skewness indicates that the data set is strung out to the right. Likewise, a negative skewness indicates a data set that is strung out to the left.%
\par
\hypertarget{p-135}{}%
Data might tend to be clustered around the mean. The "kurtosis" can be used to measure how closely data resembles a "bell-shaped" collection.%
\par
\hypertarget{p-136}{}%
\begin{definition}{Kurtosis.}{definition-14}%
\hypertarget{Kurtosis}{}%
For population data, the Kurtosis of \(x_1, x_2, ..., x_n\) is given by%
\begin{equation*}
\frac{1}{\sigma^4} \frac{\sum_{k=1}^n ( x_k-\mu )^4}{n}.
\end{equation*}
%
\par
\hypertarget{p-138}{}%
For sample data, the Kurtosis of \(x_1, x_2, ..., x_n\) is given by%
\begin{equation*}
\frac{1}{s^4} \frac{\sum_{k=1}^n ( x_k-\overline{x} )^4}{n}.
\end{equation*}
%
\end{definition}
%
\par
\hypertarget{p-139}{}%
A kurtosis of 3 indicates that the data is perfectly bell shaped (a "normal" distribution) whereas data further away from 3 indicates data that is less bell shaped.%
\begin{theorem}{Alternate Formulas for Skewness and Kurtosis.}{}{theorem-2}%
%
\begin{equation*}
\text{skewness} = \frac{1}{s^3} 
\left [ \frac{\sum_{k=1}^n x_k^3}{n} - 3 v \overline{x} - \overline{x}^3 \right ]
\end{equation*}
%
\begin{equation*}
\text{kurtosis} = \frac{1}{s^4} 
\left [ \frac{\sum_{k=1}^n x_k^4}{n} - 4 \overline{x} \frac{\sum_{k=1}^n x_k^3 }{n} + 6 \overline{x}^2 v - 3 \overline{x}^4  \right ]
\end{equation*}
\begin{proof}\hypertarget{proof-2}{}
\hypertarget{p-140}{}%
For skewness, expand the cubic and break up the sum. Factoring out constants (such as \(\overline{x}\)) gives%
\begin{align*}
& \frac{\sum_{k=1}^n ( x_k-\overline{x} )^3}{n}\\
& = \frac{\sum_{k=1}^n x_k^3}{n} - 3 \overline{x} \frac{\sum_{k=1}^n x_k^2 }{n} + 3 \overline{x}^2 \frac{\sum_{k=1}^n x_k}{n} - \frac{\sum_{k=1}^n \overline{x}^3}{n}\\
& = \frac{\sum_{k=1}^n x_k^3}{n} - 3 \overline{x}(v + \overline{x}^2) + 3 \overline{x}^3 - \overline{x}^3\\
& = \frac{\sum_{k=1}^n x_k^3}{n} - 3 \overline{x}v - \overline{x}^3
\end{align*}
and divide by the cube of the standard deviation to finish. Note that the first expansion in the derivation above can be used quickly if the data is collected in a table and powers easily computed.%
\par
\hypertarget{p-141}{}%
For kurtosis, similarly expand the quartic and break up the sum as before. Note that you can extract the value of the cubic term by solving for that term in the skewness formula above. Then,%
\begin{align*}
& \frac{\sum_{k=1}^n ( x_k-\overline{x} )^4}{n}\\
& = \frac{\sum_{k=1}^n x_k^4}{n} - 4 \overline{x} \frac{\sum_{k=1}^n x_k^3 }{n} + 6 \overline{x}^2 \frac{\sum_{k=1}^n x_k^2}{n} - 4 \overline{x}^3 \frac{\sum_{k=1}^n x_k}{n} + \frac{\sum_{k=1}^n \overline{x}^4}{n}\\
& = \frac{\sum_{k=1}^n x_k^4}{n} - 4 \overline{x} \frac{\sum_{k=1}^n x_k^3 }{n} + 6 \overline{x}^2 (v+\overline{x}^2) - 4 \overline{x}^4 + \overline{x}^4\\
& = \frac{\sum_{k=1}^n x_k^4}{n} - 4 \overline{x} \frac{\sum_{k=1}^n x_k^3 }{n} + 6 \overline{x}^2 v - 3 \overline{x}^4 
\end{align*}
and then divide by the fourth power of the standard deviation.  Note again that the first expansion in the derivation above might also be a useful shortcut.%
\end{proof}
\end{theorem}
\hypertarget{p-142}{}%
Going back to a previous example...%
\par
\hypertarget{p-143}{}%
Computing skewness and kurtosis by hand can often be better organized using a table. Below, notice that the \(x_k\) column would be the given data values but the other columns you could again easily compute. \begin{table}
\centering
\begin{tabular}{rrrr}
\(x_k\)&\(x_k^2\)&\(x_k^3\)&\(x_k^4\)\tabularnewline\hrulemedium
1&1&1&1\tabularnewline[0pt]
-1&1&-1&1\tabularnewline[0pt]
0&0&0&0\tabularnewline[0pt]
2&4&8&16\tabularnewline[0pt]
2&4&8&16\tabularnewline[0pt]
5&25&125&625
\end{tabular}
\caption{Computing data statistics by hand\label{table-5}}
\end{table}
 So, \(\Sigma x_k = 9\) and \(\Sigma x_k^2 = 35\) as before and so \(\overline{x} = \frac{3}{2}\), \(v = \frac{26}{6}\), \(s^2 = \frac{6}{5} \times v = \frac{26}{5}\), and so \(s = \sqrt{\frac{26}{5}} = \sqrt{5.2}\). But also, \(\Sigma x_k^3 = 141\) and \(\Sigma x_k^4 = 659\).  Use these in the formulas above to obtain skewness of%
\begin{equation*}
\left [ \frac{141}{6} - 3 \cdot \frac{26}{5} \cdot \frac{3}{2} - \left ( \frac{3}{2} \right )^2 \right ] / s^3 
\end{equation*}
and kurtosis of%
\begin{equation*}
\left [ \frac{659}{6} - 4 \cdot \frac{3}{2} \cdot \frac{141}{6} + 6 \left ( \frac{3}{2} \right )^2  \cdot \frac{26}{5}  - 3 \cdot \left ( \frac{3}{2} \right )^4 \right ] / s^4.
\end{equation*}
%
%
%
\typeout{************************************************}
\typeout{Section 1.8 Visual Statistical Measures - Graphical Representation of Data}
\typeout{************************************************}
%
\section[{Visual Statistical Measures - Graphical Representation of Data}]{Visual Statistical Measures - Graphical Representation of Data}\label{section-8}
\hypertarget{p-144}{}%
Data sets can range from small to very large. Visual representations of these data sets often allow you to see trends and reveal a lot about the distribution of the data values.%
%
%
\typeout{************************************************}
\typeout{Subsection 1.8.1 Histograms}
\typeout{************************************************}
%
\subsection[{Histograms}]{Histograms}\label{subsection-3}
\hypertarget{p-145}{}%
Frequency Histograms - height matters%
\par
\hypertarget{p-146}{}%
Consider the data set given by \leavevmode%
\begin{table}
\centering
\begin{tabular}{rr}
\(x_k\)&\(f_k\)\tabularnewline\hrulemedium
1&8\tabularnewline[0pt]
2&12\tabularnewline[0pt]
3&6\tabularnewline[0pt]
4&3\tabularnewline[0pt]
5&1\tabularnewline[0pt]
6&2
\end{tabular}
\caption{Basic Frequency Table\label{table-6}}
\end{table}
%
\par
\hypertarget{p-147}{}%
A frequency histogram representing this data looks like \begin{figure}\centering\includegraphics[width=1\linewidth]{images/frequencyhistogram.png}
\end{figure}%
\par
\hypertarget{p-148}{}%
Experiment with creating your own histogram by inputting your data into the interactive Sage cell below. \leavevmode%
\begin{sageinput}
#  This function is used to convert an input string into separate entries
def g(s): return str(s).replace(',',' ').replace('(',' ').replace(')',' ').split()

@interact
def _(freq = input_box("1,1,1,1,2,2,2,3,3,3,3,1,5",label="Enter data separated by commas")):
    freq = g(freq)
    freq = [int(k) for k in freq]
    m = min(freq)
    M = max(freq)
    bn = M-m+1
    histogram( freq, range=[m-1/2,M+1/2], bins = bn, align="mid", linewidth=2, edgecolor="blue", color="yellow").show()
\end{sageinput}
%
\par
\hypertarget{p-149}{}%
Relative Frequency Histograms - In this case, area describes relative frequency.  Notice in the interactive cell above that each bar is of width one. Therefore, frequency = area. In some instances where data may be grouped the total width of the interval may be different and so the height will need to be adjusted so that the total area of each bar corresponds to the relative frequency of that category.%
\par
\hypertarget{p-150}{}%
Cummulative Histograms.  In these a running total is presented using all values from the given point and below. \leavevmode%
\begin{sageinput}
#  This function is used to convert an input string into separate entries
def g(s): return str(s).replace(',',' ').replace('(',' ').replace(')',' ').split()

@interact
def _(freq = input_box("1,1,1,1,2,2,2,3,3,3,3,1,5",label="Enter data separated by commas")):
    freq = g(freq)
    freq = [int(k) for k in freq]
    top = len(freq)
    m = min(freq)
    M = max(freq)
    bn = M-m+1
    histogram( freq, range=[m-1/2,M+1/2], cumulative = "true", bins = bn, align="mid", linewidth=2, edgecolor="blue", color="yellow").show(ymax=top)
\end{sageinput}
%
%
%
\typeout{************************************************}
\typeout{Subsection 1.8.2 Stem and Leaf Plot}
\typeout{************************************************}
%
\subsection[{Stem and Leaf Plot}]{Stem and Leaf Plot}\label{subsection-4}
\hypertarget{p-151}{}%
A Stem and Leaf Plot allows you to create a histogram of sorts but maintain the individual data values. To create one of these plots, you will need to consider your particular data set and create a two-step sieve for organizing the set.  The first part is to create "stems" that are often associated with the highest digit(s) of each data value and the "leaves" that are often associated with the remaining digit(s) of the data value.%
\par
\hypertarget{p-152}{}%
Once the data set is broken down into stems and leaves, it is often simple to sort the leaves under each stem to yield an "ordered Stem and Leaf Plot". Such as mechanism is a simple two-step procedure that allows you to sort a data set by hand.%
\par
\hypertarget{p-153}{}%
\begin{example}{Simple Stem and Leaf Plot.}{example-11}%
\hypertarget{p-154}{}%
Consider the data points 25, 3, 17, 12, 22, 34, 12, 11, 16, 42, 9, 12, 17. In this case we will consider the stems to be the tens digits and the leaves to be the ones digits. This gives%
\par
\hypertarget{p-155}{}%
\leavevmode%
\begin{table}
\centering
\begin{tabular}{rr}
Stems&Leaves\tabularnewline\hrulemedium
0&\multicolumn{1}{l}{3 9}\tabularnewline[0pt]
1&\multicolumn{1}{l}{7 2 2 1 6 2 7}\tabularnewline[0pt]
2&\multicolumn{1}{l}{5 2}\tabularnewline[0pt]
3&\multicolumn{1}{l}{4}\tabularnewline[0pt]
4&\multicolumn{1}{l}{2}
\end{tabular}
\caption{Stem and Leaf Plot (unordered)\label{table-7}}
\end{table}
 Then, an ordered Stem and Leaf Plot would be \begin{table}
\centering
\begin{tabular}{rr}
Stems&Leaves\tabularnewline\hrulemedium
0&\multicolumn{1}{l}{3 9}\tabularnewline[0pt]
1&\multicolumn{1}{l}{1 2 2 2 6 7 7}\tabularnewline[0pt]
2&\multicolumn{1}{l}{2 5}\tabularnewline[0pt]
3&\multicolumn{1}{l}{4}\tabularnewline[0pt]
4&\multicolumn{1}{l}{2}
\end{tabular}
\caption{Stem and Leaf Plot (ordered)\label{table-8}}
\end{table}
%
\par
\hypertarget{p-156}{}%
Notice, in each case you can extract the original data values by recombining the stem with a corresponding leaf. Indeed, for these 13 data values the median should be be 7th in the sorted list or the value in the 10's stem with leaf 6...that is, 16.%
\end{example}
%
\par
\hypertarget{p-157}{}%
\begin{example}{Stem and Leaf Plot for State Populations.}{example-12}%
\hypertarget{p-158}{}%
Using the state population data above, consider organizing the data but using a "two-pass sort" where you first roughly break data up into groups based upon ranges which relate to their first digit(s). In this case, let's break up into groups according to populations corresponding to 0-4 million, 5-9 million, 10-14 million, 15-19, million, 20-24 million, 25-29 million, 30-35 million, and 35-39 million. We can represent these classes by using the stems 0L, 0H, 1L, 1H, 2L, 2H, 3L, and 3H where the L and H represent the one's digits L in {0, 1, 2, 3, 4} and H in {5, 6, 7, 8, 9}.  Once we group the data into these smaller groups then we can write the remaining portion of the number horizontally as leaves (in this case with one decimal place for all values.) This gives a step-and-leaf plot. If we additionally sort the data in the leaves then this gives you an ordered stem-and-leaf plot. For the state population data, the ordered stem-and-leaf plot is given by \begin{figure}\centering\includegraphics[width=1\linewidth]{images/stemandleaf.png}
\end{figure} Notice how it is easy to now see that most state populations are relatively small and that there are relatively few states with larger population. Also, notice that you can use this plot to relatively easily identify minimum, maximum, and other order statistics.%
\end{example}
%
%
%
\typeout{************************************************}
\typeout{Subsection 1.8.3 Box and Whisker Diagram (Box Plot)}
\typeout{************************************************}
%
\subsection[{Box and Whisker Diagram (Box Plot)}]{Box and Whisker Diagram (Box Plot)}\label{subsection-5}
\hypertarget{p-159}{}%
This graphical display identifies the "5-number-summary" associated with the minimum, quartiles, and the maximum. That is, \(y_1, Q_1, Q_2, Q_3, y_n\).  These values separate the data roughly into quarters. To distinguish these quarters connect \(y_1\) and \(Q_1\) with a straight line (a whisker) and do the same with \(Q_3\) and \(y_n\). Use a box to connect \(Q_1\) with \(Q_2\) and the same to connect \(Q_2\) with \(Q_3\). Then the boxed areas also identify the IQR.%
\par
\hypertarget{p-160}{}%
\leavevmode%
\begin{sageinput}
data <- c (0.6,0.6,0.6,0.7,0.7,0.8,0.9,1,1.1,1.3,1.3,
1.4,1.6,1.9,1.9,2.1,2.8,2.9,2.9,3,3,3.1,
3.6,3.9,3.9,4.4,4.6,4.8,4.8,5.3,5.4,5.7,
5.9,6,6.5,6.6,6.6,6.7,7,8.3,8.9,9.8,9.9,
10,11.6,12.8,12.9,19.6,19.7,26.4,38.3)
paste("Inter Quantile Range =",IQR(data))
paste("Box and Whisker Diagram - Box Plot):")
boxplot(data, horizontal=TRUE)
\end{sageinput}
%
\par
\hypertarget{p-161}{}%
 \begin{inlineexercise}{}{exercise-5}%
\hypertarget{p-162}{}%
Let's use a box plot to determine some order statistics.%
\par\medskip
\hypertarget{p-163}{}%
Consider the following box and whisker plot.  Find the indicated values of the represented data.%
\begin{sidebyside}{1}{-0.083333333333335}{-0.083333333333335}{0}%
\begin{sbspanel}{1.16666666666667}%
\includegraphics[width=1\linewidth]{images/webwork-2-image-1.png}
\end{sbspanel}%
\end{sidebyside}%
\par
\hypertarget{p-164}{}%
Median:  \fillin{5}%
\par
\hypertarget{p-165}{}%
Maximum:  \fillin{5}%
\par\medskip\noindent \hypertarget{p-166}{}%
%
\par
\end{inlineexercise}
%
%
%
\typeout{************************************************}
\typeout{Subsection 1.8.4 Density Plots}
\typeout{************************************************}
%
\subsection[{Density Plots}]{Density Plots}\label{subsection-6}
\hypertarget{p-167}{}%
A Density Plot can be created to visually interpret if the variable is close to normal%
\begin{sageinput}
library(e1071)
par(mfrow=c(1,2))    # graph into two columns
plot(density(cars$speed), main="Density Plot: Speed", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(cars$speed),2)))
\end{sageinput}
\hypertarget{p-168}{}%
www.machinelearningplus.com%
%
%
\typeout{************************************************}
\typeout{Section 1.9 Summary}
\typeout{************************************************}
%
\section[{Summary}]{Summary}\label{section-9}
\hypertarget{p-169}{}%
Links to the main formulas related to descriptive statistics:%
\hypertarget{p-170}{}%
\hyperlink{OrderStatistics}{Order Statistics~}%
\par
\hypertarget{p-171}{}%
\hyperlink{MaxMin}{Maximum and Minimum~}%
\par
\hypertarget{p-172}{}%
\hyperlink{Percentiles}{Percentiles~}%
\par
\hypertarget{p-173}{}%
\hyperlink{Quartiles}{Quartiles~}%
\par
\hypertarget{p-174}{}%
\hyperlink{Deciles}{Deciles~}%
\par
\hypertarget{p-175}{}%
\hyperlink{FiveNumberSummary}{5-number summary~}%
\par
\hypertarget{p-176}{}%
\hyperlink{Mean}{Mean~}%
\par
\hypertarget{p-177}{}%
\hyperlink{Median}{Median~}%
\par
\hypertarget{p-178}{}%
\hyperlink{Midrange}{Midrange~}%
\par
\hypertarget{p-179}{}%
\hyperlink{Range}{Range~}%
\par
\hypertarget{p-180}{}%
\hyperlink{IQR}{Inter Quartile Range~}%
\par
\hypertarget{p-181}{}%
\hyperlink{Variance}{Variance~}%
\par
\hypertarget{p-182}{}%
\hyperlink{Skewness}{Skewness~}%
\par
\hypertarget{p-183}{}%
\hyperlink{Kurtosis}{Kurtosis~}%
%
%
\typeout{************************************************}
\typeout{Section 1.10 Exercises}
\typeout{************************************************}
%
\section[{Exercises}]{Exercises}\label{section-10}
\hypertarget{p-184}{}%
Complete the online WebWorK homework set "Computational Measures".%
\begin{inlineexercise}{}{exercise-6}%
\hypertarget{p-185}{}%
Create a data set with about 10 elements. For your data set, compute each of the measures from this chapter and present your data using a frequency histogram.%
\end{inlineexercise}
\begin{inlineexercise}{}{exercise-7}%
\hypertarget{p-186}{}%
Find a "real-world" data set (similar perhaps to the Census data presented above.) Compute each of the measures from this chapter. Interpret and present your conclusions in an electronic report which can include an excel spreadsheet.%
\end{inlineexercise}
%
%
\typeout{************************************************}
\typeout{Chapter 2 Regression}
\typeout{************************************************}
%
\chapter[{Regression}]{Regression}\label{CurveEstimation}
%
%
\typeout{************************************************}
\typeout{Section 2.1 Introduction}
\typeout{************************************************}
%
\section[{Introduction}]{Introduction}\label{section-11}
\hypertarget{p-187}{}%
When computing means, medians, variances, etc. in the previous chapter, you took given data and create measures that in some sense describe the data using a single value. These single values can be called "descriptive statistics" or perhaps "point estimates" that help understand the properties of the original data set.  In this chapter, you will instead take a data set and create a mathematical model that can be used to predict or infer properties of the underlying problem. Statistical procedures such as in this chapter that are used to predict are often lumped into the world of "inferential statistics".%
\par
\hypertarget{p-188}{}%
So, given a set of data points \((x_0,y_0), (x_1,y_1), ... (x_n,y_n)\), it is often desirable to have a nice continuous formula \(y = f(x)\) that expresses the general nature of those data points. Such a formula "interpolates" the data points if%
\begin{equation*}
y_k = f(x_k),
\end{equation*}
that is the formula gives a graph that exactly passes through each of the given data points.%
\par
\hypertarget{p-189}{}%
On the other hand, sometimes the data points are known to be only approximate or the complexity of the formula needed to interpolate all of the data points exactly is too large. In this case, the formula may only be required to return values that are relatively close to the data points. Such a formula is said to "approximate" and gives%
\begin{equation*}
y_k \approx f(x_k).
\end{equation*}
Let's consider ways to create useful models that approximate the data points.%
\par
\hypertarget{p-190}{}%
From basic algebra, if you are given two distinct points then there is one line which passes exactly through (i.e. interpolates) both. There are many ways to create this linear model but for points \((x_0,y_0), (x_1,y_1)\),%
\begin{equation*}
y = \frac{y_1 - y_0}{x_1 - x_0}(x - x_0) + y_0
\end{equation*}
is the linear function which passes through both points if the x-values are distinct.  If the x's are equal then%
\begin{equation*}
x = x_0
\end{equation*}
is linear and interpolates both data points. However, once you collect three or more points it is likely that there is no line which exactly "interpolates" all of the points. If we desire a linear model then we must settle for a model that approximates. In this chapter, you will investigate how to create polynomial functions which in some manner approximate a collection of data point in some "best" manner.%
%
%
\typeout{************************************************}
\typeout{Section 2.2 Linear Regression - Best Fit Line}
\typeout{************************************************}
%
\section[{Linear Regression - Best Fit Line}]{Linear Regression - Best Fit Line}\label{section-12}
\hypertarget{p-191}{}%
In the next few sections, we will presume only one independent variable x and one dependent variable y. Toward that end, consider a collection of data points%
\begin{equation*}
(x_0,y_0), (x_1,y_1), ... , (x_n,y_n)
\end{equation*}
and a general linear function%
\begin{equation*}
f(x) = mx + b.
\end{equation*}
It is possible but generally unlikely that each of the given data points will be interpolated exactly by the linear function. However, you may notice that the data points exhibit a linear tendency or that the underlying physics might suggest a linear model. A "scatter plot" of a example data set is created in teh interactive cell below and the provided data appears to indicate a linear trend. In general, if this is the case then you may find it easier to predict values of y for given values of x using a linear approximation. That is why this method for doing so is also often called a "best-fit line".%
\par
\hypertarget{p-192}{}%
\leavevmode%
\begin{sageinput}
var('x')
@interact
def _(Points = input_box([(-1,1),(3,2),(4,3),(6,4)])):   
    G = points(Points,size=20)
    G.show(title = "Scatter Plot")
\end{sageinput}
%
\par
\hypertarget{p-193}{}%
But why even bother creating a formula (a line here) to approximate data that does not satisfy that formula? Remember that you would expect collected data to vary slightly as one repeatedly collects that data in the same way that you would expect to make a slightly different score on repeated attempts at exams on the same material. Creating a formula that is close to your data gives a well-defined way to predict a y value for a given x value. This predictive behavior is illustrated in the exercise below.%
\par
\hypertarget{p-194}{}%
\begin{inlineexercise}{WebWork - Using an approximating line.}{exercise-8}%
\hypertarget{p-197}{}%
An airline has determined that the relationship between the number of passengers on a flight and the total weight of luggage stored in the baggage compartment can be estimated by the least squares regression equation%
\par
\hypertarget{p-198}{}%
%
\begin{equation*}
y = 127 + 28 x.
\end{equation*}
%
\par
\hypertarget{p-199}{}%
Predict the weight of luggage for a flight with 121 passengers.%
\par
\hypertarget{p-200}{}%
Answer:  \fillin{15} pounds%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-5}{}\quad%
\hypertarget{p-201}{}%
Since x represents weight, choose x = 121 and evaluate the line at that value to get the estimated total luggage weight to be%
\par
\hypertarget{p-202}{}%
%
\begin{equation*}
y = 127 + 28 (121) = 3515
\end{equation*}
%
\end{inlineexercise}
%
\par
\hypertarget{p-203}{}%
To determine this best-fit line, you need to determine what is meant by the word "best". For linear regression, to reach this goal consider the total of all vertical deviations between the desired line and the provided data points.  Indeed, this vertical error would be of the form%
\begin{equation*}
e_k = f(x_k) - y_k
\end{equation*}
and would be zero if f(x) exactly interpolated at the given data point.  Note, some of these errors will be positive and some will be negative. To avoid any possible cancellation of errors, we could consider taking absolute values (which is tough to deal with algebraically) or perhaps squaring the errors. This second option is the standard approach. This approach is similar to the approach taken earlier when developing formulas for the variance.%
\par
\hypertarget{p-204}{}%
The best-fit line therefore will be the line \(f(x) = mx+b\) so that the "total squared error" is minimized. This total squared error is given by%
\begin{equation*}
TSE(m,b) = \sum_{k=1}^n e_k^2 = \sum_{k=1}^n (f(x_k) - y_k)^2 = \sum_{k=1}^n (m x_k + b - y_k)^2.
\end{equation*}
%
\par
\hypertarget{p-205}{}%
For the following interactive cell, consider for the given data points various values for the slope and y-intercept and see if you can make the total squared error as small as possible. In doing so, notice the vertical distances from the line to the given data points generally decreases as this error measure gets smaller.%
\par
\hypertarget{p-206}{}%
\leavevmode%
\begin{sageinput}
var('x')
@interact
def _(Points = input_box([(-1,1),(3,1),(4,3),(6,4)]), m = slider(-4,4,1/50,1),b = slider(-2,2,1/50,1)):   
    G = points(Points,size=20)
    xpt = []
    ypt = []
    f = m*x + b
    TSE = 0
    for k in range(len(Points)):
        x0 = Points[k][0]
        xpt.append(x0)
        y0 = Points[k][1]
        ypt.append(y0)
        TSE += (f(x=x0) - y0)^2
        G += line([(x0,f(x=x0)),(x0,y0)],color='orange')
    G += plot(f,x,min(xpt)-0.2,max(xpt)+0.2,color='gray')
    T = 'Total Squared Error = $%s$'%str(n(TSE))
    G.show(title = T)
\end{sageinput}
%
\par
\hypertarget{p-207}{}%
\begin{inlineexercise}{Non-functional data.}{exercise-9}%
\hypertarget{p-208}{}%
Experiment in the interactive cell above using exactly two data points that have the same x-value.  Such as (1,1) and (1,2). Next, add some additional data points in the same general vicinity as your original two points. What is the effect to your best-fit line of adding non-functional points?%
\end{inlineexercise}
%
\par
\hypertarget{p-209}{}%
So that we don't have to guess the best values for slope and intercept, we can appeal to calculus. Indeed, to minimize this function of the two variables m and b take partial derivatives and set them equal to zero to get the critical values:%
\begin{equation*}
TSE_m = \sum_{k=1}^n 2(m x_k + b - y_k) \cdot x_k
\end{equation*}
and%
\begin{equation*}
TSE_b = \sum_{k=1}^n 2(m x_k + b - y_k) \cdot 1 .
\end{equation*}
Setting each of these equations equal to zero (using calculus!) and solving gives what is known as the "normal equations":%
\begin{equation*}
m \sum_{k=1}^n x_k^2 + b \sum_{k=1}^n x_k = \sum_{k=1}^n x_k y_k
\end{equation*}
and%
\begin{equation*}
m \sum_{k=1}^n x_k + b \sum_{k=1}^n 1 = \sum_{k=1}^n y_k.
\end{equation*}
Notice that these normal equations are a linear system of equations and (among perhaps other reasons) is why this is called linear regression. Solving these for m and b gives the best fit line.%
\par
\hypertarget{p-210}{}%
So, let's see how to graph points against the best-fit line using R%
\par
\hypertarget{p-211}{}%
\leavevmode%
\begin{sageinput}
x <- c(1, 2, 3, 5, 5, 6)
y <- c(5, 4, 2, 2, 3, 1)
cor(x,y)   # correlation coefficient
pts = data.frame(x, y)
plot(pts,pch = 16, cex = 1.0, col = "blue", main = "Scatter Plot vs Best-Fit Line", xlab = "x", ylab = "y")
# pch = 16 creates solid dots, while cex = 1.5 creates dots that are 1.5 times bigger than the default.
lm(y ~ x)
abline(lm(y ~ x))
\end{sageinput}
%
\par
\hypertarget{p-212}{}%
\begin{inlineexercise}{WebWork.}{exercise-10}%
\hypertarget{p-213}{}%
Ok.  Let's see if you can apply this to get a best fit line.%
\par\medskip
\hypertarget{p-214}{}%
A study was conducted to detemine whether a the final grade of a student in an introductory psychology course is linearly related to his or her performance on the verbal ability test administered before college entrance.  The verbal scores and final grades for \(10\) students are shown in the table below.%
\begin{sidebyside}{1}{0}{0}{0}%
\begin{sbspanel}{1}%
{\centering%
\begin{tabular}{BlBlBlB}\hrulemedium
Student&Verbal Score \(x\)&Final Grade \(y\)\tabularnewline\hrulemedium
\(1\)&\(49\)&\(60\)\tabularnewline\hrulemedium
\(2\)&\(49\)&\(54\)\tabularnewline\hrulemedium
\(3\)&\(72\)&\(89\)\tabularnewline\hrulemedium
\(4\)&\(66\)&\(75\)\tabularnewline\hrulemedium
\(5\)&\(30\)&\(40\)\tabularnewline\hrulemedium
\(6\)&\(33\)&\(36\)\tabularnewline\hrulemedium
\(7\)&\(70\)&\(82\)\tabularnewline\hrulemedium
\(8\)&\(57\)&\(68\)\tabularnewline\hrulemedium
\(9\)&\(79\)&\(95\)\tabularnewline\hrulemedium
\(10\)&\(65\)&\(79\)\tabularnewline\hrulemedium
\end{tabular}
\par}
\end{sbspanel}%
\end{sidebyside}%
\par
\hypertarget{p-215}{}%
Find the least squares line.%
\par
\hypertarget{p-216}{}%
\(\hat{y} =\)  \fillin{10} \(+\)  \fillin{10} \(x\)%
\par
\hypertarget{p-217}{}%
Should the regression be used to predict the final grade of a student with a verbal score of 100?%
\par
\hypertarget{p-218}{}%
answer:  \fillin{10}%
\par\medskip\noindent \hypertarget{p-219}{}%
%
\par
\end{inlineexercise}
%
%
%
\typeout{************************************************}
\typeout{Section 2.3 Correlation}
\typeout{************************************************}
%
\section[{Correlation}]{Correlation}\label{section-13}
\hypertarget{p-220}{}%
You can plot points and plot the resulting best-fit line determined in the previous section but the question remains whether the line is any good. In particular, the real use of the line often is to subsequently predict y-values for a given x-value. However, it is very likely that the best-fit line does not even pass through any of the provided data points.  So, how can something that misses every marker still be considered a good fit. To quantify this, we first need to discuss a way to measure how two variables might vary with each other.%
\par
\hypertarget{p-221}{}%
\begin{definition}{Covariance.}{Covariance}%
\hypertarget{p-222}{}%
Given paired (sample) data%
\begin{equation*}
(x_0,y_0), (x_1,y_1), ... , (x_n,y_n)
\end{equation*}
with corresponding means \(\overline{x}\) and \(\overline{y}\), the covariance is given by%
\begin{equation*}
Cov(X,Y) = \sum_{k=0}^n (x_k-\overline{x})(y_k-\overline{y})/n
\end{equation*}
and similarly if using population data in which you would use instead the mean of the x-values \(\mu_x\) and the mean of the y-values \(\mu_y\).%
\end{definition}
%
\par
\hypertarget{p-223}{}%
\begin{theorem}{Alternate Formula for Covariance.}{}{theorem-3}%
%
\begin{equation*}
Cov(X,Y) = \frac{\sum_{k=0}^n x_k y_k}{n} - \overline{x} \cdot \overline{y}
\end{equation*}
\end{theorem}
\begin{proof}\hypertarget{proof-3}{}
\hypertarget{p-224}{}%
%
\begin{align*}
Cov(X,Y) & = \sum_{k=0}^n (x_k-\overline{x})(y_k-\overline{y})/n\\
& = \sum_{k=0}^n \left [ x_k y_k -\overline{x}  \cdot y_k-\overline{y} \cdot x_k + \overline{x} \cdot \overline{y} \right ]/n.\\
& = \sum_{k=0}^n x_k y_k /n - \overline{x}  \cdot \sum_{k=0}^n y_k /n - \overline{y} \cdot \sum_{k=0}^n x_k /n + \overline{x} \cdot \overline{y} 
\end{align*}
which simplifies to the desired result using the definition of the mean.%
\end{proof}
%
\par
\hypertarget{p-225}{}%
This general definition provides a general measure which is a second order term (like variance) but also maintains "units". To provide a unit-less metric, consider the following measure.%
\par
\hypertarget{p-226}{}%
\begin{definition}{Correlation Coefficient.}{CorrelationCoefficient}%
\hypertarget{p-227}{}%
Given a collection of data points, the correlation coefficient is given by%
\begin{equation*}
r = \frac{Cov(X,Y)}{s_x s_y}
\end{equation*}
where \(s_x\) is the standard deviation of the x-values only and \(s_y\) is the standard deviation of the y-values only. A similar statistics for population data would instead utilize \(\sigma_x\) and \(\sigma_y\) as the respective standard deviations of the x-values and y-values.%
\end{definition}
%
\par
\hypertarget{p-228}{}%
\begin{theorem}{Correlation Coefficient for Linear Data.}{}{theorem-4}%
\hypertarget{p-229}{}%
If the points are colinear with a positive slope then r=1 and if the points are collinear with a negative slope then r=-1.%
\end{theorem}
\begin{proof}\hypertarget{proof-4}{}
\hypertarget{p-230}{}%
Assume the data points are colinear with a positive slope. Then the \(TSE(m_0,b_0) = 0\) for some \(m_0\) and \(b_0\). For this line notice that \(f(x_k) = y_k\) exactly for all data points. It is easy to show then that \(\overline{y} = m_0 \overline{x} + b_0\) and \(s_y = | m_0 | s_x\). Therefore,%
\begin{equation*}
Cov(X,Y) = \sum_{k=0}^n (x_k-\overline{x})(m_0 x_k + b_0 - (m_0 \overline{x} + b_0))/n = m_0 s_x^2
\end{equation*}
Putting these together gives correlation coefficient%
\begin{equation*}
r = \frac{m_0 s_x^2}{s_x m_0 s_x} = 1.
\end{equation*}
A similar proof follows in the second case by noting that \(m_0 / | m_0 | = -1\).%
\end{proof}
%
\par
\hypertarget{p-231}{}%
\begin{definition}{Coefficient of Determination.}{definition-17}%
\hypertarget{p-232}{}%
Given the correlation coefficient r, the coefficient of determination is given by%
\begin{equation*}
r^2.
\end{equation*}
This measure indicates the percentage of the variation in y that can be explained by the collection of x values. Note, if r=1 (or r=-1), then the theorem above indicates that the linear model explains the variability for all of the y-values.%
\end{definition}
%
\par
\hypertarget{p-233}{}%
\begin{inlineexercise}{WebWork.}{exercise-11}%
\hypertarget{p-234}{}%
Interpreting correlation coefficients.%
\par\medskip
\hypertarget{p-235}{}%
For each problem, select the best response.%
\par
\hypertarget{p-236}{}%
(a) A study found a correlation of \emph{r} = -0.61 between the gender of a worker and his or her income.  You may correctly conclude%
\par
\hypertarget{p-237}{}%
\par
\begin{itemize}[label=$\odot$,leftmargin=3em,]
\item{}\hypertarget{p-238}{}%
an arithmetic mistake was made.  Correlation must be positive.%

\item{}\hypertarget{p-239}{}%
women earn more than men on average.%

\item{}\hypertarget{p-240}{}%
women earn less than men on average.%

\item{}\hypertarget{p-241}{}%
this is incorrect because \emph{r} makes no sense here.%

\item{}\hypertarget{p-242}{}%
None of the above.%

\end{itemize}
%
\par
\hypertarget{p-243}{}%
(b) For a biology project, you measure the weight in grams and the tail length in millimeters of a group of mice. The correlation is \emph{r} = 0.8. If you had measured tail length in centimeters instead of millimeters, what would be the correlation? (There are 10 millimeters in a centimeter.)%
\par
\hypertarget{p-244}{}%
\par
\begin{itemize}[label=$\odot$,leftmargin=3em,]
\item{}\hypertarget{p-245}{}%
0.8/10 = 0.08%

\item{}\hypertarget{p-246}{}%
(0.8)(10) = 8%

\item{}\hypertarget{p-247}{}%
0.8%

\item{}\hypertarget{p-248}{}%
None of the above.%

\end{itemize}
%
\par\medskip\noindent \hypertarget{p-249}{}%
%
\par
\end{inlineexercise}
%
\par
\hypertarget{p-250}{}%
\begin{inlineexercise}{WebWork.}{exercise-12}%
\hypertarget{p-251}{}%
Interpreting correlation coefficients.%
\par\medskip
\hypertarget{p-252}{}%
For each problem, select the best response.%
\par
\hypertarget{p-253}{}%
(a)   What are all the values that a correlation \emph{r} can possibly take?%
\par
\hypertarget{p-254}{}%
\par
\begin{itemize}[label=$\odot$,leftmargin=3em,]
\item{}\hypertarget{p-255}{}%
0 \(\le\) \emph{r} \(\le\) 1%

\item{}\hypertarget{p-256}{}%
\emph{r} \(\ge\) 0%

\item{}\hypertarget{p-257}{}%
-1  \(\le\) \emph{r} \(\le\) 1%

\item{}\hypertarget{p-258}{}%
None of the above.%

\end{itemize}
%
\par
\hypertarget{p-259}{}%
(b)   You have data for many years on the average price of a barrel of oil and the average retail price of a gallon of unleaded regular gasoline. When you make a scatterplot, the explanatory variable on the \emph{x} -axis%
\par
\hypertarget{p-260}{}%
\par
\begin{itemize}[label=$\odot$,leftmargin=3em,]
\item{}\hypertarget{p-261}{}%
can be either oil price or gasoline price.%

\item{}\hypertarget{p-262}{}%
is the price of gasoline.%

\item{}\hypertarget{p-263}{}%
is the price of oil.%

\item{}\hypertarget{p-264}{}%
None of the above.%

\end{itemize}
%
\par
\hypertarget{p-265}{}%
(c)   In a scatterplot of the average price of a barrel of oil and the average retail price of a gallon of gasoline, you expect to see%
\par
\hypertarget{p-266}{}%
\par
\begin{itemize}[label=$\odot$,leftmargin=3em,]
\item{}\hypertarget{p-267}{}%
a positive association.%

\item{}\hypertarget{p-268}{}%
a negative association.%

\item{}\hypertarget{p-269}{}%
very little association.%

\item{}\hypertarget{p-270}{}%
None of the above.%

\end{itemize}
%
\par\medskip\noindent \hypertarget{p-271}{}%
%
\par
\end{inlineexercise}
%
\par
\hypertarget{p-272}{}%
\begin{inlineexercise}{Correlation equaling 0.}{exercise-13}%
\hypertarget{p-273}{}%
Consider the data points (1,1), (1,2), (2,1), (2,2).  Plot these points and consider the nature of the best fit line. Show using software that the correlation coefficient is zero. Justify why TSE(m,b) = 1 must be the minimum.%
\end{inlineexercise}
%
%
%
\typeout{************************************************}
\typeout{Section 2.4 Higher Degree Linear Regression}
\typeout{************************************************}
%
\section[{Higher Degree Linear Regression}]{Higher Degree Linear Regression}\label{section-14}
\hypertarget{p-274}{}%
Continuing in a similar fashion to the previous section, consider now an approximation using a quadratic function \(f(x) = a x^2 + b x + c\).  In this case, the total squared error would be of the form%
\begin{equation*}
TSE(a,b,c) = \sum_{k=0}^n (a x_k^2 + b x_k + c - y_k)^2.
\end{equation*}
Taking all three partials gives%
\begin{equation*}
TSE_a = \sum_{k=0}^n 2(a x_k^2 + b x_k + c - y_k) \cdot x_k^2
\end{equation*}
%
\begin{equation*}
TSE_b = \sum_{k=0}^n 2(a x_k^2 + b x_k + c - y_k) \cdot x_k
\end{equation*}
%
\begin{equation*}
TSE_c = \sum_{k=0}^n 2(a x_k^2 + b x_k + c - y_k) \cdot 1 .
\end{equation*}
Once again, setting equal to zero and solving gives the normal equations for the best-fit quadratic%
\begin{equation*}
a \sum_{k=0}^n x_k^4 + b \sum_{k=0}^n x_k^3 + c \sum_{k=0}^n x_k^2 = \sum_{k=0}^n x_k^2 y_k
\end{equation*}
%
\begin{equation*}
a \sum_{k=0}^n x_k^3 + b \sum_{k=0}^n x_k^2 + c \sum_{k=0}^n x_k = \sum_{k=0}^n x_k y_k
\end{equation*}
%
\begin{equation*}
a \sum_{k=0}^n x_k^2 + b \sum_{k=0}^n x_k + c \sum_{k=0}^n 1 = \sum_{k=0}^n y_k.
\end{equation*}
%
\par
\hypertarget{p-275}{}%
Notice that even though you are creating the best-fit quadratic, to find that quadratic boils down to solving a (slightly larger) linear system.  In other words, linear regression again. Indeed, we can also approach the derivation of regression forumulas directly using a linear algebra approach. To do this, consider the equations generated by plugging in the data points \((x_0,y_0), (x_1, y_1), ... , (x_n, y_n)\) into the quadratic model. This yields a (likely overdetermined) system of equations.  Appending an error term \(\epsilon_k\) for each equation gives the following matrix form:%
%
\begin{equation*}
\begin{bmatrix}
y_0
\\y_1
\\ y_2
\\ ...
\\ y_n
\end{bmatrix}
=
\begin{bmatrix}
x_0^2 \amp x_0 \amp 1 \\ 
x_1^2 \amp x_1 \amp 1 \\ 
x_2^2 \amp x_2 \amp 1 \\ 
... \amp ... \amp ... \\ 
x_n^2 \amp x_n \amp 1 
\end{bmatrix} 
\cdot 
\begin{bmatrix}
a
\\ 
b
\\
c
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_0 \\ 
\epsilon_1 \\ 
\epsilon_2 \\ 
... \\ 
\epsilon_n 
\end{bmatrix}
\end{equation*}
\hypertarget{p-276}{}%
which in matrix form looks something like%
\begin{equation*}
Y = XA + \epsilon.
\end{equation*}
Solving for \(\epsilon\) and then minimizing \(\epsilon^t \epsilon\) yields the same solution as above. In matrix form, after some work this becomes%
\begin{equation*}
A = (X^t X)^{-1} X^t Y
\end{equation*}
with the matrix A containing the three unknowns a, b, and c.%
\par
\hypertarget{p-277}{}%
We can also use Matlab (or the opensource alternative "octave") to compute this linear algebra for us. The graph here using the sagecell is a text graph and is very rudimentary but plugging this code into Matlab or a desktop version of octave should present a very nice graph.%
\par
\hypertarget{p-278}{}%
\leavevmode%
\begin{sageinput}
x = [-1 0 1 3 5 5]
y = [5 3 0 -1 3 6]
n = max(size(x));
for k = 1:n
  X(k,1) = x(k)^2;
  X(k,2) = x(k);
  X(k,3) = 1;
end
Y = y';  # transpose the set of y-values to be a column vector
A = inv(X'*X)*X'*Y;
a = A(1)
b = A(2)
c = A(3)
u = min(x):0.1:max(x);  # create a set of input values for plotting
v = a * u.^2 + b * u + c;
plot(x,y,'o',u,v,'.')
\end{sageinput}
%
\par
\hypertarget{p-279}{}%
Cutting and pasting this code into perhaps http://octave-online.net gives a nice, non ASCII graph. Below, we do the same thing but using Sage.%
\par
\hypertarget{p-280}{}%
\leavevmode%
\begin{sageinput}
var('x')
xpts = vector(RR,(-1, 0, 1, 3, 5, 5))
ypts = vector(RR,(5, 3, 0, -1, 3, 6))
ones = vector(RR,(1, 1, 1, 1, 1, 1))
xpts2 = []     # accumulate the squares
pts = []       # accumulate the (x,y) pairs for plotting purposes
for k in range(len(xpts)):
    xpts2.append(xpts[k]^2)
    pts.append((xpts[k],ypts[k]))
xpts2 = vector(xpts2)

X = matrix(RR,[xpts2]).stack(xpts).stack(ones).transpose()  # create X
Y = matrix(RR,ypts).transpose()
Xt = X.transpose()
A = (Xt*X).inverse()*Xt*Y
[a,b,c] = [A[0][0], A[1][0], A[2][0]]
f = a*x^2+b*x+c
banner = "The quadratic interpolant is given by \(%s\)"%str(latex(f))
G = points(pts,size=20)
H = plot(f,x,min(xpts)-0.2,max(xpts)+0.2,title=banner)
show(G+H)
\end{sageinput}
%
\par
\hypertarget{p-281}{}%
\begin{inlineexercise}{Creating a Cubic Linear Regression Interpolant.}{exercise-14}%
\hypertarget{p-282}{}%
Modify the Sage code above to give a best-fit cubic interpolant.%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-6}{}\quad%
\begin{preformatted}
var('x')
xpts = vector((-1, 0, 1, 3, 5, 5))
ypts = vector((5, 3, 0, 4, 3, -1))
ones = vector((1, 1, 1, 1, 1, 1))
xpts3 = []
xpts2 = []
pts = []
for k in range(len(xpts)):
    xpts3.append(xpts[k]^3)
    xpts2.append(xpts[k]^2)
    pts.append((xpts[k],ypts[k]))
xpts3 = vector(xpts3)
xpts2 = vector(xpts2)

X = matrix([xpts3]).stack(xpts2).stack(xpts).stack(ones).transpose()
Y = matrix(ypts).transpose()
Xt = X.transpose()

A = (Xt*X).inverse()*Xt*Y
[a,b,c,d] = [A[0][0],A[1][0],A[2][0],A[3][0]]


f = a*x^3 + b*x^2 + c*x + d
banner = "The cubic interpolant is given by $%s$"%str(latex(f))
G = points(pts,size=20)
H = plot(f,x,min(xpts)-0.2,max(xpts)+0.2,title=banner)
show(G+H)
\end{preformatted}
\end{inlineexercise}
%
\par
\hypertarget{p-283}{}%
\begin{inlineexercise}{WebWork.}{exercise-15}%
\hypertarget{p-284}{}%
Doing Cubic Linear Regression...use your Sage work from above.%
\par\medskip
\hypertarget{p-285}{}%
In some cases, the best-fitting multiple regression equation is of the form \(\hat{y} = b_0 + b_1 x + b_2 x^2 + b_3 x^3.\) The graph of such an equation is called a cubic. Using the data set given below, and letting \(x_1 = x,\) \(x_2 = x^2,\) and \(x_3 = x^3,\) find the multiple regression equation for the cubic that best fits the given data.%
\hypertarget{p-286}{}%
%
\begin{equation*}
\begin{array}{c|ccccccc}
x \amp  -8 \amp  -5 \amp  -2 \amp  -1 \amp  4 \amp  6 \amp  8 \cr
\hline
y \amp  36.2 \amp  14.7 \amp  2 \amp  -0.9 \amp  -19.9 \amp  -37.4 \amp  -61.7
\end{array}
\end{equation*}
%
\par
\hypertarget{p-287}{}%
The equation is \(\hat{y} =\)  \fillin{10} \(+\)  \fillin{10} \(x+\)  \fillin{10} \(x^2 +\)  \fillin{10} \(x^3.\)%
\end{inlineexercise}
%
%
%
\typeout{************************************************}
\typeout{Section 2.5 Multi-variable Linear Regression}
\typeout{************************************************}
%
\section[{Multi-variable Linear Regression}]{Multi-variable Linear Regression}\label{section-15}
\hypertarget{p-288}{}%
The regression models that we have looked at presumed a single independent variable. It is much more likely when investigating cause and effect relationships that there are perhaps many independent variables that contribute.%
\par
\hypertarget{p-289}{}%
Let's consider a linear model with two independent variables. Indeed, a basic two-variable linear model of the form%
\begin{equation*}
z = \alpha_1 x + \alpha_2 y + \beta
\end{equation*}
can be used to approximate data points%
\begin{equation*}
(x_0,y_0,z_0), (x_1,y_1,z_1), (x_2,y_2,z_2), ... , (x_n,y_n,z_n) . 
\end{equation*}
Using a linear systems approach similar to the previous section by evaluating at these data points and appending an error term to each equation gives, in matrix form:%
\par
\hypertarget{p-290}{}%
%
\begin{equation*}
\begin{bmatrix}
z_0
\\z_1
\\ z_2
\\ ...
\\ z_n
\end{bmatrix}
=
\begin{bmatrix}
x_0 \amp y_0 \amp 1 \\ 
x_1 \amp y_1 \amp 1 \\ 
x_2 \amp y_2 \amp 1 \\ 
... \amp ... \amp ... \\ 
x_n \amp y_n \amp 1 
\end{bmatrix} 
\cdot 
\begin{bmatrix}
\alpha_1
\\ 
\alpha_2
\\
\beta
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_0 \\ 
\epsilon_1 \\ 
\epsilon_2 \\ 
... \\ 
\epsilon_n 
\end{bmatrix}
\end{equation*}
%
\par
\hypertarget{p-291}{}%
where the \(\epsilon_k\) terms are the deviation between the exact data point and the approximation of that point on some plane. Symbolically%
\begin{equation*}
Z = XA + \epsilon.
\end{equation*}
If all of the points lie on the same plane (unlikely), then \(\epsilon = 0\). Otherwise, once again applying a least squares solution approach is the same as minimizing \(\epsilon^t \epsilon\) and eventually gives%
\begin{equation*}
A = (X^t X)^{-1} X^t Z
\end{equation*}
in general.  Evaluating this with X and Z as above gives the needed coefficients%
\begin{equation*}
A = \begin{bmatrix}
\alpha_1 \\ 
\alpha_2 \\  
\beta 
\end{bmatrix}
\end{equation*}
%
\par
\hypertarget{p-292}{}%
Let's first see how this is done automatically in R using one of the built-in data sets%
\par
\hypertarget{p-293}{}%
\leavevmode%
\begin{sageinput}
x1 &- c(1, 2, 3, 5, 5)
x2 &- c(1, 1, 2, 2, 3)
y &- c(5, 1, 4, 3, -1)
fit <- lm(y ~ x1+x2, data=(x,y,z))
summary(fit)             # basic results
coefficients(fit)        # a,b,c, for y = a x1 + b x2 + c x3
fitted(fit)              # predicted values
residuals(fit)           # errors
\end{sageinput}
%
\par
\hypertarget{p-294}{}%
A good example of the usefulness and limitations of multi-variate linear regression is the calculation of the "Heat Index". This measure determines a measure of discomfort relative to the ambient temperature and the relative humidity. Indeed, in warm climates a high temperature is more difficult to bear if the humidity is also high. One reason is that with high humidity the body is less effective in shedding heat through evaporation of body sweat.%
\par
\hypertarget{p-295}{}%
The National Weather Service in 1990 published the following multiple regression equation for Heat Index (HI) relative to the ambient temperature (T) and the relative humidity (RH)%
\begin{align*}
H & = -42.379 + 2.04901523 \cdot T + 10.14333127 \cdot R - 0.22475541 \cdot T \cdot R \\
& - 6.83783 \cdot 10^{-3} \cdot T^2 - 5.481717 \cdot 10^{-2} \cdot R^2 + 1.22874 \cdot 10^{-3} \cdot T^2 \cdot R \\
& + 8.5282 \cdot 10^{-4} \cdot T \cdot R^2-1.99 \cdot 10^{-6} \cdot T^2 \cdot R^2.
\end{align*}
Since this model utilizes a linear combination of terms and it's derivation could also be generated using a generalization of the linear regression method presented above. Details on how this equation was determined and other details are available at https://www.wpc.ncep.noaa.gov/html/heatindex_equation.shtml .%
\par
\hypertarget{p-296}{}%
\leavevmode%
\begin{sageinput}
@interact
def _(T = (90),R = (95)):
    H =-42.379+2.04901523*T+10.14333127*R \
    -0.22475541*T*R-6.83783*10^(-3)*T^2 \
    -5.481717*10^(-2)*R^2+1.22874*10^(-3)*T^2*R \
    +8.5282*10^(-4)*T*R^2-1.99*10^(-6)*T^2*R^2
    print "For T = ",T," with humidity = ",R,"percent , Heat Index =",H
\end{sageinput}
%
\par
\hypertarget{p-297}{}%
Below one can compute a table for various ambient Temperature readings given one value for relative humidity.  Notice what happens for a relatively high humidity and relatively high temperature.%
\par
\hypertarget{p-298}{}%
\leavevmode%
\begin{sageinput}
R = 95
for T in range(80,121):
    H = -42.379+2.04901523*T+10.14333127*R \
    -0.22475541*T*R-6.83783*10^(-3)*T^2 \
    -5.481717*10^(-2)*R^2+1.22874*10^(-3)*T^2*R \
    +8.5282*10^(-4)*T*R^2-1.99*10^(-6)*T^2*R^2
    print "For T = ",T," with humidity = ",R,"percent , Heat Index =",H
\end{sageinput}
%
\par
\hypertarget{p-299}{}%
Indeed, you cannot roast a turkey by simply turning the oven on 120 and pumping in a lot of humidity since the turkey is not trying to cool itself anymore. Any discomfort measured on the turkey's behalf would certainly be matched by the human since the bird would be a rare bird and remain very much uncooked. The issue is that this model doesn't presume the possibility of 120F and 95% humidity. Often, in situations where the temperature is able to reach that level, such as a desert, then the relative humidity is correspondingly low. This idea of using a model to predict extreme values beyond the measured data is called extrapolation and should be utilized with care. Interpolation to estimate values within the confines of the measured data is however generally a safe bet.%
%
%
\typeout{************************************************}
\typeout{Section 2.6 Summary}
\typeout{************************************************}
%
\section[{Summary}]{Summary}\label{section-16}
\hypertarget{p-300}{}%
Here are the important formulas from this section:%
\hypertarget{p-301}{}%
Later%
%
%
\typeout{************************************************}
\typeout{Chapter 3 Counting and Combinatorics}
\typeout{************************************************}
%
\chapter[{Counting and Combinatorics}]{Counting and Combinatorics}\label{Combinatorics}
%
%
\typeout{************************************************}
\typeout{Section 3.1 Introduction}
\typeout{************************************************}
%
\section[{Introduction}]{Introduction}\label{section-17}
\hypertarget{p-302}{}%
One of the earliest applications of mathematics you probably remember is how you could use number to count things. For many, this is what they think people do when they do mathematics. In this chapter, we will discover that it is possible to count items without actually listing them all.%
\par
\hypertarget{p-303}{}%
\begin{example}{Counting by actually listing out all possibilities.}{example-13}%
\hypertarget{p-304}{}%
Consider counting the number of ways one can arrange Peter, Paul, and Mary with the order important.  Listing the possibilities: \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}Peter, Paul, Mary%
\item{}Peter, Mary, Paul%
\item{}Paul, Peter, Mary%
\item{}Paul, Mary, Peter%
\item{}Mary, Peter, Paul%
\item{}Mary, Paul, Peter%
\end{itemize}
 So, it is easy to see that these are all of the possible outcomes and that the total number of such outcomes is 6. What happens however if we add Simone to the list? \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}Simone, Peter, Paul, Mary%
\item{}Simone, Peter, Mary, Paul%
\item{}Simone, Paul, Peter, Mary%
\item{}Simone, Paul, Mary, Peter%
\item{}Simone, Mary, Peter, Paul%
\item{}Simone, Mary, Paul, Peter%
\item{}Peter, Simone, Paul, Mary%
\item{}Peter, Simone, Mary, Paul%
\item{}Paul, Simone, Peter, Mary%
\item{}Paul, Simone, Mary, Peter%
\item{}Mary, Simone, Peter, Paul%
\item{}Mary, Simone, Paul, Peter%
\item{}Peter, Paul, Simone, Mary%
\item{}Peter, Mary, Simone, Paul%
\item{}Paul, Peter, Simone, Mary%
\item{}Paul, Mary, Simone, Peter%
\item{}Mary, Peter, Simone, Paul%
\item{}Mary, Paul, Simone, Peter%
\item{}Peter, Paul, Mary, Simone%
\item{}Peter, Mary, Paul, Simone%
\item{}Paul, Peter, Mary, Simone%
\item{}Paul, Mary, Peter, Simone%
\item{}Mary, Peter, Paul, Simone%
\item{}Mary, Paul, Peter, Simone%
\end{itemize}
 Notice how the list quickly grows when just one more choice is added. This example illustrates how keeping track of the number of items in a set can quickly get impossible to manage unless we can use a more mathematical approach that allows you to count the number of possibilities without having to list them all.%
\end{example}
%
%
%
\typeout{************************************************}
\typeout{Section 3.2 General Counting Principles}
\typeout{************************************************}
%
\section[{General Counting Principles}]{General Counting Principles}\label{section-18}
\hypertarget{p-305}{}%
\begin{definition}{Cardinality.}{definition-18}%
\hypertarget{p-306}{}%
Given a set of elements A, the number of elements in the set is known as its cardinality and is denoted |A|. If the set has an infinite number of elements then we set |A| = \(\infty\).%
\end{definition}
%
\par
\hypertarget{p-307}{}%
In order to "count without counting" we establish the following foundational principle:%
\par
\hypertarget{p-308}{}%
\begin{theorem}{Multiplication Principle.}{}{MultiplicationPrinciple}%
\hypertarget{p-309}{}%
Given two successive events A and B, the number of ways to perform A and then B is |A||B|.%
\end{theorem}
\begin{proof}\hypertarget{proof-5}{}
\hypertarget{p-310}{}%
If either of the events has infinite cardinality, then it is clear that the number of ways to perform A and then B will also be infinite. So, assume that both |A| and |B| are finite. In order to count the successive events, enumerate the elements in each set%
\begin{gather*}
A = \left \{ a_1, a_2, a_3, ... , a_{|A|} \right \}\\
B = \left \{  b_1, b_2, b_3, ... , b_{|B|} \right \}
\end{gather*}
and consider the function f(k,j) = (k-1)|B| + j. This function is one-to-one and onto from the set%
\begin{gather*}
\left \{ (k,j): 1 \le k \le |A|, 1 \le j \le |B| \right \} 
\end{gather*}
onto%
\begin{gather*}
\left \{ s : 1 \le s \le |A| |B| \right \}.
\end{gather*}
Since this second set has |A| |B| elements then the conclusion follows. coordinates.%
\end{proof}
%
\par
\hypertarget{p-311}{}%
\begin{inlineexercise}{WebWork.}{exercise-16}%
\hypertarget{p-312}{}%
Let's apply the Multiplication Principle.%
\par\medskip
\hypertarget{p-313}{}%
A fair \(6\)-sided die is rolled \(10\) times and the resulting sequence of \(10\) numbers is recorded.%
\par
\hypertarget{p-314}{}%
How many different sequences are possible?   \fillin{20}%
\par
\hypertarget{p-315}{}%
How many different sequences consist entirely of even numbers?  \fillin{20}%
\par
\hypertarget{p-316}{}%
How many different sequences are possible if the first, third, and fourth numbers must be the same?  \fillin{20}%
\end{inlineexercise}
%
\par
\hypertarget{p-317}{}%
\begin{inlineexercise}{WebWork.}{exercise-17}%
\hypertarget{p-318}{}%
%
\par\medskip
\hypertarget{p-319}{}%
Find how many positive integers with exactly four decimal digits, that is, positive integers between 1000 and 9999 inclusive, have the following properties:%
\par
\hypertarget{p-320}{}%
(a) are not divisible by either 5 or 7.%
\par
\hypertarget{p-322}{}%
(b) are divisible by 5 but not by 7.%
\par
\hypertarget{p-324}{}%
(c) are divisible by 7.%
\par
\hypertarget{p-326}{}%
(d) are divisible by 5 and by 7.%
\end{inlineexercise}
%
\par
\hypertarget{p-328}{}%
\begin{definition}{Factorial.}{Factorial}%
\hypertarget{p-329}{}%
For any natural number n,%
\begin{gather*}
n! = n(n-1)(n-2) ... 3 \cdot 2 \cdot 1
\end{gather*}
and by convention set 0! = 1.%
\end{definition}
%
\par
\hypertarget{p-330}{}%
\begin{example}{iPad security code.}{example-14}%
\hypertarget{p-331}{}%
Consider your ipad's security. To unlock the screen suppose you need to enter a four digit pass code. How easy is it to guess this pass code?%
\par
\hypertarget{p-332}{}%
Using the standard 10 digit keypad, we first have two questions to consider? \leavevmode%
\begin{enumerate}
\item\hypertarget{li-60}{}Does the order in which the digits are entered matter?%
\item\hypertarget{li-61}{}Can you reuse a digit more than once?%
\end{enumerate}
 For the ipad, if the order does matter and you cannot reuse digits, the number of possible codes can be determined by considering each digit as a separate event with four such events in succession providing the right code. By successively applying the multiplication principle, you find that the number of possible codes is the number of remaining available digits at each step.  Namely, \(10 \times 9 \times 8 \times 7 = 5040.\)%
\par
\hypertarget{p-333}{}%
On the other hand, if you were allowed to reuse the digits then the number of possible outcomes would be more since all 10 digits would be available for each event.  Namely, \(10 \times 10 \times 10 \times 10 = 10000.\)%
\par
\hypertarget{p-334}{}%
Now, consider how this changes if you can use a 4 or 6 digit passcode. Determine the number of possible passcodes.%
\end{example}
%
\par
\hypertarget{p-335}{}%
\begin{example}{iPad security code with greasy fingers.}{example-15}%
\hypertarget{p-336}{}%
Reconsider your ipad's security. In this case, you like to eat chocolate bars and have greasy fingers. When you type in your passcode your fingers leave a residue over the four numbers pressed. If someone now tries to guess your passcode, how many possible attempts are necessary?%
\par
\hypertarget{p-337}{}%
Since there are only four numbers to pick from with order important, the number of possible passcodes remaining is \(4 \times 3 \times 2 \times 1 = 24\)%
\end{example}
%
\par
\hypertarget{p-338}{}%
\begin{example}{National Treasure.}{example-16}%
\hypertarget{p-339}{}%
In the 2004 movie National Treasure, Ben and Riley are attempting to guess Abagail's password to enter the room with the Declaration. They are able to determine the passphrase to get into the vault room by doing a scan that detects the buttons pushed (not due to chocolate but just due to the natural oils on fingers). They notice that the buttons pushed include the characters AEFGLORVY.%
\par
\hypertarget{p-340}{}%
Assuming these characters are used only once each, how many possible passphrases are possible?%
\par
\hypertarget{p-341}{}%
In this case, the order of the characters matters but all of the characters are distinct. Since we have 9 characters provided, the we can consider each character as an event with the first event as a choice from the 9, the second event as a choice from the remaining 8, etc. This gives \(9 \times 8 \times ... \times 1 = 362880\) possible passphrases.%
\par
\hypertarget{p-342}{}%
Assuming that some of the characters could be used more than once, how many passphrases need to be considered if the total length of passphrase can be at most 12 characters?%
\par
\hypertarget{p-343}{}%
Notice, in this case you don't know which characters might be reused and so the number of possible outcomes will be much larger. What is the answer?%
\par
\hypertarget{p-344}{}%
You can break this problem down into distinct cases: \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}\hypertarget{p-345}{}%
Using 9 characters: The answer was computed above.%
\item{}\hypertarget{p-346}{}%
Using 10 characters: In this case, 1 character can be used twice. To determine the number of possibilities, let's first pick which character can be doubled. There are 9 options for picking that character.  Next, if we consider the two instances of that letter as distinct values then we can just count the number of ways to arrange unique 10 characters which is 10! However, swapping the two characters (which are actually identical) would not give a new passphrase. Since these are counted twice, let's divide these out to give 10!/2.%
\item{}\hypertarget{p-347}{}%
Using 11 characters: In this situation we have two unique options: %
\begin{itemize}[label=$\circ$]
\item{}\hypertarget{p-348}{}%
One character is used three times and the others just once.%
\par
\hypertarget{p-349}{}%
Continuing as in the previous case, 11!/3!.%
Two characters are used twice and the others just once.\end{itemize}
%
\item{}\hypertarget{p-350}{}%
Using 12 characters%
%
\begin{enumerate}
\item\hypertarget{li-68}{}One letter from the nine is used four times and all the others are used once.%
\item\hypertarget{li-69}{}One letter is used three times, another letter is used two times, and the others are used once.%
\item\hypertarget{li-70}{}Three letters are used twice and the others are used once.%
\end{enumerate}
\end{itemize}
%
\par
\hypertarget{p-351}{}%
With this large collection of possible outcomes, how are the movie characters able to determine the correct "VALLEYFORGE" passphrase?%
\end{example}
%
%
%
\typeout{************************************************}
\typeout{Section 3.3 Permutations}
\typeout{************************************************}
%
\section[{Permutations}]{Permutations}\label{section-19}
\hypertarget{p-352}{}%
When counting various outcomes the order of things sometimes matters. When the order of a set of elements changes we call the second a permutation (or an arrangement) of the first.%
\par
\hypertarget{p-353}{}%
\begin{theorem}{Permutations of everything.}{}{theorem-6}%
\hypertarget{p-354}{}%
The number of permutations of n distinct items is n!%
\end{theorem}
\begin{proof}\hypertarget{proof-6}{}
\hypertarget{p-355}{}%
Notice that if n=1, then there is only 1 item to arrange and that there is only one possible arrangment.%
\par
\hypertarget{p-356}{}%
By induction, assume that any set with n elements has n! arrangments and assume that%
\begin{gather*}
A = \left \{ a_1, a_2, ... , a_n, a_{n+1} \right \}.
\end{gather*}
Notice that there are n+1 ways to choose 1 element from A and that in doing so leaves a set with n elements. Combining the induction hypothesis with the multiplication principle this gives (n+1)n! = (n+1)! possible outcomes.%
\end{proof}
%
\par
\hypertarget{p-357}{}%
One can interpret successive ordered selections as branching through a "tree" structure.  Indeed, starting with the set {A,B,C} one may pick any of the three but then a subsequent selection only has two possibilities for the next selection and so forth. The tree below illustrates that there are six ways to order two items from a group of size three. \begin{figure}\centering\includegraphics[width=1\linewidth]{images/PermutationSmall.png}
\end{figure}%
\par
\hypertarget{p-358}{}%
Going one step further, what about ordering the letters in {A, B, C, D}? You can start by picking one of the four letters, say A, and then arranging B, C, and D.  Then, start with B and arrange A, C, and D and so on.  This gives:%
\par
\hypertarget{p-359}{}%
ABCD, ABDC, ACBD, ACDB, ADBC, ADCB%
\par
\hypertarget{p-360}{}%
BACD, BADC, BCAD, BCAB, BDAC, BDCA%
\par
\hypertarget{p-361}{}%
CBAD, CBDA, CABD, CADB, CDBA, CDAB%
\par
\hypertarget{p-362}{}%
DBCA, DBAC, DCBA, DCAB, DABC, DACB%
\par
\hypertarget{p-363}{}%
which is 24 different options or \(4! = 4 \cdot 3 \cdot 2 \cdot 1 = 24\).  This can be viewed using a tree structure where each decision creates a new branch. \begin{figure}\centering\includegraphics[width=1\linewidth]{images/PermutationAllTree.png}
\end{figure}%
\par
\hypertarget{p-364}{}%
\begin{theorem}{Permutations of a subset without replacement.}{}{PermutationWithoutReplacementDistinguishable}%
\hypertarget{p-365}{}%
The number of ways to arrange r items from a set of n distinct items is%
\begin{equation*}
_nP_r = \frac{n!}{(n-r)!} 
\end{equation*}
This is sometimes denoted also as P(n,r) or \(P_r^n\).%
\end{theorem}
\begin{proof}\hypertarget{proof-7}{}
\hypertarget{p-366}{}%
If \(r \gt n\) or \(r \lt 0 \) then this is not possible and so the result would be no permuatations. Otherwise, apply the multiplication principle r times noting that there are n choices for the first selection, n-1 choices for the second selection, and with n-r+1 choices for the rth selection. This gives%
\begin{align*}
_nP_r & = n(n-1) ... (n-r+1)\\
& = n(n-1) ... (n-r+1)\frac{(n-r)!}{(n-r)!}\\
& = \frac{n(n-1) ... (n-r+1)(n-r)!}{(n-r)!}\\
& = \frac{n!}{(n-r)!}\qedhere
\end{align*}
%
\end{proof}
%
\par
\hypertarget{p-367}{}%
Following the tree idea from above, continue for several steps but then stop once you have gone r steps in.  For example, it is easy to see that \(_5P_2 = 20\) using a tree. \begin{figure}\centering\includegraphics[width=1\linewidth]{images/PermutationSubTree.png}
\end{figure}%
\par
\hypertarget{p-368}{}%
\begin{inlineexercise}{WebWork.}{exercise-18}%
\hypertarget{p-369}{}%
Let's apply the the Permutation formula.%
\par\medskip
\hypertarget{p-370}{}%
In how many ways can 3 different novels, 4 different mathematics books, and 1 biology book be arranged on a bookshelf if%
\par
\hypertarget{p-371}{}%
(a) the books can be arranged in any order?%
\par
\hypertarget{p-372}{}%
Answer:  \fillin{20}%
\par
\hypertarget{p-373}{}%
(b) the mathematics books must be together and the novels must be together?%
\par
\hypertarget{p-374}{}%
Answer:  \fillin{20}%
\par
\hypertarget{p-375}{}%
(c) the novels must be together but the other books can be arranged in any order?%
\par
\hypertarget{p-376}{}%
Answer:  \fillin{20}%
\par\medskip\noindent \hypertarget{p-377}{}%
So, these are simple calculations.%
\par
\end{inlineexercise}
%
\par
\hypertarget{p-378}{}%
\begin{theorem}{Permutations of a subset with replacement.}{}{PermutationWithReplacementDistinguishable}%
\hypertarget{p-379}{}%
The number of ways to obtain an arrangement of r choices from a group of size n is%
\begin{equation*}
n^r
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-8}{}
\hypertarget{p-380}{}%
Use the multiplication principle r times and see that for each choice all n objects in the universe remain available.  That is,%
\begin{equation*}
n \cdot n \cdot n ... n = n^r\qedhere
\end{equation*}
%
\end{proof}
%
\par
\hypertarget{p-381}{}%
\begin{theorem}{Permutations when not all items are distinguishable (Multinomial Coefficients).}{}{MultinomialCoefficients}%
\hypertarget{p-382}{}%
If n items belong to s categories, \(n_1\) in first, \(n_2\) in second, ... , \(n_s\) in the last, the number of ways to pick all is%
\begin{equation*}
\frac{n!}{n_1! \cdot n_2! ... n_s!}
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-9}{}
\hypertarget{p-383}{}%
Enumerate all of the n data items individually with the \(n_1!\) identical values first and the remaining groups in like manner to get the enumerated list%
\begin{equation*}
\left { x_{1,1}, ..., x_{1,n_1}, x_{2,1}, ..., x_{2,n_2}, ... , x_{s,1}, ..., x_{s,n_2}, \right }
\end{equation*}
In this order, there are \(n_1!\) ways to arrange the first group, \(n_2!\) ways to arrange the second, etc. There are \(n_1! \times n_2! \times ... \times \n_s!\) ways to arrange all of the categories together with groups in this order but none of those group reorders does anything since those data values are all the same. Dividing out those from the \(n!\) original permutations of all items leaves one with the multinomial coefficient.%
\end{proof}
%
\par
\hypertarget{p-384}{}%
\begin{inlineexercise}{WebWork.}{exercise-19}%
\hypertarget{p-385}{}%
Let's apply the this new Permutation formula.%
\par\medskip
\hypertarget{p-386}{}%
How many anagrams can be created from the word 'accommodate' if the new words do not need to be meaningful?  \fillin{35}%
\par\medskip\noindent \hypertarget{p-387}{}%
Another one bites the dust.%
\par
\end{inlineexercise}
%
%
%
\typeout{************************************************}
\typeout{Section 3.4 Combinations}
\typeout{************************************************}
%
\section[{Combinations}]{Combinations}\label{section-20}
\hypertarget{p-388}{}%
When counting various outcomes sometimes the order of things does not matter. If so, each unique unordered outcome is called a combination.%
\par
\hypertarget{p-389}{}%
Once again, consider the permutations when selecting three letters from {A, B, C, D}. \leavevmode%
\begin{enumerate}
\item\hypertarget{li-71}{}%
\begin{itemize}[label=\textbullet]
\item{}A,B,C%
\item{}A,C,B%
\item{}B,A,C%
\item{}B,C,A%
\item{}C,A,B%
\item{}C,B,A%
\end{itemize}
%
\item\hypertarget{li-78}{}%
\begin{itemize}[label=\textbullet]
\item{}A,B,D%
\item{}A,D,B%
\item{}B,A,D%
\item{}B,D,A%
\item{}D,A,B%
\item{}D,B,A%
\end{itemize}
%
\item\hypertarget{li-85}{}%
\begin{itemize}[label=\textbullet]
\item{}A,C,D%
\item{}A,D,C%
\item{}C,A,D%
\item{}C,D,A%
\item{}D,A,C%
\item{}D,C,A%
\end{itemize}
%
\item\hypertarget{li-92}{}%
\begin{itemize}[label=\textbullet]
\item{}B,C,D%
\item{}B,D,C%
\item{}C,B,D%
\item{}C,D,B%
\item{}D,B,C%
\item{}D,C,B%
\end{itemize}
%
\end{enumerate}
 Notice how these 24 permutations fall into only four distinct categories if the order does not matter. Therefore, from a group of size four you can pick an unordered subset of size three in only 4 ways rather than the original 24.%
\par
\hypertarget{p-390}{}%
In general, it would be nice to have a direct formula to determine the number of such combinations without having to explicitly list them all out.%
\par
\hypertarget{p-391}{}%
\begin{theorem}{Combinations of a subset without replacement.}{}{CombinationWithoutReplacementDistinguishable}%
\hypertarget{p-392}{}%
The number of ways to arrange r items from a set of n distinct items is%
\begin{equation*}
_nC_r = \frac{n!}{r!(n-r)!} 
\end{equation*}
This is sometimes denoted C(n,r) or \(C_r^n\) or \({n \choose r}\).%
\end{theorem}
\begin{proof}\hypertarget{proof-10}{}
\hypertarget{p-393}{}%
Consider creating a permutation of r objects from a set of size n by first picking an unordered subset of size r and then counting the number of ways to order that subset. Using our notation and the multiplication principle,%
\begin{equation*}
_nP_r = _nC_r \cdot r!
\end{equation*}
Dividing by \(r!\) gives the result.%
\end{proof}
%
\par
\hypertarget{p-394}{}%
\begin{theorem}{Combinations of a subset with replacement.}{}{CombinationWithReplacementDistinguishable}%
\hypertarget{p-395}{}%
The number of ways to arrange r items from a set of n distinct items is%
\begin{equation*}
_{n+r-1}C_r = {{n+r-1} \choose r} = \frac{(r+n-1)!}{r!(n-1)!} 
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-11}{}
\hypertarget{p-396}{}%
Label each item in your group in some defined order. Since order doesn't matter, as you repeatedly sample r times with replacement you can always write down your outcomes sorted from low to high placement. Finally, separate like values by some symbol, say "|", and consider each of the n distinct objects as indistinct *'s. There will be n-1 of these separators since there will be n to choose from. For example, if choosing r=6 times from the set {a, b, c, d}, then the outcome b, b, a, d, a, b could be collected as a, a, b, b, b, d and written in our code as **|***||* .  Notice that shuffling around the identical *'s would not change the code (and similarly for the identical |'s) but swapping a * with a | would be a different outcome. Therefore, we can consider this to be a \hyperref[MultinomialCoefficients]{multinomial coefficient~\ref{MultinomialCoefficients}} and the number of ways to rearrange this code is%
\begin{equation*}
\frac{(r + n-1)!}{r!(n-1)!}.\qedhere
\end{equation*}
%
\end{proof}
%
\par
\hypertarget{p-397}{}%
\begin{inlineexercise}{WebWork.}{exercise-20}%
\hypertarget{p-398}{}%
Let's apply the this new Combination forumula.%
\par\medskip
\hypertarget{p-399}{}%
A standard deck of cards consists of four suits (clubs, diamonds, hearts, and spades), with each suit containing 13 cards (ace, two through ten, jack, queen, and king) for a total of 52 cards in all.%
\par
\hypertarget{p-400}{}%
How many 7-card hands will consist of exactly 4 hearts and 3 clubs?%
\par\medskip\noindent \hypertarget{p-402}{}%
Notice that to determine the number of outcomes required you to use the combination forumula several times and then multiply the results using the multiplication principle.%
\par
\end{inlineexercise}
%
\par
\hypertarget{p-403}{}%
\begin{inlineexercise}{WebWork.}{exercise-21}%
\hypertarget{p-404}{}%
Use the new Combination formula again.%
\par\medskip
\hypertarget{p-405}{}%
A school dance committee is to consist of 2 freshmen, 3 sophomores, 4 juniors, and 5 seniors. If 6 freshmen, 8 sophomores, 7 juniors, and 8 seniors are eligible to be on the committee, in how many ways can the committee be chosen?%
\par
\hypertarget{p-406}{}%
Your answer is :  \fillin{25}%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-8}{}\quad%
There are \({6 \choose 2}\) ways to choose 2 freshmen for the committee, \({8 \choose 3}\) ways to choose 3 sophomores for the committee, \({7 \choose 4}\) ways to choose 4 juniors for the committee, and \({8 \choose 5}\) ways to choose 5 seniors for the committee. So by the generalized basic principle of counting, there are a total of%
\begin{equation*}
{6 \choose 2} \cdot {8 \choose 3} \cdot {7 \choose 4} \cdot {8 \choose 5}
= \frac{6 !}{2!4 !} \cdot \frac{8 !}{3! 5 !} \cdot \frac{7 !}{4! 3 !} \cdot \frac{8 !}{5! 3 !} = 1646400
\end{equation*}
different possible committees.\par\medskip\noindent \hypertarget{p-407}{}%
Once again, you can see that using the formulas can be easy and also can be part of a bigger problem pasted together using the multiplication principle.%
\par
\end{inlineexercise}
%
\par
\hypertarget{p-408}{}%
\begin{example}{Ipad Security.}{example-17}%
\hypertarget{p-409}{}%
Revisiting your ipad's security, what happens if the order in which the digits are entered does not matter? If so, then you would be picking a combination of 4 digits without replacement from a group of 10 digits. Namely,%
\begin{align*}
\frac{10!}{4!6!} & = \frac{10 \times 9 \times 8 \times 7 \times 6!}{4 \times 3 \times 2 \times 1 \times 6!}\\
& = \frac{10 \times 9 \times 8 \times 7}{4 \times 3 \times 2 \times 1}\\
& = \frac{5040}{24}\\
& = 210.
\end{align*}
Notice that the total number of options is much smaller when order does not matter.%
\par
\hypertarget{p-410}{}%
Note that if you were allowed to reuse the digits then the number of possible outcomes would be%
\begin{align*}
\frac{13!}{4!9!} & = \frac{13 \times 12 \times 11 \times 10}{4 \times 3 \times 2 \times 1} \\
& = 715
\end{align*}
which once again is more since numbers are allowed to repeat.%
\end{example}
%
\par
\hypertarget{p-411}{}%
\begin{definition}{Binomial Coefficients.}{BinomialCoefficients}%
\hypertarget{p-412}{}%
The value \(_nC_r\) is known as the binomial coefficient. It is denoted by \({n \choose r}\) and is read "n choose k".%
\end{definition}
%
\par
\hypertarget{p-413}{}%
Binomial coefficients have a number of interesting properties.  Many of these are very useful as well in probability calculations.  Several of these properties are collected below. In particular, these relationships verify that the binomial coefficients are the values found in Pascal's Triangle.%
\par
\hypertarget{p-414}{}%
\leavevmode%
\begin{sageinput}
@interact
def _(n = slider(1,15,1,5)):
    for row in range(n+1):
        binoms = sorted(binomial_coefficients(row).items())
        given_n = []
        for k in range(row+1):
            given_n.append(binoms[k][1])
        pretty_print('%s'%given_n)
\end{sageinput}
%
\par
\hypertarget{p-415}{}%
\begin{theorem}{Binomial Coefficient Formulas.}{}{theorem-12}%
\hypertarget{p-416}{}%
For \(n \in \mathbb{N}\), \leavevmode%
\begin{enumerate}
\item\hypertarget{li-99}{}\(\binom{n}{0} = 1\)%
\item\hypertarget{li-100}{}\(\binom{n}{n} = 1\)%
\item\hypertarget{li-101}{}\(\binom{n}{1} = n\)%
\item\hypertarget{li-102}{}\(\binom{n}{n-1} = n\)%
\item\hypertarget{li-103}{}\(\binom{n}{r} = \binom{n}{n-r}\)%
\item\hypertarget{li-104}{}\(\binom{n+1}{r+1} = \binom{n}{r} + \binom{n}{r+1}\)%
\end{enumerate}
%
\end{theorem}
\begin{proof}\hypertarget{proof-12}{}
\hypertarget{p-417}{}%
\leavevmode%
\begin{enumerate}
\item\hypertarget{li-105}{}\(\binom{n}{0} = \frac{n!}{0!(n-0)!} = 1\)%
\item\hypertarget{li-106}{}\(\binom{n}{n} = \frac{n!}{n!(n-n)!} = 1\)%
\item\hypertarget{li-107}{}\(\binom{n}{1} = \frac{n!}{1!(n-1)!} = n\)%
\item\hypertarget{li-108}{}\(\binom{n}{n-1} = \frac{n!}{(n-1)!(n-(n-1))!} = n\)%
\item\hypertarget{li-109}{}\(\binom{n}{r} = \frac{n!}{r!(n-r)!} = \frac{n!}{(n-r)!(n-(n-r))!} = \binom{n}{n-r}\)%
\item\hypertarget{li-110}{}%
\begin{align*}
\binom{n}{r} + \binom{n}{r+1} & = \frac{n!}{r!(n-r)!} + \frac{n!}{(r+1)!(n-(r+1))!}\\
& = (r+1) \frac{n!}{(r+1)!(n-r)!} // + (n-r) \frac{n!}{(r+1)!(n-r))!}\\
& = \frac{(r+1) n! + (n-r)n!}{(r+1)!(n-r)!}\\
& = \frac{(n+1) n!}{(r+1)!((n+1)-(r+1))!}\\
& = \binom{n+1}{r+1}
\end{align*}
%
\end{enumerate}
%
\end{proof}
%
%
%
\typeout{************************************************}
\typeout{Section 3.5 Summary}
\typeout{************************************************}
%
\section[{Summary}]{Summary}\label{section-21}
\hypertarget{p-418}{}%
Here are the important results from this chapter:%
\hypertarget{p-419}{}%
\hyperref[MultiplicationPrinciple]{Multiplication Principle~\ref{MultiplicationPrinciple}}%
\par
\hypertarget{p-420}{}%
\hyperref[Factorial]{Factorial~\ref{Factorial}}%
\par
\hypertarget{p-421}{}%
\hyperref[PermutationWithoutReplacementDistinguishable]{Permutations without replacement~\ref{PermutationWithoutReplacementDistinguishable}}%
\par
\hypertarget{p-422}{}%
\hyperref[PermutationWithReplacementDistinguishable]{Permutations with replacement~\ref{PermutationWithReplacementDistinguishable}}%
\par
\hypertarget{p-423}{}%
\hyperref[MultinomialCoefficients]{Multinomial Coefficients~\ref{MultinomialCoefficients}}%
\par
\hypertarget{p-424}{}%
\hyperref[CombinationWithoutReplacementDistinguishable]{Combinations without replacement~\ref{CombinationWithoutReplacementDistinguishable}}%
\par
\hypertarget{p-425}{}%
\hyperref[CombinationWithReplacementDistinguishable]{Combinations with replacement~\ref{CombinationWithReplacementDistinguishable}}%
\par
\hypertarget{p-426}{}%
\hyperref[BinomialCoefficients]{Binomial Coefficients~\ref{BinomialCoefficients}}%
%
%
\typeout{************************************************}
\typeout{Section 3.6 Exercises}
\typeout{************************************************}
%
\section[{Exercises}]{Exercises}\label{section-22}
\hypertarget{p-427}{}%
Complete the online homework "Counting".%
\par
\hypertarget{p-428}{}%
A standard deck of playing cards consists of 52 cards broken up into four "suits" known as Hearts, Spades, Diamonds, and Clubs. Each suit is broken up additionally into unique cards with "face values" from {2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King, Ace} and generally in that order from low to high.%
\par
\hypertarget{p-429}{}%
1.  Pick two cards without replacement one after the other from this deck and determine the following number of possible outcomes: \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}The number of ways to get an Ace for both cards.%
\item{}The number of ways to get an Ace for only one of the two cards.%
\item{}The number of ways to get an Ace on the first draw and a Spade on the second draw.%
\end{itemize}
 2.  Pick five cards without replacement one after the other from a newly shuffled full deck and determine the following number of possible outcomes: \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}All cards have different faces%
\item{}"A pair". That is, two cards have the same face but the others are from three other faces.%
\item{}"Three of a kind".  That is, three cards have the same face but the others are from two other faces.%
\item{}"Two Pair". That is, two cards come from one face, two other cards come from a common face that is not the same as the first two cards, and the last card comes from some other face.%
\item{}"Full House". That is, three cards have the same face and the other two come from a common face that is not the same as the first three cards.%
\item{}"Four of a Kind". That is, four cards have the same face and the other card comes from some other face.%
\item{}"Flush". That is, the five cards for a sequence in order of adjacent faces in the original list and from the same suit.%
\item{}"Royal Flush". That is, a flush but only with the cards {Ace, King, Queen, Jack, 10}.%
\end{itemize}
%
\par
\hypertarget{p-430}{}%
Completely determine the number of possible passphrases for the National Treasure example started above. Present your answer in a report form.%
%
%
\typeout{************************************************}
\typeout{Chapter 4 Probability Theory}
\typeout{************************************************}
%
\chapter[{Probability Theory}]{Probability Theory}\label{ProbabilityGeneralities}
%
%
\typeout{************************************************}
\typeout{Section 4.1 Introduction}
\typeout{************************************************}
%
\section[{Introduction}]{Introduction}\label{section-23}
\hypertarget{p-431}{}%
Mathematics generally focuses on providing precise answers with absolute certainty. For example, solving an equation generates specific (and non-varying) solutions. Statistics on the other hand deals with providing precise answers to questions when there is uncertainty. It might seem impossible to provide such precise answers but the focus of this text is to show how that can be done so long as the questions are properly posed and the answers properly interpreted.%
\par
\hypertarget{p-432}{}%
Indeed, people often make claims about being the biggest, best, most often recommended, etc. One sometimes even believes these claims based upon subjective metrics. In this chapter, we will start by looking at relative frequency and notice several properties regarding relative frequencies as the number of trials increases. We will use these examples to motivate a definition for probability and investigate the resulting consequences of that definition.%
%
%
\typeout{************************************************}
\typeout{Section 4.2 Relative Frequency}
\typeout{************************************************}
%
\section[{Relative Frequency}]{Relative Frequency}\label{RelativeFrequency}
\hypertarget{p-433}{}%
When attempting to precisely measure uncertainty one often resorts to examples or experiments that model the theoretical question of interest. Before we investigate statistical experiments, we need to create some notation that we will utilize throughout the rest of this text.%
\leavevmode%
\begin{itemize}[label=\textbullet]
\item{}S = Universal Set or Sample Space Experiment or Outcome Space. This is the collection of all possiblilities.%
\item{}Random Experiment. A random experiment is a repeatable activity that has more than one possible outcome all of which can be specified in advance but can not be known in advance with certainty.%
\item{}Trial. Performing a Random Experiment one time and measuring the result.%
\item{}A = Event. A collection of outcomes.  Generally denoted by an upper case letter such as A, B, C, etc.%
\item{}Success/Failure. When recording the result of a trial, a success for event A occurs when the outcome lies in A. If not, then the trial was a failure. There is no qualitative meaning to this term.%
\item{}Mutually Exclusive Events. Two events that share no common outcomes. Also known as disjoint events.%
\item{}|A| = Frequency. In a sequence of n events, the frequency is the number of trials which resulted in a success for event A.%
\item{}|A| / n = Relative Frequency. A proportion of successes to total number of trials.%
\item{}Histogram. A bar chart representation of data where area corresponds to the value being described.%
\end{itemize}
\hypertarget{p-434}{}%
To investigate these terms and to motivate our discussion of probability, consider flipping coins using the interactive cell below. Notice in this case, the sample space S = \{ Heads, Tails \} and the random experiment consists of flipping a fair coin one time. Each trial results in either a Head or a Tail. Since we are measuring both Heads and Tails then we will not worry about which is a success or failure. Further, on each flip the outcomes of Heads or Tails are mutually exclusive events. We count the frequencies and compute the relative frequencies for a varying number of trials selected by you as you move the slider bar. Results are displayed using a histogram.%
\par
\hypertarget{p-435}{}%
\leavevmode%
\begin{sageinput}
coin = ["Heads", "Tails"]
@interact
def _(num_rolls = slider([5..5000],label="Number of Flips")):
	rolls = [choice(coin) for roll in range(num_rolls)]
	show(rolls)   
	freq = [0,0]
	for outcome in rolls:
		if (outcome=='Tails'):
			freq[0] = freq[0]+1
		else:
			freq[1] = freq[1]+1
	print("\nThe frequency of tails = "+ str(freq[0]))+" and heads = "+ str(freq[1])+"."
	rel = [freq[0]/num_rolls,freq[1]/num_rolls]
	print("\nThe relative frequencies for Tails and Heads:"+str(rel))
	show(bar_chart(freq,axes=False,ymin=0))     #  A histogram of the results
\end{sageinput}
%
\par
\hypertarget{p-436}{}%
Question 1: What do you notice as the number of flips increases?%
\par
\hypertarget{p-437}{}%
Question 2: Why do you rarely (if ever) get exactly the same number of Heads and Tails? Would you not "expect" that to happen?%
\par
\hypertarget{p-438}{}%
You should have noticed that as the number of flips increases, the relative frequency of Heads (and Tails) stabilized around 0.5. This makes sense intuitively since there are two options for each individual flip and 1/2 of those options are Heads while the other 1/2 is Tails.%
\par
\hypertarget{p-439}{}%
Let's try again by doing a random experiment consisting of rolling a single die one time. Note that the sample space in this case will be the outcomes S = \{ 1, 2, 3, 4, 5, 6 \}.%
\par
\hypertarget{p-440}{}%
\leavevmode%
\begin{sageinput}
@interact
def _(num_rolls = slider([20..5000],label='Number of rolls'),Number_of_Sides = [4,6,8,12,20]):
	die = list((1..Number_of_Sides))
	rolls = [choice(die) for roll in range(num_rolls)]
	show(rolls)   

	freq = [rolls.count(outcome) for outcome in set(die)]  # count the numbers for each outcome
	print 'The frequencies of each outcome is '+str(freq)

	print 'The relative frequencies of each outcome:'
	rel_freq = [freq[outcome-1]/num_rolls for outcome in set(die)]  # make frequencies relative
	print rel_freq
	fs = []
	for f in rel_freq:
		fs.append(f.n(digits=4))
	print fs
	show(bar_chart(freq,axes=False,ymin=0))
\end{sageinput}
%
\par
\hypertarget{p-441}{}%
Notice for a single die there are a larger number of options (for example 6 on a regular die) but once again the relative frequencies of each  outcome was close to 1/n (i.e. 1/6 for the regular die) as the number of rolls increased.%
\par
\hypertarget{p-442}{}%
In general, this suggests a rule: if there are n outcomes and each one has the same chance of occurring on a given trial then on average on a large number of trials the relative frequency of that outcome is 1/n. In general, if a number of outcomes are "equally likely" then this is a good model for measuring the proportion of outcomes that would be expected to have any given outcome. However, it is not always true that outcomes are equally likely. Consider rolling two die and measuring their sum:%
\par
\hypertarget{TwoDiceSage}{}%
\leavevmode%
\begin{sageinput}
@interact
def _(num_rolls = slider([20..5000],label='Number of rolls'),num_sides = slider(4,20,1,6,label='Number of sides')):
    die = list((1..num_sides))
    dice = list((2..num_sides*2))
    rolls = [(choice(die),choice(die)) for roll in range(num_rolls)]
    sums = [sum(rolls[roll]) for roll in range(num_rolls)]
    show(rolls)   

    freq = [sums.count(outcome) for outcome in set(dice)]  # count the numbers for each outcome
    print 'The frequencies of each outcome is '+str(freq)
    
    print 'The relative frequencies of each outcome:'
    rel_freq = [freq[outcome-2]/num_rolls for outcome in set(dice)]  # make frequencies relative
    print rel_freq        
    show(bar_chart(freq,axes=False,ymin=0))     #  A histogram of the results
    print "Relative Frequence of ",dice[0]," is about ",rel_freq[0].n(digits=4)
    print "Relative Frequence of ",dice[num_sides-1]," is about ",rel_freq[num_sides-1].n(digits=4)
\end{sageinput}
%
\par
\hypertarget{p-444}{}%
Question 1: What do you notice as the number of rolls increases?%
\par
\hypertarget{p-445}{}%
Question 2: What do you expect for the relative frequencies and why are they not all exactly the same?%
\par
\hypertarget{p-446}{}%
Notice, not only are the answers not the same but they are not even close. To understand why this is different from the examples before, consider the possible outcomes from each pair of die. Since we are measuring the sum of the dice then (for a pair of standard 6-sided dice) the possible sums are from 2 to 12. However, there is only one way to get a 2--namely from a (1,1) pair--while there are 6 ways to get a 7--namely from the pairs (1,6), (2,5), (3,4), (4,3), (5,2), and (6,1). So it might make some sense that the likelihood of getting a 7 is 6 times larger than that of getting a 2. Check to see if that is the case with your experiment above.%
\par
\hypertarget{p-447}{}%
Play with the following several times to investigate what you might expect to get when you repeatedly receive a "hand" of 5 standard playing cards. Can you imagine how you might possible enumerate the entire list of possible outcomes by hand? However, using this interactive cell, you can shuffle and deal 5-card hands over and over easily and then count the number of special poker outcomes.%
\par
\hypertarget{p-448}{}%
\leavevmode%
\begin{sageinput}
var('A C D H J K Q S') 

suits = [S, D, C, H] 
values = [2, 3, 4, 5, 6, 7, 8, 9, 10, J, Q, K, A] 

full_deck = [(value, suit) for suit in suits for value in values]
@interact
def _(num_hands=slider[50..5000]):                  # Set up the number of hands to create
    hands= []                    # Start with a blank list. 
    for i in range(num_hands):   # This loops the following operation num_hands times. 
        deck = copy(full_deck)   # start over
        shuffle(deck)
        hands.append([deck.pop() for card in range(5)])
    freq_values = []
    one_pair = 0
    two_pair = 0
    three_kind = 0
    full_house = 0
    four_kind = 0
    for i in range(num_hands):
        hand = hands[i]
        hand_values = [hand[k][0] for k in range(5)]
        freq_values = [hand_values.count(value) for value in set(values)]
        freq_values.sort(reverse=True)
        if freq_values[0]==4:
            four_kind=four_kind+1
        if freq_values[0]==3:
            if freq_values[1]==2:
                full_house=full_house+1
            if freq_values[1]==1:
                three_kind=three_kind+1
        if freq_values[0]==2:
            if freq_values[1]==2:
                two_pair=two_pair+1
            if freq_values[1]==1:
                one_pair=one_pair+1
    print "       One Pair frequency = ",one_pair," with relative frequency ",one_pair/num_hands
    print "       Two Pair frequency = ",two_pair," with relative frequency ",two_pair/num_hands
    print "Three of a Kind frequency = ",three_kind," with relative frequency ",three_kind/num_hands
    print "     Full House frequency = ",full_house," with relative frequency ",full_house/num_hands
    print " Four of a Kind frequency = ",four_kind," with relative frequency ",four_kind/num_hands
\end{sageinput}
%
\par
\hypertarget{p-449}{}%
Sometimes you will find it useful to keep a running total of the relative frequencies. Such a cumulative approach is often called a distribution function.%
\par
\hypertarget{p-450}{}%
\begin{definition}{Cumulative relative frequency.}{definition-21}%
\hypertarget{p-451}{}%
For a collection of ordered events \(x_1 \lt x_2 \lt ... \lt x_s\) with corresponding frequencies \(f_1, f_2, ..., f_s\), the cumulative relative frequency is the function%
\begin{equation*}
F(x) = \sum_{x_k \le x} f_{x_k}
\end{equation*}
%
\end{definition}
%
\par
\hypertarget{p-452}{}%
Let's consider the cumulative relative frequency with the sum of dice example seen at the beginning of this chapter. \hypertarget{p-453}{}%
\leavevmode%
\begin{sageinput}
@interact
def _(num_rolls = slider([20..5000],label='Number of rolls'),num_sides = slider(4,20,1,6,label='Number of sides')):
    die = list((1..num_sides))
    dice = list((2..num_sides*2))
    rolls = [(choice(die),choice(die)) for roll in range(num_rolls)]
    sums = [sum(rolls[roll]) for roll in range(num_rolls)]
    show(rolls)   

    freq = [sums.count(outcome) for outcome in set(dice)]  # count the numbers for each outcome
    n = len(freq)
    CF = freq
    for k in range(1,n):
        CF[k] = freq[k] + CF[k-1]
    
    print 'The cumulative relative frequencies of each outcome:'
    Crel_freq = [CF[outcome-2]/num_rolls for outcome in set(dice)]  # make frequencies relative
    print Crel_freq        
    show(bar_chart(CF,axes=False,ymin=0))     #  A histogram of the results
    print "Cumulative Relative Frequence of ",dice[0]," is about ",Crel_freq[0].n(digits=4)
    print "Cumulative Relative Frequence of ",dice[num_sides-1]," is about ",Crel_freq[num_sides-1].n(digits=4)
\end{sageinput}
%
%
%
%
\typeout{************************************************}
\typeout{Section 4.3 Definition of Probability}
\typeout{************************************************}
%
\section[{Definition of Probability}]{Definition of Probability}\label{section-25}
\hypertarget{p-454}{}%
\hypertarget{p-455}{}%
Relative frequency gives a way to measure the proportion of "successful" outcomes when doing an experimental approach. From the interactive applications above, it appears that the relative frequency does jump around as the experiment is repeated but that the amount of variation decreases as the number of experiments increases. This is known to be true in general and is known as the "Law of Large Numbers".%
\par
\hypertarget{p-456}{}%
We would like to formalize what these relative frequencies are approaching and will call this theoretical limit the "probability" of the outcome. In doing so, we will do our best to model our definition so that it follow the behavior of relative frequency.%
%
\par
\hypertarget{p-457}{}%
To generate a general definition for probability, we need to know what is is that we measuring. In general, we will be finding the probability of sets of possible outcomes...that is, a subset of the Sample Space S. Toward that end, it is important to briefly look at some properties of sets.%
\par
\hypertarget{p-458}{}%
\begin{definition}{Pairwise Disjoint Sets.}{DefnMutuallyExclusive}%
\hypertarget{p-459}{}%
\(\{ A_1, A_2, ... , A_n \}\) are pairwise disjoint provided \(A_k \cap A_j = \emptyset\) so long as \(k \ne j\). Disjoint sets as also often called mutually exclusive.%
\end{definition}
%
\par
\hypertarget{p-460}{}%
Play around with the interactive cell below by adding and removing items in each of the three sets. Find elements so that the intersection of all three sets is empty but at least one of the paired sets are not disjoint.  See if you can make all of the paired sets not disjoint but the intersection of all three disjoint. This is why we need to consider "pairwise" disjoint sets.%
\par
\hypertarget{p-461}{}%
\leavevmode%
\begin{sageinput}
def f(s, braces=True): 
    t = ', '.join(sorted(list(s)))
    if braces: return '{' + t + '}'
    return t
def g(s): return set(str(s).replace(',',' ').split())

@interact
def _(X='1,2,3', Y='2,a,3,4,apple', Z='a,b,10,apple'):
    S = [g(X), g(Y), g(Z)]
    X,Y,Z = S
    XY = X & Y
    XZ = X & Z
    YZ = Y & Z
    XYZ = XY & Z

    Txy = " - NOT disjoint "
    if Set(XY).is_empty():
        Txy = ' - disjoint '
    pretty_print(html("$X \cap Y$ = %s"%f(XY)+"%s"%Txy))
    Txz = " - NOT disjoint "
    if Set(XZ).is_empty():
        Txz = ' - disjoint '
    pretty_print(html("$X \cap Z$ = %s"%f(XZ)+"%s"%Txz))
    Tyz = " - NOT disjoint "
    if Set(YZ).is_empty():
        Tyz = ' - disjoint ' 
    pretty_print(html("$Y \cap Z$ = %s"%f(YZ)+"%s"%Tyz))
    Txyz = " - NOT disjoint "
    if Set(XYZ).is_empty():
        Txyz = ' - disjoint ' 
    pretty_print(html("$X \cap Y \cap Z$ = %s"%f(XYZ)+"%s"%Txyz))
    centers = [(cos(n*2*pi/3), sin(n*2*pi/3)) for n in [0,1,2]]
    scale = 1.7
    clr = ['yellow', 'blue', 'green']
    G = Graphics()
    for i in range(len(S)):
        G += circle(centers[i], scale, rgbcolor=clr[i], 
             fill=True, alpha=0.3)
    for i in range(len(S)):
        G += circle(centers[i], scale, rgbcolor='black')

    # Plot what is in one but neither other
    for i in range(len(S)):
        Z = set(S[i])
        for j in range(1,len(S)):
            Z = Z.difference(S[(i+j)%3])
        G += text(f(Z,braces=False), (1.5*centers[i][0],1.7*centers[i][1]), rgbcolor='black')


    # Plot pairs of intersections
    for i in range(len(S)):
        Z = (set(S[i]) & S[(i+1)%3]) - set(XYZ)
        C = (1.3*cos(i*2*pi/3 + pi/3), 1.3*sin(i*2*pi/3 + pi/3))
        G += text(f(Z,braces=False), C, rgbcolor='black')

    # Plot intersection of all three
    G += text(f(XYZ,braces=False), (0,0), rgbcolor='black')

    # Show it
    G.show(aspect_ratio=1, axes=False)
\end{sageinput}
%
\par
\hypertarget{p-462}{}%
Consider how we might create a definition for the expectation of a given outcome. To do so, first consider a desired collection of outcomes A. If each outcome in A is chosen randomly then we might consider using a formula similar to relative frequency and set a measure of expectation to be |A|/|S|. For example, on a standard 6-sided die, the expectation of the outcome A={2} from the collection S = {1,2,3,4,5,6} could be |A|/|S| = 1/6.%
\par
\hypertarget{p-463}{}%
From our example where we take the sum of two die, the outcome A = \{ 4,5 \} from the collection S = {2,3,4,...,12} would be%
\begin{gather*}
|A| = | \{ (1,3),(2,2),(3,1),(1,4),(2,3),(3,2),(4,1) \}| = 7\\
|S| = | \{ (1,1),...,(1,6),(2,1),...,(2,6),...,(6,1),...,(6,6) \}| = 36
\end{gather*}
and so the expected relative frequency would be |A|/|S| = 7/36. Compare this theoretical value with the sum of the two outcomes from your experiment above.%
\par
\hypertarget{p-464}{}%
We are ready to now formally give a name to the theoretical measure of expectation for outcomes from an experiment. Taking our cue from our examples, let's make our definition agree with the following relative frequency properties: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-131}{}Relative frequency cannot be negative, since cardinality cannot be negative%
\item\hypertarget{li-132}{}Relative frequencies for disjoint events should sum to one%
\item\hypertarget{li-133}{}Relative frequencies for collections of disjoint outcomes should equal the sum of the individual relative frequencies%
\end{enumerate}
%
\par
\hypertarget{p-465}{}%
which leads us to the following formal definition...%
\par
\hypertarget{p-466}{}%
\begin{definition}{Probability.}{DefnProb}%
\hypertarget{p-467}{}%
The probability P(A) of a given outcome A is a set function that satisfies:%
\par
\hypertarget{p-468}{}%
\leavevmode%
\begin{enumerate}
\item\hypertarget{li-134}{}(Nonnegativity) P(A) \(\ge 0\)%
\item\hypertarget{li-135}{}(Totality) P(S) = 1%
\item\hypertarget{li-136}{}(Subadditivity) If A \(\cap\) B = \(\emptyset\), then P(A \(\cup\) B) = P(A) + P(B). In general, if {\(A_k\)} are pairwise disjoint then \(P( \cup_k A_k) = \sum_k P(A_k)\).%
\end{enumerate}
%
\end{definition}
%
\par
\hypertarget{p-469}{}%
\begin{inlineexercise}{}{exercise-22}%
\hypertarget{p-470}{}%
Using the definition above, determine the following probabilities.%
\par\medskip
\hypertarget{p-471}{}%
Suppose you select a letter at random from the word MISSISSIPPI.%
\par
\hypertarget{p-472}{}%
The probability of selecting the letter S is  \fillin{4}%
\par
\hypertarget{p-473}{}%
The probability of selecting the letter M is  \fillin{4}%
\par
\hypertarget{p-474}{}%
The probability of selecting the letters P or I is  \fillin{4}%
\par
\hypertarget{p-475}{}%
The probability of not selecting the letter I is  \fillin{4}%
\par\smallskip%
\noindent\textbf{Hint}.\hypertarget{hint-1}{}\quad%
\hypertarget{p-476}{}%
Count the number of letters in the word.  When computing each probability, this is the number that goes on the bottom.%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-9}{}\quad%
\hypertarget{p-477}{}%
The probability of 'M's is 1/11.%
\par
\hypertarget{p-478}{}%
The probability of 'I's is 4/11.%
\par
\hypertarget{p-479}{}%
The probability of 'S's is 4/11.%
\par
\hypertarget{p-480}{}%
The probability of 'P's is 2/11.%
\par\medskip\noindent \hypertarget{p-481}{}%
Notice when you are given complete information regarding the entire data set then determining probabilities for events can be relatively easy to compute.%
\par
\end{inlineexercise}
%
\par
\hypertarget{p-482}{}%
Based upon this definition we can immediately establish a number of results.%
\par
\hypertarget{p-483}{}%
\begin{theorem}{Probability of Complements.}{}{ProbabilityComplements}%
\(P(A) + P(A^c) = 1\)\end{theorem}
\begin{proof}\hypertarget{proof-13}{}
\hypertarget{p-484}{}%
Let A be any event and note that%
\begin{equation*}
A \cap A^c = \emptyset.
\end{equation*}
But \(A \cup A^c = S\). So, by subadditivity%
\begin{equation*}
1 = P(S) = P(A \cup A^c) = P(A) + P(A^c)
\end{equation*}
as desired.%
\end{proof}
%
\par
\hypertarget{p-485}{}%
\begin{theorem}{}{}{ProbabilityEmptySet}%
\hypertarget{p-486}{}%
\(P(\emptyset) = 0\)%
\end{theorem}
\begin{proof}\hypertarget{proof-14}{}
\hypertarget{p-487}{}%
Note that \(\emptyset^c = S\). So, by the theorem above,%
\begin{equation*}
1 = P(S) + P(\emptyset) \Rightarrow 1 = 1 + P(\emptyset).
\end{equation*}
Cancelling the 1 on both sides gives \(P(\emptyset) = 0\).%
\end{proof}
%
\par
\hypertarget{p-488}{}%
\begin{theorem}{}{}{ProbabilityContainment}%
\(A \subset B, P(A) \le P(B)\)\end{theorem}
\begin{proof}\hypertarget{proof-15}{}
\hypertarget{p-489}{}%
Assume sets A and B satisfy \(A \subset B\). Then, notice that%
\begin{equation*}
A \cap (B-A) = \emptyset
\end{equation*}
and%
\begin{equation*}
B = A \cup (B-A).
\end{equation*}
Therefore, by subadditivity and nonnegativity%
\begin{gather*}
0 \le P(B-A)\\
P(A) \le P(A) + P(B-A) \\
P(A) \le P(B)\qedhere
\end{gather*}
%
\end{proof}
%
\par
\hypertarget{p-490}{}%
\begin{theorem}{}{}{ProbabilityLessThanOne}%
\(P(A) \le 1\)\end{theorem}
\begin{proof}\hypertarget{proof-16}{}
\hypertarget{p-491}{}%
Notice \(A \subset S\). By the theorem above \(P(A) \le P(S) = 1\)%
\end{proof}
%
\par
\hypertarget{p-492}{}%
\begin{theorem}{}{}{ProbabilityTwoUnions}%
\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)\end{theorem}
\begin{proof}\hypertarget{proof-17}{}
\hypertarget{p-493}{}%
Notice that we can write \(A \cup B\) as the disjoint union%
\begin{equation*}
A \cup B = (A-B) \cup (A \cap B) \cup (B-A).
\end{equation*}
We can also write disjointly%
\begin{gather*}
A = (A-B) \cup (A \cap B)\\
B = (A \cap B) \cup (B-A)
\end{gather*}
Hence,%
\begin{align*}
P(A) & + P(B) - P(A \cap B) \\
& = [P(A-B) + P(A \cap B)] \\
& + [P(A \cap B) + P(B-A)] - P(A \cap B)\\
& = P(A-B) + P(A \cap B) + P(B-A)\\
& = P(A \cup B)\qedhere
\end{align*}
%
\end{proof}
%
\par
\hypertarget{p-494}{}%
This result can be extended to more that two sets using a property known as inclusion-exclusion. The following two theorems illustrate this property and are presented without proof.%
\par
\hypertarget{p-495}{}%
\begin{corollary}{}{}{ProbabilityThreeUnions}%
\hypertarget{p-496}{}%
For any sets A, B and C,%
\begin{align*}
P(A \cup B \cup C) & = P(A) + P(B) + P(C)\\
& - P(A \cap B) - P(A \cap C) - P(B \cap C) \\
& + P(A \cap B \cap C)
\end{align*}
%
\end{corollary}
%
\par
\hypertarget{p-497}{}%
\begin{corollary}{}{}{ProbabilityFourUnions}%
\hypertarget{p-498}{}%
For any sets A, B, C and D,%
\begin{align*}
P(A \cup B \cup C \cup D) & = P(A) + P(B) + P(C) + P(D)\\
& - P(A \cap B) - P(A \cap C) - P(A \cap D) \\
& - P(B \cap C) - P(B \cap D) - P(C \cap D)\\
& + P(A \cap B \cap C) + P(A \cap B \cap D) \\
& + P(A \cap C \cap D) + P(B \cap C \cap D)\\
& - P(A \cap B \cap C \cap D)
\end{align*}
%
\end{corollary}
%
\par
\hypertarget{p-499}{}%
Many times, you will be dealing with making selections from a sample space where each item in the space has an equal chance of being selected. This may happen (for example) when items in the sample space are of equal size or when selecting a card from a completely shuffled deck or when coins are flipped or when a normal fair die is rolled.%
\par
\hypertarget{p-500}{}%
It is important to notice that not all outcomes are equally likely--even in times when there are only two of them. Indeed, it is generally not an equally likely situation when picking the winner of a football game which pits, say, the New Orleans Saints professional football team with the New Orleans Home School Saints. Even though there are only two options the probability of the professional team winning in most years ought to be much greater than the chances that the high school will prevail.%
\par
\hypertarget{p-501}{}%
When items are equally likely (sometimes also called "randomly selected") then each individual event has the same chance of being selected as any other. In this instance, determining the probability of a collection of outcomes is relatively simple.%
\par
\hypertarget{p-502}{}%
\begin{theorem}{Probability of Equally Likely Events.}{}{theorem-18}%
\hypertarget{p-503}{}%
If outcomes in S are equally likely, then for \(A \subset S,\)%
\begin{equation*}
P(A) = \frac{|A|}{|S|}.
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-18}{}
\hypertarget{p-504}{}%
Enumerate S = {\(x_1, x_2, ..., x_{|S|}\)} and note \(P( \{ x_k \} ) = c\) for some constant c since each item is equally likely. However, using each outcome as a disjoint event and the definition of probability,%
\begin{align*}
1 = P(S) & = P( \{ x_1 \} \cup \{x_2 \} \cup ... \cup \{x_{|S|} \} )\\
& = P(\{ x_1 \}) + P(\{ x_2 \} ) + ... + P(\{ x_{|S|} \} )\\
& = c + c + ... + c = {|S|} \times c
\end{align*}
and so \(c = \frac{1}{{|S|}}\). Therefore, \(P( \{ x_k \} ) = \frac{1}{|S|}\) .%
\par
\hypertarget{p-505}{}%
Hence, with A = {\(a_1, a_2, ..., a_{|A|}\)}, breaking up the disjoint probabilities as above gives%
\begin{align*}
P(A) & = P( \{ a_1 \} \cup \{ a_2 \} \cup ... \cup \{ a_{|A|} \} )\\
& = P(\{ a_1 \}) + P(\{ a_2 \} ) + ... + P(\{ a_{|A|} \} )\\
& = \frac{1}{{|S|}} + \frac{1}{{|S|}} + ... + \frac{1}{{|S|}}\\
& = \frac{|A|}{{|S|}}
\end{align*}
as desired.%
\end{proof}
%
\par
\hypertarget{p-506}{}%
\leavevmode%
\begin{sageinput}
var('A C D H J K Q S') 

def L(str):
    n = len(str)
    m = int(n/5)
    top = m+1
    if m == n/5:
        top = m
    for k in range(top):
        print str[5*k:5*k+5]
        
suits = [S, D, C, H] 
values = [2, 3, 4, 5, 6, 7, 8, 9, 10, J, Q, K, A] 

deck = [(value, suit) for suit in suits for value in values]
full_deck = copy(deck)  # to save a copy of the original deck for later use.

L(deck)
shuffle(deck)
L(deck)
deck1 = copy(full_deck)
shuffle(deck1)

@interact
def _(auto_update=False):
    global deck1    
    shuffle(deck1)            
    if (Set(deck1).cardinality()<5):
        print 'Deck is too small...getting a new deck'
        deck1 = copy(full_deck)
    else:
        hand = [deck1.pop() for card in range(5)] 
        print "The cards dealt:"
        L(hand)
        print
        print " The remaining cards in the deck:"
        L(deck1)
        print
        print(html("\n The number of remaining cards in the deck = %s"%str(Set(deck1).cardinality())))
\end{sageinput}
%
\par
\hypertarget{p-507}{}%
\begin{inlineexercise}{WebWork.}{exercise-23}%
\hypertarget{p-508}{}%
Let's see if you understand the relationship between frequency and relative frequency. In this exercise, presume "Probabiity" to be the expected fraction of outcomes you might logically expect.%
\par\medskip
\hypertarget{p-509}{}%
A fun size bag of M\(\\amp \)Ms has about 18 candies.  You open one of the bags and discover:%
\par
\hypertarget{p-510}{}%
3 Blues, 4 Yellows, 5 Browns, 2 Reds and 4 Greens.%
\par
\hypertarget{p-511}{}%
The probability of choosing a brown is  \fillin{4}.%
\par
\hypertarget{p-512}{}%
The odds in favor of choosing a yellow is  \fillin{4}%
\par
\hypertarget{p-513}{}%
The probability of choosing either a blue or a red is  \fillin{4}%
\par
\hypertarget{p-514}{}%
The odds against a green being chosen is  \fillin{4}%
\par\smallskip%
\noindent\textbf{Hint}.\hypertarget{hint-2}{}\quad%
\hypertarget{p-515}{}%
Odds in favor of an event = number of favorable outcomes / number of unfavorable outcomes.%
\par
\hypertarget{p-516}{}%
Odds against an event = number of unfavorable outcomes / number of favorable outcomes.%
\par\medskip\noindent \hypertarget{p-517}{}%
So, these are simple calculations.%
\par
\end{inlineexercise}
%
\par
\hypertarget{p-518}{}%
\begin{inlineexercise}{WebWork.}{exercise-24}%
\hypertarget{p-519}{}%
This one is a little harder and uses the binomial coefficients from Combinatorics.%
\par\medskip
\hypertarget{p-520}{}%
(a) \(\) Count the number of ways to arrange a sample of \(5\) elements from a population of \(10\) elements. NOTE: Order is not important.%
\par
\hypertarget{p-521}{}%
answer:  \fillin{10}%
\par
\hypertarget{p-522}{}%
(b) \(\) If random sampling is to be employed, the probability that any particular sample will be selected is  \fillin{10}%
\par\medskip\noindent \hypertarget{p-523}{}%
Notice how the probabilities look similar to relative frequencies. It's just the case that you are counting ALL of the individual simple possibilities that lead to a success.%
\par
\end{inlineexercise}
%
%
%
\typeout{************************************************}
\typeout{Section 4.4 Exercises}
\typeout{************************************************}
%
\section[{Exercises}]{Exercises}\label{section-26}
\hypertarget{p-524}{}%
\begin{inlineexercise}{Poker.}{exercise-25}%
\hypertarget{p-525}{}%
Determine the probabilities associated with the various 5-card hands. That is \leavevmode%
\begin{enumerate}
\item\hypertarget{li-137}{}P(one pair)%
\item\hypertarget{li-138}{}P(two pair)%
\item\hypertarget{li-139}{}P(three of a kind)%
\item\hypertarget{li-140}{}P(full house)%
\item\hypertarget{li-141}{}P(four of a kind%
\item\hypertarget{li-142}{}P(straight)%
\item\hypertarget{li-143}{}P(flush)%
\item\hypertarget{li-144}{}P(royal flush)%
\end{enumerate}
%
\end{inlineexercise}
%
\par
\hypertarget{p-526}{}%
\begin{inlineexercise}{Dice.}{exercise-26}%
\hypertarget{p-527}{}%
Determine the 36 possible outcomes related to the rolling a pair of fair dice. Justify why each of these outcomes is equally likely. Determine the probabilities associated with each possible sum.%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-10}{}\quad%
\hypertarget{p-528}{}%
Remember, when using equally likely outcomes |A|/|S| assumes that the items counted for A are also in the sample space S.  In this case, for example, to determine the Probability of getting a sum of (say) 4 includes the rolls (1,3), (2,2), and (3,1). These three "successes" from the 36 possible ordered pairs gives P(4) = 3/36.  Similarly, P(5) = |dice rolls with a sum of 5|/36 = |(1,4), (2,3), (3,2), (4,1)| / 36 = 4/36.  Continue in this manner to determine the other possibilities and then compare to the experimental sage cell seen earlier for \hyperlink{TwoDiceSage}{the sum of two dice~}.%
\end{inlineexercise}
%
\par
\hypertarget{p-529}{}%
\begin{inlineexercise}{Skew Dice.}{exercise-27}%
\hypertarget{p-530}{}%
Suppose you have one die which only has three possible sides labeled 1, 2, or 3. Suppose a second die has twelve equally likely sides with labels 1,2,3,4,4,5,5,6,6,7,8,9.  Justify that the probabilities associate with each possible sum is the same as the probabilities when using two normal 6-sided dice.%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-11}{}\quad%
\hypertarget{p-531}{}%
Consider the outcome space%
\begin{equation*}
S = {(1,1), (1,2), (1,3), (1,4), (1,4), (1,5), (1, 5), \\
(1,6), (1,6), (1,7), (1,8), (1,9), (2,1) ... (3,9)}
\end{equation*}
Then P(5) = |(1,4), (1,4), (2,3), (3,2) |/36 = 4/36. Compare this to the exercise with regular dice performed above.  Similarly, compute the remaining probabilities.%
\end{inlineexercise}
%
\par
\hypertarget{p-532}{}%
\begin{inlineexercise}{Craps.}{exercise-28}%
\hypertarget{p-533}{}%
Analyze the dice game known as "craps": Roll a pair of dice and consider the sum. If that sum is 7 or 11, the one who rolls wins and can roll again. If the sum is 2, 3, or 12 -- known as craps -- the one who rolls loses but keeps the dice. For any other outcome (called the "point"), the one who rolls continues hoping to roll the point value again before rolling a 7. If successful, then the roller wins and starts the game anew. If a 7 appears first, the roller loses and the next person gets to be the roller.%
\par
\hypertarget{p-534}{}%
So, a win can be obtained in two ways: 7 or 11 on first roll or getting the point before the 7 thereafter. Therefore, determine the probability of a win and the probability of a loss.%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-12}{}\quad%
\hypertarget{p-535}{}%
\href{http://mathworld.wolfram.com/Craps.html}{"craps"}.%
\end{inlineexercise}
%
%
%
\typeout{************************************************}
\typeout{Section 4.5 Conditional Probability}
\typeout{************************************************}
%
\section[{Conditional Probability}]{Conditional Probability}\label{section-27}
\hypertarget{p-536}{}%
When finding the probability of an event, sometimes you may need to consider past history and how it might affect things. Indeed, you might think that when the local station forecasts rain then the probability of it actually raining should be greater than if they forecast fair skies. At least that is the hope. :)  In this section, you will develop a way to deal with the probability of some event that might change dependent upon the occurence or not of some other event.%
\hypertarget{p-537}{}%
Indeed, consider what happens when you keep on dealing a hand of five cards from a shuffled deck but without replacement.  Notice how the probability of the same thing (such as P(getting a Heart on the next card)) oscillates based upon what cards came out of the deck on previous hands.%
\par
\hypertarget{p-538}{}%
\leavevmode%
\begin{sageinput}
print "Conditional Events - successively deal 5 cards w/o replacement" 

var('Ace Clubs Diamonds Hearts Jack King Queen Spades') 

suits = [Spades, Diamonds, Clubs, Hearts] 
values = [2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King, Ace] 

deck = [(value, suit) for suit in suits for value in values]
full_deck = copy(deck)  # to save a copy of the original deck for later use.

deck1 = copy(full_deck)
history1=[]
@interact
def _(choice=['Hearts','Spades','Diamonds','Clubs','New Deck'],again=['Repeat Same Suit']):
    global deck1, history1
    shuffle(deck1)
    if choice=='Hearts':
        suit = Hearts
    elif choice=='Spades':
        suit = Spades
    elif choice=='Diamonds':
        suit = Diamonds
    elif choice=='Clubs':
        suit = Clubs
    else:
        deck1 = copy(full_deck)
        shuffle(deck1)
        history1=[]
    if (Set(deck1).cardinality()<5):
        print "Deck is too small...get a new deck"
    elif choice<>'New Deck':
        hand = [deck1.pop() for card in range(5)] 
        print "Click on a desired suit above to deal out another 5 card hand.  The cards dealt:"
        print hand
        print "The remaining cards in the deck:"
        print deck1
        num = Set(deck1).cardinality()
        print "\nThe number of remaining cards in the deck = %s"%str(num)
        looking = []
        for card in deck1:
            if card[1]==suit:
                looking.append(card)
        prob = float(Set(looking).cardinality())/num
        history1.append(prob)
        
        print 'So, the remaining probability of getting a card from '+choice+' from the remaining cards is %s'%str(prob)
    list_plot(history1).show(xmin=0,xmax=9,ymin=0,ymax=1,figsize=(5,2))
\end{sageinput}
%
\par
\hypertarget{p-539}{}%
Now, consider the case when you put the cards back in, reshuffle, and then get 5 new cards...%
\par
\hypertarget{p-540}{}%
\leavevmode%
\begin{sageinput}
print "Independent Events - Successively deal 5 cards but WITH replacement"

var('Ace Clubs Diamonds Hearts Jack King Queen Spades') 

suits = [Spades, Diamonds, Clubs, Hearts] 
values = [2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King, Ace] 

deck = [(value, suit) for suit in suits for value in values]
full_deck = copy(deck)  # to save a copy of the original deck for later use.
deck1 = copy(full_deck)

h2=[]
@interact
def _(choice=['Heart','Spade','Diamond','Club'],again=['Repeat Same Suit']):

    if choice=='Hearts':
        suit = Hearts
    elif choice=='Spades':
        suit = Spades
    elif choice=='Diamonds':
        suit = Diamonds
    else:
        suit = Clubs

    deck1 = copy(full_deck)
    shuffle(deck1) 
    hand = [deck1.pop() for card in range(5)] 
    print "The cards dealt:"
    print hand
    print "Replacing this hand and reshuffling gives the remaining cards in the deck:"
    deck1 = copy(full_deck)
    shuffle(deck1)
    print(deck1)
    
    num = Set(deck1).cardinality()
    print "\nThe number of remaining cards in the deck = %s"%str(num)
    looking = []
    for card in deck1:
        if card[1]==suit:
            looking.append(card)
    prob = float(Set(looking).cardinality())/num
    h2.append(prob)
    
    print 'So, the remaining probability of getting a '+choice+' from the remaining cards is %s'%str(prob)
    list_plot(h2).show(xmin=0,xmax=15,ymin=0,ymax=1,figsize=(5,2))
    
    print 'Independent Events - Successively deal 5 cards but WITH replacement'
\end{sageinput}
%
\par
\hypertarget{p-541}{}%
\emph{Changing Sample Space - Balls:} Consider a box with three balls: one Red, one White, and one Blue.  Using an equally likely assumption, the probability of randomly pulling out a Red ball should be 1/3.  That is P(Red) = 1/3.  However, suppose that for a first trial you pull out the White ball and set it aside. Attempting to pull out another ball leaves you with only two options and so the probability of randomly pulling out a Red ball is 1/2. Notice that the probability changed for the second trial dependent on the outcome of the first trial.%
\par
\hypertarget{p-542}{}%
\emph{Changing Sample Space - Cards:} Consider a deck of 52 standard playing cards and a success occurs when a Heart is selected from the deck. When extracting one card randomly, the probability	of that card being a Heart is P(Heart) = 13/52. Now, assume that one card has already been extracted and setaside.  Now, prepare to extract another. If the first card drawn was a Heart, then there are only 12 Hearts left for the second draw. However, if the first card drawn was not a Heart, then there are 13 Hearts available for the second draw. To compute this probability correctly, one need to formulate the question so that subadditivity can be utilized.%
\par
\hypertarget{p-543}{}%
Let \(H_1\) be the outcome Heart on 1st draw and \(H_2\) be the outcome Heart on 2nd draw. Then,%
\begin{align*}
P(\text{Heart on 2nd draw}) & = P( [ H_1 \cap H2 ] \cup [ H_1^c \cap H_2 ] )\\
& = P( H_1 \cap H_2 ) + P( H_1^c \cap H_2 )\\
& = \frac{ | H_1 \cap H_2 |}{| P( \text{Number of ways to get two cards} | }\\
& + \frac{ | H_1^c \cap H_2 | }{ | \text{Number of ways to get two cards} | }\\
& = \frac{13 12}{52 51} + \frac{39 13}{52 51} = \frac{12}{4 51} + \frac{3 13}{ 4 51}
\end{align*}
%
\par
\hypertarget{p-544}{}%
\begin{definition}{Conditional Probability.}{definition-24}%
\hypertarget{p-545}{}%
For sets A and B,%
\begin{equation*}
P(B | A) = \frac{P(A \cap B)}{ P(A) },
\end{equation*}
provided P(A)\(\gt 0\).%
\end{definition}
%
\par
\hypertarget{p-546}{}%
You can read P(B|A) as "the probability of B given A".%
\par
\hypertarget{p-547}{}%
\begin{theorem}{}{}{theorem-19}%
\hypertarget{p-548}{}%
Conditional Probability satisfies all of the requirements of regular probability.%
\end{theorem}
\begin{proof}\hypertarget{proof-19}{}
\hypertarget{p-549}{}%
By definition, for any event probability must be nonnegative. Therefore%
\begin{equation*}
P(A \cap B) \ge 0.
\end{equation*}
So,%
\begin{equation*}
P(B | A) = \frac{\text{positive or zero}}{\text{positive}}\ge 0.
\end{equation*}
%
\par
\hypertarget{p-550}{}%
Further,%
\begin{equation*}
P (S | A) = P(A \cap S)/P(A) = P(A)/P(A) = 1.
\end{equation*}
For the third part, we will only consider the case when there are two disjoint sets B and C.  Then,%
\begin{align*}
P(B \cup C | A) & = \frac{P(A \cap (B \cup C)}{P(A)} \\
& = \frac{P( (A \cap B) \cup (A \cap C) )}{P(A)}\\
& = \frac{P(A \cap B)}{P(A)} + \frac{P(A \cap C)}{P(A)}\\
& = P(B | A) + P(C | A).\qedhere
\end{align*}
%
\end{proof}
%
\par
\hypertarget{p-551}{}%
\begin{theorem}{Multiplication Rule.}{}{theorem-20}%
\hypertarget{p-552}{}%
For any sets A and B,%
\begin{equation*}
P(A \cap B) = P(A) P(B | A) = P(B) P(A | B)
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-20}{}
\hypertarget{p-553}{}%
If P(A)=0 or P(B)=0, then the result is trivial. Otherwise, unravel the definition of conditional probably by taking the denominator to the other side. Also note that you can write \(A \cap B = B \cap A\).%
\end{proof}
%
\par
\hypertarget{p-554}{}%
\begin{inlineexercise}{WebWork.}{exercise-29}%
\hypertarget{p-555}{}%
Conditional Probability sometimes makes you have to think carefully about the ways to get the desired outcome.%
\par\medskip
\hypertarget{p-556}{}%
A bag contains \(6\) red marbles and \(7\) white marbles.  Two marbles are drawn in succession without replacement.  Find the probabilities of the following events:%
\par
\hypertarget{p-557}{}%
\emph{1.} The first marble drawn is red and the second is white.%
\par
\hypertarget{p-558}{}%
Answer:  \fillin{20}%
\par
\hypertarget{p-559}{}%
\emph{2.} Both marbles drawn are red.%
\par
\hypertarget{p-560}{}%
Answer:  \fillin{20}%
\par\medskip\noindent \hypertarget{p-561}{}%
See how you had to break the given question up into two disjoint pieces.%
\par
\end{inlineexercise}
%
%
%
\typeout{************************************************}
\typeout{Section 4.6 Bayes Theorem}
\typeout{************************************************}
%
\section[{Bayes Theorem}]{Bayes Theorem}\label{section-28}
\hypertarget{p-562}{}%
Conditional probabilities can be computed using the methods developed above if the appropriate information is available. Some times you will however have some information available, such as \(P(A | B)\) but need \(P(B | A)\). The ability to "play around with history" by switching what has been presumed to occur leads to an important result known as Baye's Theorem.%
\begin{theorem}{Bayes Theorem.}{}{theorem-21}%
\hypertarget{p-563}{}%
Let \(S = \{ S_1, S_2, ... , S_m \}\) where the \(S_k\) are pairwise disjoint and \(S_1 \cup S_2 \cup ... \cup S_m = S\) (i.e. a partition of the space S).  Then for any \(A \subset S\)%
\begin{equation*}
P(S_j | A) = \frac{P(S_j)P(A | S_j)}{\sum_{k=1}^m P(S_k)P(A | S_k)}.
\end{equation*}
The conditional probability \(P(S_j | A)\) is called the posterior probability of \(S_k\).%
\end{theorem}
\begin{proof}\hypertarget{proof-21}{}
\hypertarget{p-564}{}%
Notice, by the definition of conditional probability and the multiplication rule%
\begin{equation*}
P(S_j | A) = \frac{P(S_j \cap A)}{P(A)} = \frac{P(S_j)P( A | S_j)}{P(A)}.
\end{equation*}
But using the disjointness of the partition%
\begin{align*}
P(A) & = P( (A \cap S_1) \cup (A \cup S_2) \cup ... \cup (A \cup S_m) )\\
& = P(A \cap S_1) + P(A \cup S_2) + ... + P(A \cup S_m)\\
& = P(S_1 \cap A) + P(S_2 \cup A) + ... + P(S_m \cup A)\\
& = P(S_1) P(A | S_1) + P(S_2)P(A | S_2) + ... + P(S_m)P(A | S_m)\\
& = \sum_{k=1}^m P(S_k)P(A | S_k)
\end{align*}
Put these two expansions together to obtain the desired result.%
\end{proof}
\hypertarget{p-565}{}%
To illustrate this result, from the web site \url{http://stattrek.com/probability/bayes-theorem.aspx} consider the following problem:%
\par
\hypertarget{p-566}{}%
Marie is getting married tomorrow, at an outdoor ceremony in the desert. In recent years, it has rained only 5 days each year. Unfortunately, the weatherman has predicted rain for tomorrow. When it actually rains, the weatherman correctly forecasts rain 90% of the time. When it doesn't rain, he incorrectly forecasts rain 10% of the time. What is the probability that it will rain on the day of Marie's wedding?%
\par
\hypertarget{p-567}{}%
Notice, all days can be classified into one of two disjoint options: \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}Rainy, in which case we can deduce from the given info that P(Rain) = 5/365%
\item{}Not Rainy, and since this is the complement of above, P(Not Rain) = 360/365%
\end{itemize}
 In the notation of Bayes Theorem, let A represent a forecast of Rain and note you have%
\begin{equation*}
P(\text{Rain}) = P(S_1) = \frac{5}{365}
\end{equation*}
and%
\begin{equation*}
P(\text{Not Rain}) = P(S_2) = \frac{360}{365}.
\end{equation*}
Further, you are given the conditional probabilities%
\begin{equation*}
P(\text{ Forecast Rain | Rain}) = P( A | S_1) = 0.9
\end{equation*}
%
\begin{equation*}
P(\text{ Forecast Rain | Not Rain}) = P( A | S_2) = 0.1
\end{equation*}
Notice that the question provided requests that you find the probability of Rain given that the weatherman has forecasted rain. What is given on the other hand is the reverse of that conditional probability. Using Bayes Theorem allows you to turn this around...%
\begin{align*}
P(\text{Rain}) &  = P(S_1) P( A | S_1) + P(S_2) P(A | S_2)\\
& = \frac{5}{365} \cdot 0.9 + \frac{360}{365} \cdot 0.1
\end{align*}
Hence, putting these together gives%
\begin{align*}
P(\text{Rain | Forecast Rain}) & = \frac{\frac{5}{365} \cdot 0.9}{\frac{5}{365} \cdot 0.9 + \frac{360}{365} \cdot 0.1}\\
& = \frac{5 \cdot 0.9}{5 \cdot 0.9 + 360 \cdot 0.1}\\
& = \frac{45}{45+360} \approx 0.111
\end{align*}
So, normally there is only a 5 percent chance of rain on a given day but given that the weatherman has forecast rain, the chance of rain has risen to a little more than 11 percent.%
\par
\hypertarget{p-568}{}%
\begin{inlineexercise}{WebWork.}{exercise-30}%
\hypertarget{p-569}{}%
Let's try a Bayes Theorem example...%
\par\medskip
\hypertarget{p-570}{}%
A biomedical research company produces \(50 \lt percent />\) of its insulin at a plant in Kansas City, and the remainder is produced at a plant in Jefferson City.  Quality control has shown that \(1.25 \lt percent />\) of the insulin produced at the plant in Kansas City is defective, while \(0.7 \lt percent />\) of the insulin produced at the plant in Jefferson City is defective.  What is the probability that a randomly chosen unit of insulin came from the plant in Jefferson City given that it is defective?%
\par
\hypertarget{p-572}{}%
(\emph{Hint:} Draw a tree diagram first)%
\par\medskip\noindent \hypertarget{p-573}{}%
You have to be careful to extract the conditional probabilities from the problem.%
\par
\end{inlineexercise}
 \begin{inlineexercise}{WebWork.}{exercise-31}%
\hypertarget{p-574}{}%
Here is a more extensive Bayes Theorem example...%
\par\medskip
\hypertarget{p-575}{}%
Data from Office on Smoking and Health, Centers for Disease Control and Prevention, indicate that 36\% of adults who did not finish high school, 33\% of high school graduates, 26\% of adults who completed some college, and 15\% of college graduates smoke.  Suppose that one individual is selected at random and it is discovered that the individual smokes.  Use the probabilities in the following table to calculate the probability that the individual is a college graduate.%
\begin{sidebyside}{1}{0}{0}{0}%
\begin{sbspanel}{1}%
{\centering%
\begin{tabular}{BlBlBlB}\hrulemedium
Education&Employed&Unemployed\tabularnewline\hrulemedium
Not a high school graduate&0.0975&0.0080\tabularnewline\hrulemedium
High school graduate&0.3108&0.0128\tabularnewline\hrulemedium
Some college, no degree&0.1785&0.0062\tabularnewline\hrulemedium
Associate Degree&0.0849&0.0023\tabularnewline\hrulemedium
Bachelor Degree&0.1959&0.0041\tabularnewline\hrulemedium
Advanced Degree&0.0975&0.0015\tabularnewline\hrulemedium
\end{tabular}
\par}
\end{sbspanel}%
\end{sidebyside}%
\par
\hypertarget{p-576}{}%
Probability =   \fillin{15}%
\par
\hypertarget{p-577}{}%
Hints: This problem has all the information you need, but not in the typical ready-to-use form.  The table above can tell you the proportion of people with various levels of education in the population. Keep in mind that any degree (Associate, Bachelor, or Advanced) counts as graduating from college.%
\par\medskip\noindent \hypertarget{p-578}{}%
Notice that having the data expressed in tabular form sometimes makes it easier to deal with.%
\par
\end{inlineexercise}
%
\par
\hypertarget{p-579}{}%
The interactive cell below can be used to easily compute all of the conditional probabilities associated with Bayes's Theorem. Notice how the relative size of the pie-shaped partition changes when you presume that an event in the space has already occurred.%
\par
\hypertarget{BayesSage}{}%
\leavevmode%
\begin{sageinput}
#  This function is used to convert an input string into separate entries
def g(s): return str(s).replace(',',' ').replace('(',' ').replace(')',' ').split()

@interact
def _(Partition_Probabilities=input_box('0.35,0.25,0.40',label="$ P(S_1),P(S_2),... $"),
        Conditional_Probabilities=input_box('0.02,0.01,0.03',label='$ P(A|S_1),P(A|S_2),... $'),
        print_numbers=checkbox(True,label='Numerical Results on Graphs?'),
        auto_update=False):
            
    Partition_Probabilities = g(Partition_Probabilities)
    Conditional_Probabilities = g(Conditional_Probabilities)
    n = len(Partition_Probabilities)
    n0 = len(Conditional_Probabilities)
    
    # below needs to be n not equal to n0 but mathbook xml will not let me get the other
    if (n > n0):
        pretty_print("You must have the same number of partition probabilities and conditional probabilities.")
        
    else:                               # input data streams now are the same size!
        colors = rainbow(n)
        accum = float(0)                # to test whether partition probs sum to one
        ends = [0]                      # where the graphed partition sectors change in pie chart 
        mid = []                        # middle of each pie chart sector used for placement of text
        p_Sk_given_A = []               # P( S_k | A )
        pA = 0                          # P(A)
        PP=[]                           # array to hold the numerical Partition Probabilities 
        CP=[]                           # array to hold the numerical Conditional Probabilities     
        for k in range(n):
            PP.append(float(Partition_Probabilities[k]))
            CP.append(float(Conditional_Probabilities[k]))    
            p_Sk_given_A.append(PP[k]*CP[k] )
            pA += p_Sk_given_A[k]
            accum = accum + PP[k]
            ends.append(accum)
            mid.append((ends[k]+accum)/2)
#
#  Marching along from 0 to 1, saving angles for each partition sector boundary.
#  Later, we will multiple these by 2*pi to get actual sector boundary angles.
#
        if abs(accum-float(1))>0.0000001:     #  Due to roundoff issues, this should be close enough.                     
            pretty_print("Sum of probabilities should equal 1.")
        
        else:                           # probability data is sensible
 
#        
#  Draw the Venn diagram by drawing sectors from the angles determined above
#  First, create a circle of radius 1 to illustrate the the sample space S
#  Then draw each sector with varying colors and print out their names on the edge
#
            G = circle((0,0), 1, rgbcolor='black',fill=False, alpha=0.4,aspect_ratio=True,axes=False,thickness=5)
            for k in range(n):
                G += disk((0,0), 1, (ends[k]*2*pi, ends[k+1]*2*pi), color=colors[mod(k,10)],alpha = 0.2)
                G += text('$S_'+str(k+1)+'$',(1.1*cos(mid[k]*2*pi), 1.1*sin(mid[k]*2*pi)), rgbcolor='black')
                
            G += circle((0,0), 0.6, facecolor='yellow', fill = True, alpha = 0.1, thickness=5,edgecolor='black') 
    
#  Print the probabilities corresponding to each particular region as a list and on the graphs
            if print_numbers:               

                html("$P(A) = %s$"%(str(pA),))
                for k in range(n):
                    html("$P(S_{%s} | A)$"%(str(k+1))+"$ = %s$"%str(p_Sk_given_A[k]/pA))
                                        
                    G += text(str(p_Sk_given_A[k]),(0.4*cos(mid[k]*2*pi), 0.4*sin(mid[k]*2*pi)), rgbcolor='black')
                    G += text(str(PP[k] - p_Sk_given_A[k]),(0.8*cos(mid[k]*2*pi), 0.8*sin(mid[k]*2*pi)), rgbcolor='black')
        
#  This is essentially a repeat of some of the above code but focused only on creating the smaller inner circle dealing
#  with the set A so that the sectors now correspond in area to the Bayes Theorem probabilities


            accum = float(0)                        
            ends = [0]                     # where the graphed partition sectors change in pie chart 
            mid = []                       # middle of each pie chart sector used for placement of text
            for k in range(n): 
                accum += float(p_Sk_given_A[k]/pA) 
                ends.append(accum)
                mid.append((ends[k]+accum)/2)
            H = circle((0,0), 1, rgbcolor='black',fill=False, alpha=0,aspect_ratio=True,axes=False,thickness=0)
            H += circle((0,0), 0.6, facecolor='yellow',fill=True, alpha=0.1,aspect_ratio=True,axes=False,thickness=5,edgecolor='black')
            
            for k in range(n):
                H += disk((0,0), 0.6, (ends[k]*2*pi, ends[k+1]*2*pi), color=colors[mod(k,10)],alpha = 0.2)
                H += text('$S_'+str(k+1)+'|A$',(0.7*cos(mid[k]*2*pi), 0.7*sin(mid[k]*2*pi)), rgbcolor='black')
                    
        #  Now, print out the bayesian probabilities using the smaller set A only
    
            if print_numbers:
                for k in range(n):
                    H += text(str( N(p_Sk_given_A[k]/pA,digits=4) ),(0.4*cos(mid[k]*2*pi), 0.4*sin(mid[k]*2*pi)), rgbcolor='black')
                    
            G.show(title='Venn diagram of partition with A in middle')
            print
            H.show(title='Venn diagram presuming A has occured')
\end{sageinput}
%
\par
\hypertarget{p-581}{}%
\begin{inlineexercise}{Insured vs Accident.}{exercise-32}%
\hypertarget{p-582}{}%
Your automobile insurance company uses past history to determine how to set rates by measuring the number of accidents caused by clients in various age ranges. The following table summarizes the proportion of those insured and the corresponding probabilities by age range: \leavevmode%
\begin{table}
\centering
\begin{tabular}{rrr}
\multicolumn{1}{rB}{Age}&\multicolumn{1}{rB}{Proportion of Insured}&Probability of Accident\tabularnewline\hrulemedium
\multicolumn{1}{rB}{16-20}&\multicolumn{1}{rB}{0.05}&0.08\tabularnewline[0pt]
\multicolumn{1}{rB}{21-25}&\multicolumn{1}{rB}{0.06}&0.07\tabularnewline[0pt]
\multicolumn{1}{rB}{26-55}&\multicolumn{1}{rB}{0.49}&0.02\tabularnewline[0pt]
\multicolumn{1}{rB}{55-65}&\multicolumn{1}{rB}{0.25}&0.03\tabularnewline[0pt]
\multicolumn{1}{rB}{over 65}&\multicolumn{1}{rB}{0.15}&0.04
\end{tabular}
\caption{Age vs Accident Likelihood\label{table-9}}
\end{table}
 One of your family friends insured by this company has an accident. \leavevmode%
\begin{enumerate}
\item\hypertarget{li-147}{}Determine the conditional probability that the driver was in the 16-20 age range.%
\item\hypertarget{li-148}{}Compare this to the probability that the driver was in the 18-20 age range. Discuss the difference.%
\item\hypertarget{li-149}{}Determine how much more the company should charge for someone in the 16-20 age range compared to someone in the 26-55 age range.%
\end{enumerate}
%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-13}{}\quad%
\hypertarget{p-583}{}%
Plug the middle column into the first input box and the right column into the second input box of the  \hyperlink{BayesSage}{Bayes Sage Cell~}%
\end{inlineexercise}
%
\par
\hypertarget{p-584}{}%
\begin{inlineexercise}{Spinal bifida odds.}{exercise-33}%
\hypertarget{p-585}{}%
Congratulations...your family is having a baby! As part of the prenatal care, some testing is part of the normal procedure including one for spinal bifida (which is a condition in which part of the spinal cord may be exposed.) Indeed, measurement of maternal serum AFP values is a standard tool used in obstetrical care to identify pregnancies that may have an increased risk for this disorder. You want to make plans for the new child's care and want to know how serious to take the test results. However, some times the test indicates that the child has the disorder when in actuality it does not (a false positive) and likewise may indicate that the child does not have the disorder when in fact it does (a false negative.)%
\par
\hypertarget{p-586}{}%
The combined accuracy rate for the screen to detect the chromosomal abnormalities mentioned above is approximately 85% with a false positive rate of 5%. This means that (from \href{http://americanpregnancy.org/prenatal-testing/first-trimester-screen/}{americanpregnancy.org}) \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}Approximately 85 out of every 100 babies affected by the abnormalities addressed by the screen will be identified. (Positive Positive)%
\item{}Approximately 5% of all normal pregnancies will receive a positive result or an abnormal level. (False Positive)%
\end{itemize}
 \leavevmode%
\begin{enumerate}
\item\hypertarget{li-152}{}Given that your test came back negative, determine the likelihood that the child will actually have spinal bifida.%
\item\hypertarget{li-153}{}Given that your test came back negative, determine the likelihood that the child will not have spina bifida%
\item\hypertarget{li-154}{}Given that a positive test means you have a 1/100 to 1/300 chance of experiencing one of the abnormalities, determine the likelihood of spinal bifida in a randomly selected child.%
\end{enumerate}
%
\end{inlineexercise}
%
%
%
\typeout{************************************************}
\typeout{Section 4.7 Independence}
\typeout{************************************************}
%
\section[{Independence}]{Independence}\label{section-29}
\hypertarget{p-587}{}%
You have seen when repeatedly sampling without replacement leads to a change the the likelihood of some event in successive trials. Indeed, this is what conditional probabilities above illustrate. However, when sampling with replacement you may find a different situation arises. Indeed, you easily notice that when flipping a coin, P(Heads) = 1/2 regardless of the outcome of any previous flip.  In situations such as this where the probability of an event is not affected by the occurrence (or lack of occurrence) of some other event determining the probability of compound events can be greatly simplified.%
\hypertarget{p-588}{}%
\begin{definition}{Independent Events.}{DefnIndependentEvents}%
\hypertarget{p-589}{}%
Events A and B are independent provided%
\begin{equation*}
P(A \cap B) = P(A) P(B)
\end{equation*}
%
\end{definition}
%
\par
\hypertarget{p-590}{}%
\begin{corollary}{Independence and Conditional Probability.}{}{corollary-3}%
\hypertarget{p-591}{}%
Given \hyperref[DefnIndependentEvents]{independent events~\ref{DefnIndependentEvents}} A and B,%
\begin{equation*}
P(B | A) = P(B)
\end{equation*}
and%
\begin{equation*}
P(A | B) = P(A).
\end{equation*}
%
\end{corollary}
\begin{proof}\hypertarget{proof-22}{}
\hypertarget{p-592}{}%
By the multiplication rule and the definition of independence, for any events A and B%
\begin{equation*}
P(A) \cdot P(B) = P(A \cap B) = P(A) \cdot P(B | A) .
\end{equation*}
Therefore, if P(A) is non-zero, canceling yields the first result. Switching around notation provides the second.%
\end{proof}
%
\par
\hypertarget{p-593}{}%
\begin{inlineexercise}{WebWork.}{exercise-34}%
\hypertarget{p-594}{}%
Independence makes combined probabilities VERY easy to compute.%
\par\medskip
\hypertarget{p-595}{}%
For two events \(A\) and \(B\), \(P(A) = 0.3\) and \(P(B) = 0.1\).%
\par
\hypertarget{p-596}{}%
(a)\(\) If \(A\) and \(B\) are independent, then%
\par
\hypertarget{p-597}{}%
\(P(A|B)\) \(=\)  \fillin{10}%
\par
\hypertarget{p-598}{}%
\(P(A \cup B)\) \(=\)  \fillin{10}%
\par
\hypertarget{p-599}{}%
\(P(A \cap B)\) \(=\)  \fillin{10}%
\par
\hypertarget{p-600}{}%
(b)\(\) If \(A\) and \(B\) are dependent and \(P(A|B) = 0.1\), then%
\par
\hypertarget{p-601}{}%
\(P(A \cap B)\) \(=\)  \fillin{10}%
\par
\hypertarget{p-602}{}%
\(P(B|A)\) \(=\)  \fillin{10}%
\par\medskip\noindent \hypertarget{p-603}{}%
Basically you just multiply individual probabilities together.  Independence is often assumed since it makes computations easier. That said, you should remember to consider each time whether independence should or should not be assumed.%
\par
\end{inlineexercise}
%
\par
\hypertarget{p-604}{}%
\begin{corollary}{Independence and Mutual Exclusivity.}{}{corollary-4}%
\hypertarget{p-605}{}%
If events A and B are both \hyperref[DefnIndependentEvents]{independent~\ref{DefnIndependentEvents}} and \hyperref[DefnMutuallyExclusive]{mutually exclusive~\ref{DefnMutuallyExclusive}}, then at least one of them has zero probability.%
\end{corollary}
\begin{proof}\hypertarget{proof-23}{}
\hypertarget{p-606}{}%
By independence, \(P(A \cap B) = P(A) \cdot P(B)\). However, by mutually exclusivity, \(A \cap B = \emptyset \Rightarrow P(A \cap B) = 0\) gives%
\begin{equation*}
P(A) \cdot P(B) = 0.
\end{equation*}
Hence, one or the other (or both) must be zero.%
\end{proof}
%
\par
\hypertarget{p-607}{}%
\begin{corollary}{Successive Independent Events.}{}{corollary-5}%
\hypertarget{p-608}{}%
Given a sequence of mutually independent events \(A_1, A_2, A_3, ...\),%
\begin{equation*}
P(\cap_{k \in R} A_k) = \prod_{k \in R} P(A_k)
\end{equation*}
%
\end{corollary}
%
%
%
\typeout{************************************************}
\typeout{Section 4.8 Summary}
\typeout{************************************************}
%
\section[{Summary}]{Summary}\label{section-30}
\hypertarget{p-609}{}%
TBA%
%
%
\typeout{************************************************}
\typeout{Section 4.9 More Exercises}
\typeout{************************************************}
%
\section[{More Exercises}]{More Exercises}\label{section-31}
\hypertarget{p-610}{}%
\begin{inlineexercise}{Conditional Basic computation.}{exercise-35}%
\hypertarget{p-611}{}%
Given P(A) = 0.43, P(B) = 0.72, and \(P(A \cap B) = 0.29\), determine \leavevmode%
\begin{enumerate}
\item\hypertarget{li-155}{}\(P(A \cup B)\)%
\item\hypertarget{li-156}{}\(P(B | A)\)%
\item\hypertarget{li-157}{}\(P(A | B)\)%
\item\hypertarget{li-158}{}\(P(A^c \cap B^c)\)%
\end{enumerate}
%
\end{inlineexercise}
%
\par
\hypertarget{p-612}{}%
\begin{inlineexercise}{Gender vs University Major.}{exercise-36}%
\hypertarget{p-613}{}%
The table below classifies students at your university according to gender and according to major. \leavevmode%
\begin{table}
\centering
\begin{tabular}{rrrr}
\multicolumn{1}{rB}{Enrollment}&Male&\multicolumn{1}{rB}{Female}&Totals\tabularnewline\hrulemedium
\multicolumn{1}{rB}{STEM}&420&\multicolumn{1}{rB}{510}&930\tabularnewline[0pt]
\multicolumn{1}{rB}{Business}&320&\multicolumn{1}{rB}{270}&590\tabularnewline[0pt]
\multicolumn{1}{rB}{Other}&610&\multicolumn{1}{rB}{710}&1320\tabularnewline\hrulemedium
\multicolumn{1}{rB}{Totals}&1350&\multicolumn{1}{rB}{1490}&2840
\end{tabular}
\caption{Gender vs Major\label{table-10}}
\end{table}
 Determine the following: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-159}{}P( STEM major )%
\item\hypertarget{li-160}{}P( STEM | Female )%
\item\hypertarget{li-161}{}P( Female | STEM )%
\item\hypertarget{li-162}{}P( Female | Not STEM)%
\end{enumerate}
%
\end{inlineexercise}
%
\par
\hypertarget{p-614}{}%
\begin{inlineexercise}{Mean Tough Teacher.}{exercise-37}%
\hypertarget{p-615}{}%
You are in a probability and statistics class with a teacher who has predetermined that only one student can make an A for the course. To be "fair", he places a number of slips of paper in a bowl equal to the number of students in the course with one of the slips having an A designation. Students in the course each can pick once randomly from the bowl and without replacement to see if they can get the lucky slip.  Determine the following: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-163}{}If there are 15 students in your course, determine the probabilities of getting an A in the course if you pick first and if you pick last.%
\item\hypertarget{li-164}{}Since the teacher likes you the most, she will give you the option of deciding whether to pick at any position. If so, determine the position that would give you the best likelihood of getting the A slop.%
\item\hypertarget{li-165}{}Suppose again that the teacher was feeling more generous and decided instead to allow for two A's. Determine how that changes your likelihood of winning and on what position you would like to choose.%
\item\hypertarget{li-166}{}Continue as above except that only one slip does not have an A on it.%
\item\hypertarget{li-167}{}Discuss how your choice is affected by the number of students in the course or the number of A slips included.%
\end{enumerate}
%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-14}{}\quad%
\hypertarget{p-616}{}%
Using the normal equally-likely definition, \(P(\text{first}) = \frac{1}{15}\).%
\par
\hypertarget{p-617}{}%
To get the A on the last pick requires that all of the previous picks to be something else. You don't get the opportunity to pick the A if it has already been selected. So, if L stands for losing (not getting the A), then%
\begin{gather*}
P(\text{last}) = P(\text{LLLLLLLLLLLLLLA}) = \frac{14 \cdot 13 \cdot 12 \cdot 11 \cdot 10 \cdot 9  \cdot 8  \cdot 7  \cdot 6  \cdot 5  \cdot 4  \cdot 3 \cdot 2  \cdot 1}{15 \cdot 14 \cdot 13 \cdot 12 \cdot 11 \cdot 10 \cdot 9 \cdot 8 \cdot 7 \cdot 6 \cdot 5 \cdot 4 \cdot 3 \cdot 2} = \frac{1}{15}.
\end{gather*}
Therefore, it is the same probability of getting the A whether you pick first or last.  In general, to win on the kth pick gives%
\begin{gather*}
P(\text{kth}) = P(\text{LL...LA}) = \frac{14 \cdot 13 \cdot ... \cdot (15-k) \cdot 1}{15 \cdot 14 ... \cdot (16-k) \cdot (15-k)} = \frac{1}{15}
\end{gather*}
Hence, it is the same probability regardless of when you get to pick.%
\par
\hypertarget{p-618}{}%
If there are two A's possible, then the options for person k in include either receiving the first of the two slips or the second. The probability for determining the first of the two is computed in a manner similar to above except that there is one more A and one less other.%
\begin{gather*}
P(\text{kth as first}) = P(\text{LL...LA}) = \frac{13 \cdot 12 \cdot ... \cdot (15-k)  \cdot 2}{15 \cdot 14 ... \cdot (16-(k+1)) \cdot (16-k)} = \frac{2 \cdot (15-k)}{15 \cdot 14}
\end{gather*}
The probability of getting the second A means exactly one of the previous k-1 selections also picked the other A. There are k-1 ways that this could happen. Computing for one of the options and multiplying by k-1 gives%
\begin{gather*}
P(\text{kth as second}) = P(\text{LL...LAA}) = (k-1) \cdot \frac{13 \cdot 12 \cdot ... \cdot (15-k) \cdot 2 \cdot 1}{15 \cdot 14 ... \cdot (16-k) \cdot (15-k)} = \frac{2 \cdot (k-1)}{15 \cdot 14}.
\end{gather*}
Adding these two together gives%
\begin{gather*}
P(\text{getting an A when there are two}) = \frac{2 \cdot (15-k) + 2 \cdot (k-1)}{15 \cdot 14}\\
= \frac{28}{15 \cdot 14} = \frac{2}{15}.
\end{gather*}
For example, if k = 5,%
\begin{gather*}
P(\text{5th as first}) = P(\text{LLLLA}) \\
= \frac{13 \cdot 12 \cdot 11 \cdot 10  \cdot 2}{15 \cdot 14 \cdot 13 \cdot 12 \cdot 11} = \frac{20}{15 \cdot 14}
\end{gather*}
%
\begin{gather*}
P(\text{5th as second}) = P(\text{LL...LAA}) \\
= 4 \cdot \frac{13 \cdot 12 \cdot \cdot 11 \cdot 2 \cdot 1}{15 \cdot 14 \cdot 13 \cdot 12 \cdot 11} = \frac{8}{15 \cdot 14}.
\end{gather*}
Adding these together yields the general result. So, once again, it doesn't matter which pick you use since the likelihood of getting an A is the same for all positions.%
\end{inlineexercise}
%
\par
\hypertarget{p-619}{}%
\begin{inlineexercise}{Shared Birthdays.}{exercise-38}%
\hypertarget{p-620}{}%
In this problem, you want to consider how many people are necessary in order to have an even chance of finding two or more who share a common birthday. Toward that end, assuming a year has exactly 365 equally likely days let r be the number of people in a sample and consider the following: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-168}{}Determine the number of different outcomes of birthdays when order matters and birthdays are allowed to be repeated.%
\item\hypertarget{li-169}{}Determine the number of different outcomes when birthdays are not allowed to be repeated.%
\item\hypertarget{li-170}{}Determine the probability that two or more of your r students have the same birthday.%
\item\hypertarget{li-171}{}Prepare a spreadsheet with the probabilities found above from r=2 to r=50. Determine the value of r for which this probability is closest to 0.5.%
\item\hypertarget{li-172}{}As best as you can, sample two groups of the size found above and gather birthday information. For each group, determine if there is a shared birthday or not.  Compare your results with others in the class to check whether the sampling validates that about half of the samples should have a shared birthday group.%
\end{enumerate}
%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-15}{}\quad%
\hypertarget{p-621}{}%
The correct sample size to get past a probability of 0.5 is 23 people. You should justify this numerically by justifying the following probabilities:\begin{preformatted}
#	P(Match)	
1	0
2	0.0027
3	0.0082
4	0.0164
5	0.0271
6	0.0405
7	0.0562
8	0.0743
9	0.0946
10	0.1169
11	0.1411
12	0.1670
13	0.1944
14	0.2231
15	0.2529
16	0.2836
17	0.3150
18	0.3469
19	0.3791
20	0.4114
21	0.4437
22	0.4757
23	0.5073
24	0.5383
25	0.5687
26	0.5982
27	0.6269
28	0.6545
29	0.6810
30	0.7063
\end{preformatted}
%
\end{inlineexercise}
%
\par
\hypertarget{p-622}{}%
\begin{inlineexercise}{Internet meme solution.}{exercise-39}%
\hypertarget{p-623}{}%
This one is from an internet meme:  Two fair 6-sided dice are rolled together and you are told that at least one of the dice is a 6. Given that a 6 will be removed, determine the probability that the other die is a 6.%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-16}{}\quad%
\hypertarget{p-624}{}%
In this case, you are presented with an outcome where the possible choices consist of (1,6), (2,6), (3,6), (4,6), (5,6), (6,6), (6,5), (6,4), (6,3), (6,2), (6,1).  Each of these would satisfy the condition that at least one of the dice is a 6. From this group, the only success that satisfies being a 6, given that another 6 has already been removed, is the (6,6) outcome. Therefore, the conditional probability is 1/11.%
\par
\hypertarget{p-625}{}%
It is interesting to note that if the question instead was posed so that one of the dice was a 6 and it was removed, then the probability of the other dice showing a 6 would be 1/6.%
\end{inlineexercise}
%
\par
\hypertarget{p-626}{}%
\begin{inlineexercise}{100 people on an airplane with boarding pass issues.}{exercise-40}%
\hypertarget{p-627}{}%
This is a famous problem.  100 people are in line, boarding an airplane with 100 seats, one at a time. They are in no particular order. The first person has lost his boarding pass, so he sits in a random seat. The second person does the following: \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}Goes to his seat (the one it says to go to on the boarding pass). If unoccupied, sit in it.%
\item{}If occupied, find a random seat to sit in.%
\end{itemize}
 Everyone else behind him does the same. What is the probability that the last person sits in his correct seat?%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-17}{}\quad%
\hypertarget{p-628}{}%
To get the idea, consider what happens with only 2 people, then only 3. Generalize.%
\par
\hypertarget{p-629}{}%
The answer is 1/2. To obtain this, you can define recursively the probability that the kth person sits in their own set as f(k).  Consider the first traveler's and your seats. Then you get the following cases: \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}P(first guy sits in his own seat and you sit in yours) = \(\frac{1}{k} \cdot 1\)%
\item{}P(first guy sits in your seat and you do not sit in yours) = \(\frac{1}{k} \cdot 0\)%
\item{}P(other k-2 travelers make their choices) = \((k-2) \frac{1}{k} f(k-1)\)%
\end{itemize}
%
\begin{equation*}
f(k) = 1/k + 0 + (k-2)/k f(k-1)
\end{equation*}
with f(2) = 1/2.%
\par
\hypertarget{p-630}{}%
For example, \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}f(3) = 1/3 + f(2)/3 = 1/3 + 1/6 = 1/2.%
\item{}f(4) = 1/4 + 2/4 f(3) = 1/4 + 1/2 1/2 = 1/2.%
\item{}f(5) = 1/5 + 3/5 1/2 = 1/2.%
\item{}f(6) = 1/6 + 4/6 1/2 = 1/2.%
\end{itemize}
 Etc.%
\end{inlineexercise}
%
\par
\hypertarget{p-631}{}%
\begin{inlineexercise}{Basic Independence Calculations.}{exercise-41}%
\hypertarget{p-632}{}%
Given P(A) = 0.43, P(B) = 0.72, and \(P(A \cap B) = 0.31\), verify that A and B are not independent.%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-18}{}\quad%
\hypertarget{p-633}{}%
A and B are \hyperref[DefnIndependentEvents]{independent by definition~\ref{DefnIndependentEvents}} provided%
\begin{equation*}
P(A \cap B) = P(A) P(B).
\end{equation*}
Using the provided values, notice that%
\begin{equation*}
P(A \cap B) = 0.31
\end{equation*}
but%
\begin{equation*}
P(A)P(B) = 0.43 \cdot 0.72 = 0.3096.
\end{equation*}
Since these are not equal (regardless how close) then A and B are not independent.%
\end{inlineexercise}
%
\par
\hypertarget{p-634}{}%
\begin{inlineexercise}{Compound events and Independence.}{exercise-42}%
\hypertarget{p-635}{}%
Given A, B, and C are independent events, with P(A) = 2/5, P(B) = 3/4, and P(C) = 1/6, determine: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-182}{}\(P(A \cap B \cap C)\)%
\item\hypertarget{li-183}{}\(P(A^c \cap B^c \cap C)\)%
\item\hypertarget{li-184}{}\(P(A \cup B \cup C)\)%
\end{enumerate}
%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-19}{}\quad%
\hypertarget{p-636}{}%
Extending the \hyperref[DefnIndependentEvents]{definition of independent events~\ref{DefnIndependentEvents}} gives%
\begin{equation*}
P(A \cap B \cap C) = \frac{2}{5} \frac{3}{4} \frac{1}{6}.
\end{equation*}
By the corollary for independent events, complements also maintain a similar independence. So%
\begin{equation*}
P(A^c \cap B^c \cap C) \frac{3}{5} \frac{1}{4} \frac{1}{6} .
\end{equation*}
To complete the third part, use the \hyperref[ProbabilityThreeUnions]{inclusion/exclusion result~\ref{ProbabilityThreeUnions}} for dealing with three sets.%
\end{inlineexercise}
%
\par
\hypertarget{p-637}{}%
\begin{inlineexercise}{Rolling multiple dice.}{exercise-43}%
\hypertarget{p-638}{}%
For a pair of dice you want to consider the events A = {rolling a 7 or 11} and B = {otherwise}...as in the first roll in the game of craps.  Further, for notation purposes let's take ABA (for example) to mean event A occurs on the first roll, event B occurs on the second roll, and event A occurs again on the third roll...in that order only. If you roll the dice 5 times, determine \leavevmode%
\begin{enumerate}
\item\hypertarget{li-185}{}P(AABBB)%
\item\hypertarget{li-186}{}P(BBBAA)%
\item\hypertarget{li-187}{}The probability of getting A on exactly two rolls of the dice.%
\end{enumerate}
%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-20}{}\quad%
\hypertarget{p-639}{}%
Successive rollings of a pair of dice are \hyperref[DefnIndependentEvents]{independent events~\ref{DefnIndependentEvents}}. Therefore,%
\begin{equation*}
P(AABBB) = P(A)P(A)P(B)P(B)P(B) = \frac{8}{36} \frac{8}{36} \frac{28}{36} \frac{28}{36} \frac{28}{36}
\end{equation*}
Similarly for the second part.%
\par
\hypertarget{p-640}{}%
For the third part, notice that there will be \(\binom{5}{2}\) ways to rearrange 2 A's and 3 B's but that each of these will have two 8/36's and three 28/36's but just in a different order. Therefore, you will get%
\begin{equation*}
10 \cdot \frac{8}{36} \frac{8}{36} \frac{28}{36} \frac{28}{36} \frac{28}{36}
\end{equation*}
%
\end{inlineexercise}
%
\par
\hypertarget{p-641}{}%
\begin{inlineexercise}{Redundancy.}{exercise-44}%
\hypertarget{p-642}{}%
To help "insure" the success of a mission, you propose several redundant components so that the mission is a success if one or more succeed. Supposing that these separate components act independently of each other and that each component has a 75% chance of success, determine: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-188}{}The probability of failure if you utilize 2 components.%
\item\hypertarget{li-189}{}The probability of failure if you utilize 5 components.%
\item\hypertarget{li-190}{}The number of components needed to insure that the probability of success is at least 99%.%
\end{enumerate}
%
\end{inlineexercise}
%
\par
\hypertarget{p-643}{}%
\begin{inlineexercise}{Internet Meme redux.}{exercise-45}%
\hypertarget{p-644}{}%
Again, from an internet meme:  Two fair 6-sided dice are rolled together and you are told that at least one of the dice is a 6. A 6 is removed and you are presented with the other die.  Determine the probability that it is a 6.%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-21}{}\quad%
\hypertarget{p-645}{}%
For this setting, notice that the outcomes from each of the two dice are independent of each other. Removing one of the dice, regardless of it's value, does not affect the other. The question in this case does not ask for a conditional probability.%
\end{inlineexercise}
%
\par
\hypertarget{p-646}{}%
\begin{inlineexercise}{Single Elimination Tounament.}{exercise-46}%
\hypertarget{p-647}{}%
Consider a n=4 team single-elimination tournament where the teams are "seeded" from 1 (the best team) to 4 (the worst team).  For this tournament, team 1 plays team 4 and team 2 plays team 3. The winner of each play each other to determine the final winner. When teams j and k play, set P(j wins) = \(\frac{k}{j+k}\) and similarly for team k.  Assuming separate games are independent of each other, determine the probability that team 4 wins the tournament. What about with 8 teams? What about 64 teams?%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-22}{}\quad%
\hypertarget{p-648}{}%
P(4 wins) = P(4 beats 1) P(4 beats the winner of the other bracket)%
\par
\hypertarget{p-649}{}%
P(4 wins) = (1/5) * P(4 beats 2 | 2 beats 3) + P(4 beats 3 | 3 beats 2)%
\par
\hypertarget{p-650}{}%
P(4 wins) = 1/5 [(3/5)(2/6) + (2/5)(3/7)] = 78/1050 = 0.0742%
\par
\hypertarget{p-651}{}%
For the other teams:%
\par
\hypertarget{p-652}{}%
P(1 wins) = 4/5 [(3/5)(2/3) + (2/5)(3/4) ] = 0.56%
\par
\hypertarget{p-653}{}%
P(2 wins) = 3/5 [(4/5)(1/3) + (1/5)(4/6) ] = 0.24%
\par
\hypertarget{p-654}{}%
P(3 wins) = 2/5 [(4/5)(1/4) + (1/5)(4/7) ] = 0.1257%
\end{inlineexercise}
%
%
%
\typeout{************************************************}
\typeout{Chapter 5 Probability Functions}
\typeout{************************************************}
%
\chapter[{Probability Functions}]{Probability Functions}\label{ProbabilityFunctions}
%
%
\typeout{************************************************}
\typeout{Section 5.1 Introduction}
\typeout{************************************************}
%
\section[{Introduction}]{Introduction}\label{section-32}
\hypertarget{p-655}{}%
Each of the probability exercises thus far required you to utilize basic definitions and theorems to determine the answer. Starting a new problem meant starting over from scratch. This is burdensome.  However, you may have noticed that some of the ways you might have created solutions for some problems ending up looking very similar to the solutions for others. In this chapter, you will consider the framework needed for creating general solution techniques. These techniques will give a number of "distributions" which are general ways to solve a particular type of problem.%
\par
\hypertarget{p-656}{}%
Toward that end, in this chapter you will see how to create a random variable which takes items in the sample space and assigns corresponding numerical values. From that, you will see how to create "Probability Functions" on that variable that provide the desired probability by simple function evaluation. General properties these functions possess will also be developed.%
%
%
\typeout{************************************************}
\typeout{Section 5.2 Random Variables}
\typeout{************************************************}
%
\section[{Random Variables}]{Random Variables}\label{section-33}
\hypertarget{p-657}{}%
For a given set of events, we might have difficulty doing mathematics since the outcomes are not numerical. In order to accomodate our desire to convert to numerical measures we want to assign numerical values to all outcomes. The process of doing this creates what is known as a random variable.%
\par
\hypertarget{p-658}{}%
\begin{definition}{Random Variable.}{definition-26}%
\hypertarget{p-659}{}%
Given a random experiment with sample space S, a function X mapping each element of S to a unique real number is called a random variable. For each element s from the sample space S, denote this function by%
\begin{equation*}
X(s) = x
\end{equation*}
and let R  be the range of X. R will be called "the space of X" and in notation%
\begin{equation*}
R = { x : X(s)=x, \text{for some} s \in S } .
\end{equation*}
%
\end{definition}
%
\begin{figure}\centering\includegraphics[width=1\linewidth]{images/randomvariable.png}
\end{figure}\hypertarget{p-660}{}%
We will make various restrictions on the range of the random variable to fit different generalized problems. Then, we will be able to work on a problem (which may be inherently non-numerical) by using the random variable in subsequent calculations.%
\par
\hypertarget{p-661}{}%
\begin{example}{Success vs Failure.}{example-18}%
\hypertarget{p-662}{}%
When dealing with only two outcomes, one might use%
\begin{equation*}
S = \text{{ success, failure }}.
\end{equation*}
Choose%
\begin{gather*}
X(success)=1\\
X(failure)=0.
\end{gather*}
Then, R={0,1}.%
\end{example}
%
\par
\hypertarget{p-663}{}%
\begin{example}{Standard Dice Pairs.}{example-19}%
\hypertarget{p-664}{}%
When gambling with a pair of dice, one might use S=ordered pairs of all possible rolls.  Then%
\begin{equation*}
S = \text{ {(a,b): a=die 1 outcome, b=die 2 outcome}}.
\end{equation*}
Choose%
\begin{equation*}
X( (a,b) ) = a+b.
\end{equation*}
Then, R={2, 3, 4, 5, ..., 12}.%
\end{example}
%
\par
\hypertarget{p-665}{}%
\begin{example}{Other Dice Options.}{example-20}%
\hypertarget{p-666}{}%
When rolling dice in a board game (like RISK), one might use%
\begin{equation*}
S= \text{{(a,b): a=die 1 outcome, b=die 2 outcome}}
\end{equation*}
Choose%
\begin{equation*}
X( (a,b) ) = \text{max{a,b}}.
\end{equation*}
Then, R={1, 2, 3, 4, 5, 6}.%
\end{example}
%
\par
\hypertarget{p-667}{}%
\begin{definition}{Countable and Uncountable Sets.}{definition-27}%
\hypertarget{p-668}{}%
R contains a countable number of points if either R is finite or there is a one to one correspondence between R and the positive integers. Such a set will be called discrete. We will see that often the set R is not countable. If R consists of an interval of points (or a union of intervals), then we call X a continuous random variable.%
\end{definition}
%
%
%
\typeout{************************************************}
\typeout{Section 5.3 Probability Functions}
\typeout{************************************************}
%
\section[{Probability Functions}]{Probability Functions}\label{section-34}
\hypertarget{p-669}{}%
In the formulas below, we will presume that we have a random variable X which maps the sample space S onto some range of real numbers R.  From this set, we then can define a probability function f(x) which acts on the numerical values in R and returns another real number.  We attempt to do so to obtain (for discrete values) P(sample space value s)\(= f(X(s))\).  That is, the probability of a given outcome s is equal to the composition which takes s to a numerical value x which is then plugged into f to get the same final values.%
\par
\hypertarget{p-670}{}%
For example, consider a random variable which assigns a 1 when you roll a 1 on a six-sided die and 0 otherwise. Presuming each side is equally likely, \(f(1) = \frac{1}{6}\) and \(f(0) = \frac{5}{6}\).%
\par
\hypertarget{p-671}{}%
\begin{definition}{Probability "Mass" Function.}{definition-28}%
\hypertarget{p-672}{}%
Given a discrete random variable X on a space R, a probability mass function on X is given by a function \(f:R \rightarrow \mathbb{R}\) such that:%
\begin{align*}
& \forall x \in R , f(x) \gt 0\\
& \sum_{x \in R} f(x) = 1\\
& A \subset R \Rightarrow P(X \in A) = \sum_{x \in A}f(x)
\end{align*}
For \(x \not\in R\), you can use the convention f(x)=0.%
\end{definition}
%
\par
\hypertarget{p-673}{}%
\begin{definition}{Probability "Density" Function.}{definition-29}%
\hypertarget{p-674}{}%
Given a continuous random variable X on a space R, a probability density function on X is given by a function \(f:R \rightarrow \mathbb{R}\) such that:%
\begin{align*}
& \forall x \in R , f(x) \gt 0\\
& \int_{R} f(x) dx = 1\\
& A \subset R \Rightarrow P(X \in A) = \int_{A} f(x) dx
\end{align*}
For \(x \not\in R\), you can use the convention f(x)=0.%
\end{definition}
%
\par
\hypertarget{p-675}{}%
For the purposes of this book, we will use the term "Probability Function" to refer to either of these options.%
\par
\hypertarget{p-676}{}%
\begin{example}{Discrete Probability Function.}{example-21}%
\hypertarget{p-677}{}%
Consider \(f(x) = x/10\) over R = {1,2,3,4}.  Then, f(x) is obviously positive for each of the values in R and certainly%
\begin{equation*}
\sum_{x \in R} f(x) = f(1) + f(2) + f(3) + f(4) = 1/10 + 2/10 + 3/10 + 4/10 = 1.
\end{equation*}
Therefore, f(x) is a probability mass function over the space R.%
\end{example}
%
\par
\hypertarget{p-678}{}%
\leavevmode%
\begin{sageinput}
# Combining all of the above into one interactive cell
@interact
def _(D = input_box([1,2,3,5,6,8,9,11,12,14],label="Enter domain R (in brackets):"), 
       Probs = input_box([1/20,1/20,1/20,3/20,1/20,4/20,4/20,1/20,1/20,3/20],label="Enter corresponding f(x) (in brackets):"),
       n_samples=slider(100,10000,100,100,label="Number of times to sample from this distribution:")):
    n = len(D)
    R = range(n)
    one_huh = sum(Probs)
    pretty_print('\n\nJust to be certain, we should check to make certain the probabilities sum to 1\n')
    pretty_print(html('$\sum_{x\epsilon R} f(x) = %s$'%str(one_huh)))
    
    G = Graphics()
    if len(D)==len(Probs):
        f = zip(D,Probs)
        meanf = 0
        variancef = 0
        for k in R:
            meanf += D[k]*Probs[k]
            variancef += D[k]^2*Probs[k]
            G += line([(D[k],0),(D[k],Probs[k])],color='green')
        variancef = variancef - meanf^2
        sd = sqrt(variancef)
        G += points(f,color='blue',size=50)
        G += point((meanf,0),color='yellow',size=60,zorder=3)
        G += line([(meanf-sd,0),(meanf+sd,0)],color='red',thickness=5)
    
        g = DiscreteProbabilitySpace(D,Probs)
        pretty_print('     mean = %s'%str(meanf))
        pretty_print(' variance = %s'%str(variancef))
    
        #  perhaps to add mean and variance for pmf here
    else:
        print 'Domain D and Probabilities Probs must be lists of the same size'
    
    #  Now, let's sample from the distribution given above and see how a random sampling matches up

    counts = [0] * len(Probs)
    X = GeneralDiscreteDistribution(Probs)
    sample = []

    for _ in range(n_samples):
        elem = X.get_random_element()
        sample.append(D[elem])
        counts[elem] += 1
    Empirical = [1.0*x/n_samples for x in counts] # random
    
    samplemean = mean(sample)
    samplevariance = variance(sample)
    sampdev = sqrt(samplevariance)
    
    E = points(zip(D,Empirical),color='orange',size=40)
    E += point((samplemean,0.005),color='brown',size=60,zorder=3)
    E += line([(samplemean-sampdev,0.005),(samplemean+sampdev,0.005)],color='orange',thickness=5)    
    (G+E).show(ymin=0,figsize=(8,5))
\end{sageinput}
%
\par
\hypertarget{p-679}{}%
\begin{example}{Continuous Probability Function.}{example-22}%
\hypertarget{p-680}{}%
Consider \(f(x) = x^2/c\) for some positive real number c and presume R = [-1,2]. Then f(x) is nonnegative (and only equals zero at one point). To make f(x) a probability density function, we must have%
\begin{equation*}
\int_{x \in R} f(x) = 1.
\end{equation*}
In this instance you get%
\begin{equation*}
1 = \int_{-1}^2 x^2/c = x^3/(3c) |_{-1}^2 = \frac{8}{3c} - \frac{-1}{3c} = \frac{3}{c}
\end{equation*}
Therefore, f(x) is a probability density function over R provided   = 3.%
\end{example}
%
\par
\hypertarget{p-681}{}%
\begin{definition}{Distribution Function.}{definition-30}%
\hypertarget{p-682}{}%
Given a random variable X on a space R, a probability distribution function on X is given by a function%
\begin{equation*}
F:\mathbb{R} \rightarrow \mathbb{R} \ni \displaystyle F(x)=P(X \le x).
\end{equation*}
%
\end{definition}
%
\par
\hypertarget{p-683}{}%
\begin{example}{Discrete Distribution Function.}{example-23}%
\hypertarget{p-684}{}%
Using \(f(x) = x/10\) over R = {1,2,3,4} again, note that F(x) will only change at these four domain values. We get \begin{table}
\centering
\begin{tabular}{rr}
\multicolumn{1}{rB}{X}&F(x)\tabularnewline\hrulemedium
\multicolumn{1}{rB}{\(x \lt 1\)}&0\tabularnewline[0pt]
\multicolumn{1}{rB}{\(1 \le x \lt 2\)}&1/10\tabularnewline[0pt]
\multicolumn{1}{rB}{\(2 \le x \lt 3\)}&3/10\tabularnewline[0pt]
\multicolumn{1}{rB}{\(3 \le x \lt 4\)}&6/10\tabularnewline[0pt]
\multicolumn{1}{rB}{\(4 \le x \)}&1
\end{tabular}
\caption{Discrete Distribution Function Example\label{table-11}}
\end{table}
%
\end{example}
%
\par
\hypertarget{p-685}{}%
\begin{example}{Continuous Distribution Function.}{example-24}%
\hypertarget{p-686}{}%
Consider \(f(x) = x^2/3\) over R = [-1,2].  Then, for \(-1 \le x \le 2\),%
\begin{equation*}
F(x) = \int_{-1}^x u^2/3 du = x^3/9 + 1/9.
\end{equation*}
Notice, F(-1) = 0 since nothing has yet been accumulated over values smaller than -1 and F(2)=1 since by that time everything has been accumulated. In summary: \begin{table}
\centering
\begin{tabular}{rr}
\multicolumn{1}{rB}{X}&F(x)\tabularnewline\hrulemedium
\multicolumn{1}{rB}{\(x \lt -1\)}&0\tabularnewline[0pt]
\multicolumn{1}{rB}{\(-1 \le x \lt 2\)}&\(x^3/9 + 1/9\)\tabularnewline[0pt]
\multicolumn{1}{rB}{\(2 \le x\)}&1
\end{tabular}
\caption{Continuous Distribution Function Example\label{table-12}}
\end{table}
%
\end{example}
%
\par
\hypertarget{p-687}{}%
\begin{theorem}{}{}{theorem-Fmin}%
\hypertarget{p-688}{}%
\(F(x)=0, \forall x \lt \inf(R)\)%
\end{theorem}
\begin{proof}\hypertarget{proof-24}{}
\hypertarget{p-689}{}%
Let a = inf(R). Then, for \(x \lt a,\)%
\begin{equation*}
F(x) = P(X \le x) \le P(X \lt a) = 0
\end{equation*}
since none of the x-values in this range are in R.%
\end{proof}
%
\par
\hypertarget{p-690}{}%
\begin{theorem}{}{}{theorem-Fmax}%
\hypertarget{p-691}{}%
\(F(x)=1, \forall x \ge \sup(R)\)%
\end{theorem}
\begin{proof}\hypertarget{proof-25}{}
\hypertarget{p-692}{}%
Let b = sup(R). Then, for%
\begin{equation*}
x \ge b, F(x) = P(X \le x)  = P(X \le b) + P( b \lt X \le x) = P(X \le b) = 1
\end{equation*}
since all of the x-values in this range are in R and therefore will either sum over or integrate over all of R.%
\end{proof}
%
\par
\hypertarget{p-693}{}%
\begin{theorem}{}{}{theorem-24}%
\hypertarget{p-694}{}%
F is non-decreasing%
\end{theorem}
\begin{proof}\hypertarget{proof-26}{}
\hypertarget{p-695}{}%
Case 1: R discrete%
\begin{align*}
\forall x_1,x_2 \in \mathbb{Z} \ni x_1 \lt x_2\\
F(x_2) & = \sum_{x \le x_2} f(x) \\
& = \sum_{x \le x_1} f(x) + \sum_{x_1 \lt x \le x_2} f(x)\\
& \ge \sum_{x \le x_1} f(x) = F(x_1)
\end{align*}
%
\par
\hypertarget{p-696}{}%
Case 2: R continuous%
\begin{align*}
\forall x_1,x_2 \in \mathbb{R} \ni x_1 \lt x_2\\
F(x_2) & = \int_{-\infty}^{x_2} f(x) dx \\
& = \int_{-\infty}^{x_1} f(x) dx + \int_{x_1}^{x_2} f(x) dx\\
& \ge \int_{-\infty}^{x_1} f(x) dx\\
& = F(x_1)\qedhere
\end{align*}
%
\end{proof}
%
\par
\hypertarget{p-697}{}%
\begin{theorem}{Using Discrete Distribution Function to compute probabilities.}{}{theorem-Fvsf-discrete}%
\hypertarget{p-698}{}%
For \(x \in R, f(x) = F(x) - F(x-1)\)%
\end{theorem}
\begin{proof}\hypertarget{proof-27}{}
\hypertarget{p-699}{}%
Assume \(x \in R\) for some discrete R. Then,%
\begin{equation*}
F(x) - F(x-1) = \sum_{u \le x} f(u) - \sum_{u \lt x} f(u) = f(x)\qedhere
\end{equation*}
%
\end{proof}
%
\par
\hypertarget{p-700}{}%
\begin{theorem}{Using Continuous Distribution function to compute probabilities.}{}{theorem-Fvsf-continuyous}%
\hypertarget{p-701}{}%
For \(a \lt b, (a,b) \in R, P(a \lt X \le b) = F(b) - F(a)\)%
\end{theorem}
\begin{proof}\hypertarget{proof-28}{}
\hypertarget{p-702}{}%
For a and b as noted, consider%
\begin{align*}
F(b) - F(a) & = \int_{-\infty}^b f(x) dx - \int_{-\infty}^a f(x) dx\\
& = \int_a^b f(x) dx \\
& = P(a \lt x \le b)\qedhere
\end{align*}
%
\end{proof}
%
\par
\hypertarget{p-703}{}%
\begin{corollary}{}{}{corollary-ProbPointZero-continuous}%
\hypertarget{p-704}{}%
For continuous distributions, P(X = a) = 0%
\end{corollary}
\begin{proof}\hypertarget{proof-29}{}
\hypertarget{p-705}{}%
We will assume that F(x) is a continuous function. With that assumption, note%
\begin{equation*}
P(a-\epsilon \lt  x \le a)  = \int_{a-\epsilon}^a f(x) dx = F(a) - F(a-\epsilon)
\end{equation*}
Take the limit as \(\epsilon \rightarrow 0^+\) to get the result noting that%
\end{proof}
%
\par
\hypertarget{p-706}{}%
\begin{theorem}{F(x) vs f(x), for continuous distributions.}{}{theorem-27}%
\hypertarget{p-707}{}%
If X is a continuous random variable, f the corresponding probability function, and F the associated distribution function, then%
\begin{equation*}
f(x) = F'(x)
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-30}{}
\hypertarget{p-708}{}%
Assume X is continuous and f and F as above. Notice, by the definition of f, \(\lim_{x \rightarrow \pm \infty} f(x) = 0\) since otherwise the integral over the entire space could not be finite.%
\par
\hypertarget{p-709}{}%
Now, let A(x) be any antiderivative of f(x). Then, by the Fundamental Theorem of Calculus,%
\begin{align*}
F(x) & = \int_{-\infty}^x f(u) du\\
& = A(x) - \lim_{u \rightarrow -\infty} A(u)
\end{align*}
Hence, \(F'(x) = A'(x) - \lim_{u \rightarrow -\infty} A'(u) = f(x)\) as desired.%
\end{proof}
%
\par
\hypertarget{p-710}{}%
\begin{definition}{Percentiles for Random Variables.}{definition-31}%
\hypertarget{p-711}{}%
For \(0 \lt p \lt 1\), the \(100p^{th}\) percentile is the largest random variable value c that satisfies%
\begin{equation*}
F(c) = p.
\end{equation*}
For continuous random variables over an interval R = [a,b], you will solve for c in the equation%
\begin{equation*}
\int_a^c f(x) dx.
\end{equation*}
For discrete random variables, it is unlikely that a particular percentile will land exactly on one of the elements of R but you will want to take the smallest value in R so that \(F(c) \ge p\).%
\par
\hypertarget{p-712}{}%
The 50th percentile (as before) is also known as the median.%
\end{definition}
%
\par
\hypertarget{p-713}{}%
\begin{example}{Continuous Percentile.}{example-25}%
\hypertarget{p-714}{}%
For our earlier example with \(f(x) = x^2/3\) on R = [-1,2], the 50th percentile (i.e. the median) is found by starting with p = 0.5 and then solving%
\begin{equation*}
F(c) = 0.5
\end{equation*}
or%
\begin{equation*}
c^3/9 + 1/9 = 1/2
\end{equation*}
or%
\begin{equation*}
c^3 + 1 = 9/2.
\end{equation*}
After solving for c, you find%
\begin{equation*}
\text{median} = \sqrt[3]{7/2} \approx 1.518.
\end{equation*}
%
\end{example}
%
\par
\hypertarget{p-715}{}%
\begin{example}{Discrete Percentile.}{example-26}%
\hypertarget{p-716}{}%
TBA, using one of the table examples from above.%
\end{example}
%
%
%
\typeout{************************************************}
\typeout{Section 5.4 Expected Value}
\typeout{************************************************}
%
\section[{Expected Value}]{Expected Value}\label{section-35}
\hypertarget{p-717}{}%
Blaise Pascal was a 	17th century mathematician and philosopher who was accomplished in many areas but may likely be best known to you for his creation of what is now known as Pascal's Triangle. As part of his philosophical pursuits, he proposed what is known as "Pascal's wager". It suggests two  mutually exclusive outcomes: that God exists or that he does not. His argument is that a rational person should live as though God exists and seek to believe in God. If God does not actually exist, such a person will have only a finite loss (some pleasures, luxury, etc.), whereas they stand to receive infinite gains as represented by eternity in Heaven and avoid an infinite losses of eternity in Hell. This type of reasoning is part of what is known as "decision theory".%
\par
\hypertarget{p-718}{}%
You may not confront such dire payouts when making your daily decisions but we need a formal method for making these determinations precise. The procedure for doing so is what we call expected value.%
\par
\hypertarget{p-719}{}%
\begin{definition}{Expected Value.}{definition-32}%
\hypertarget{p-720}{}%
Given a random variable X over space R, corresponding probability function f(x) and "value function" v(x), the expected value of v(x) is given by%
\begin{equation*}
E = E[v(X)] = \sum_{x \in R} v(x) f(x)
\end{equation*}
provided X is discrete, or%
\begin{equation*}
E = E[v(X)] = \int_R v(x)f(x) dx
\end{equation*}
provided X is continuous.%
\end{definition}
%
\par
\hypertarget{p-721}{}%
\begin{theorem}{Expected Value is a Linear Operator.}{}{theorem-28}%
\hypertarget{p-722}{}%
\leavevmode%
\begin{enumerate}
\item\hypertarget{li-191}{}E[c] = c%
\item\hypertarget{li-192}{}E[c v(X)] = c E[v(X)]%
\item\hypertarget{li-193}{}E[v(X) + w(X)] = E[v(X)] + E[w(X)]%
\end{enumerate}
%
\end{theorem}
\begin{proof}\hypertarget{proof-31}{}
\hypertarget{p-723}{}%
Each of these follows by utilizing the corresponding linearity properties of the summation and integration operations. For example, to verify part three in the continuous case:%
\begin{align*}
E[v(X) + w(X)] & = \int_{x \in R} [v(x)+w(x)]f(x) dx\\
& = \int_{x \in R} v(x)f(x) dx + \int_{x \in R} w(x)f(x) dx\\
& = E[v(X)] + E[w(X)].\qedhere
\end{align*}
%
\end{proof}
%
\par
\hypertarget{p-724}{}%
\begin{example}{Discrete Expected Value.}{example-27}%
\hypertarget{p-725}{}%
Consider \(f(x) = x/10\) over R = {1,2,3,4} where the payout is 10 euros if x=1, 5 euros if x=2, 2 euros if x=3 and -7 euros if x = 4.  Then your value function would be%
\begin{equation*}
v(1)=10, v(2) = 5, v(3)=2, v(4) = -7.
\end{equation*}
Computing the expect payout gives%
\begin{equation*}
E = 10 \times 1/10 + 5 \times 2/10 + 2 \times 3/10 - 7 \times 4/10 = -2/10
\end{equation*}
Therefore, the expected payout is actually negative due to a relatively large negative payout associated with the largest likelihood outcome and the larger positive payout only associated with the least likely outcome.%
\end{example}
%
\par
\hypertarget{p-726}{}%
\begin{example}{Continuous Expected Value.}{ContinuousEV-example}%
\hypertarget{p-727}{}%
Consider \(f(x) = x^2/3\) over R = [-1,2] with value function given by \(v(x) = e^x - 1\). Then, the expected value for v(x) is given by%
\begin{equation*}
E = \int_{-1}^2 (e^x-1) \cdot x^2/3 = -1/9 \cdot (e + 15) \cdot e^{-1} + 2/3 \cdot e^2 - 8/9 \approx 3.3129
\end{equation*}
%
\end{example}
%
\par
\hypertarget{p-728}{}%
\begin{definition}{Theoretical Measures.}{TheoreticalMeasures}%
\hypertarget{p-729}{}%
Given a random variable with probability function f(x) over space R \leavevmode%
\begin{enumerate}
\item\hypertarget{li-194}{}The mean of X = \(\mu = E[x]\)%
\item\hypertarget{li-195}{}The variance of X = \(\sigma^2 = E[(x-\mu)^2]\)%
\item\hypertarget{li-196}{}The skewness of X = \(\gamma_1 = \frac{E[(x-\mu)^3]}{\sigma^3}\)%
\item\hypertarget{li-197}{}The kurtosis of X = \(\gamma_2 = \frac{E[(x-\mu)^4]}{\sigma^4}\)%
\end{enumerate}
%
\end{definition}
%
\par
\hypertarget{p-730}{}%
\begin{theorem}{Alternate Formulas for Theoretical Measures.}{}{TheoreticalMeasuresAlternates}%
\hypertarget{p-731}{}%
\leavevmode%
\begin{enumerate}
\item\hypertarget{li-198}{}\(\sigma^2 = E[x^2] - \mu^2 = E[X(x-1)] + \mu - \mu^2\)%
\item\hypertarget{li-199}{}\(\gamma_1 = \frac{1}{\sigma^3} \cdot \left [ E[X^3] - 3 \mu E[X^2] + 2\mu^3 \right ]\)%
\item\hypertarget{li-200}{}\(\gamma_2 = \frac{1}{\sigma^4} \cdot \left [ E[X^4] - 4 \mu E[X^3] + 6\mu^2 E[X^2] - 3 \mu^4 \right ]\)%
\end{enumerate}
%
\end{theorem}
\begin{proof}\hypertarget{proof-32}{}
\hypertarget{p-732}{}%
In each case, expand the binomial inside and use the linearity of expected value.%
\end{proof}
%
\par
\hypertarget{p-733}{}%
Consider the following example when computing these statistics for a discrete variable. In this case, we will utilize a variable with a relatively small space so that the summations can be easily done by hand. Indeed, consider \leavevmode%
\begin{table}
\centering
\begin{tabular}{rr}
\multicolumn{1}{rB}{X}&f(x)\tabularnewline\hrulemedium
\multicolumn{1}{rB}{0}&0.10\tabularnewline[0pt]
\multicolumn{1}{rB}{1}&0.25\tabularnewline[0pt]
\multicolumn{1}{rB}{2}&0.40\tabularnewline[0pt]
\multicolumn{1}{rB}{4}&0.15\tabularnewline[0pt]
\multicolumn{1}{rB}{7}&0.10
\end{tabular}
\caption{Discrete Probability Function Example\label{table-13}}
\end{table}
 \begin{figure}\centering\includegraphics[width=1\linewidth]{images/DiscreteHistogramExample.png}
\end{figure}%
\par
\hypertarget{p-734}{}%
Using the definition of mean as a sum,%
\begin{align*}
\mu & = 0 \cdot 0.10 + 1 \cdot 0.25 + 2 \cdot 0.40 + 4 \cdot 0.15 + 7 \cdot 0.10\\
& = 0 + 0.25 + 0.80 + 0.60 + 0.70\\
& = 2.35
\end{align*}
Notice where this lies on the probability histogram for this distribution.%
\par
\hypertarget{p-735}{}%
For the variance%
\begin{align*}
\sigma^2 & = E[X^2] - \mu^2\\
& = \left [ 0^2 \cdot 0.10 + 1^2 \cdot 0.25 + 2^2 \cdot 0.40 + 4^2 \cdot 0.15 + 7^2 \cdot 0.10 \right ] - 2.35^2\\
& = 0 + 0.25 + 1.60 + 2.40 + 4.90 - 5.5225\\
& = 9.15 - 5.225\\
& = 3.6275 
\end{align*}
and so the standard deviation \(\sigma = \sqrt{3.6275} \approx 1.90\). Notice that 4 times this value encompasses almost all of the range of the distribution.%
\par
\hypertarget{p-736}{}%
For the skewness%
\begin{align*}
\text{Numerator = } & E[X^3] - 3 \mu E[X^2] + 2\mu^3\\
& = \left [ 0^3 \cdot 0.10 + 1^3 \cdot 0.25 + 2^3 \cdot 0.40 + 4^3 \cdot 0.15 + 7^3 \cdot 0.10 \right ] - 3 \cdot 2.35 \cdot 9.15 + 2 \cdot 2.35^3\\
& \approx 0 + 0.25 + 3.20 + 9.60 + 34.3 - 64.5075 + 25.96\\
& = 47.35 - 64.5075 + 25.96\\
& \approx 8.80
\end{align*}
which yields a skewness of \(\gamma_1 = 8.80 / \sigma^3 \approx 1.27 \). This indicates a slight skewness to the right of the mean. You can notice the 4 and 7 entries on the histogram illustrate a slight trailing off to the right.%
\par
\hypertarget{p-737}{}%
Finally, for kurtosis%
\begin{align*}
\text{Numerator = } & E[X^4] - 4 \mu E[X^3] + 6 \mu^2 E[X^2] - 3\mu^4\\
& = \left [ 0^4 \cdot 0.10 + 1^4 \cdot 0.25 + 2^4 \cdot 0.40 + 4^4 \cdot 0.15 + 7^4 \cdot 0.10 \right ] - 4 \cdot 2.35 \cdot 47.35 + 6 \cdot 2.35^2 \cdot 9.15^2 - 3 \cdot 2.35^4\\
& \approx 0 + 0.25 + 6.40 + 38.4 + 240.1 - 445.09 + 303.19 - 91.49\\
& \approx 285.15 - 445.09 + 303.19 - 91.49\\
& \approx 51.75
\end{align*}
which yields a kurtosis of \(\gamma_2 = 51.75 / \sigma^4 \approx 3.93\) which also notes that the data appears to have a modestly bell-shaped distribution.%
\par
\hypertarget{p-738}{}%
Consider the following example when computing these statistics for a continuous variable. Let \(f(x) = \frac{3}{4} \cdot (1-x^2)\) over R = [-1,1]. \begin{figure}\centering\includegraphics[width=1\linewidth]{images/ContinuousDistributionExample.png}
\end{figure}%
\par
\hypertarget{p-739}{}%
Then for the mean%
\begin{align*}
\mu & = \int_{-1}^1 x \cdot \frac{3}{4} \cdot (1-x^2) dx\\
& = \int_{-1}^1 \frac{3}{4} \cdot (x-x^3) dx\\
& = \frac{3}{4} \cdot (x^2/2-x^4/4) \big |_{-1}^1\\
& = \frac{3}{4} \cdot [(1/2)-(1/4)] - [(1/2) - (1/4)]\\
& = 0
\end{align*}
as expected since the probability function is symmetric about x=0.%
\par
\hypertarget{p-740}{}%
For the variance%
\begin{align*}
\sigma^2 & = \int_{-1}^1 x^2 \cdot \frac{3}{4} \cdot (1-x^2) dx - \mu^2\\
& = \int_{-1}^1 \cdot \frac{3}{4} \cdot (x^2-x^4) dx - 0\\
& = \frac{3}{4} \cdot (x^3 /3 -x^5 / 5) \big |_{-1}^1\\
& = \frac{3}{4} \cdot 2 \cdot (1/3-1/5)\\
& = \frac{3}{4} \cdot \frac{4}{15}\\
& = \frac{1}{5}
\end{align*}
and taking the square root gives a standard deviation slightly less than 1/2. Notice that four times this value encompasses almost all of the range of the distribution.%
\par
\hypertarget{p-741}{}%
For the skewness, notice that the graph is symmetrical about the mean and so we would expect a skewness of 0.  Just to check it out%
\begin{align*}
\text{Numerator = } & E[X^3] - 3 \mu E[X^2] + 2\mu^3\\
& = \int_{-1}^1 x^3 \cdot \frac{3}{4} \cdot (1-x^2) dx - 3 E[X^2] \cdot 0 + 0^3 \\
& = \int_{-1}^1 \cdot \frac{3}{4} \cdot (x^3-x^5) dx\\
& = \frac{3}{4} \cdot (x^4/4-x^6/6) \big |_{-1}^1\\
& = 0
\end{align*}
as expected without having to actually complete the calculation by dividing by the cube of the standard deviation.%
\par
\hypertarget{p-742}{}%
Finally, note that the probability function in this case is modestly close to a bell shaped curve so we would expect a kurtosis in the vicinity of 3. Indeed, noting that (conveniently) \(\mu = 0\) gives%
\begin{align*}
\text{Numerator = } & E[X^4] - 4 \mu E[X^3] + 6 \mu^2 E[X^2] - 3 \mu^4\\
& = \int_{-1}^1 x^4 \cdot \frac{3}{4} \cdot (1-x^2) dx\\
& = \frac{3}{4} \cdot (x^5 /5-x^7 /7) \big |_{-1}^1\\
& = \frac{3}{4} \cdot 2(1/5-1/7)\\
& = \frac{3}{35}
\end{align*}
and so by dividing by \(\sigma^4 = \sqrt{\frac{1}{5}}^4 = \frac{1}{25}\) gives a kurtosis of%
\begin{equation*}
\gamma_2 = \frac{3}{35} / \frac{1}{25} = \frac{75}{35} \approx 2.14.
\end{equation*}
%
\par
\hypertarget{p-743}{}%
\begin{example}{}{example-29}%
\hypertarget{p-744}{}%
Consider \hyperref[ContinuousEV-example]{our previous example~\ref{ContinuousEV-example}}. To compute the mean and standard deviation for this distribution,%
\begin{equation*}
\mu = \int_{-1}^2 x \cdot x^2/3 dx = \int_{-1}^2 x^3/3 dx = \frac{2^4}{12} - \frac{(-1)^4}{12} = \frac{15}{12} = \frac{5}{4}
\end{equation*}
and by using \hyperref[TheoreticalMeasuresAlternates]{the alternate forumulas~\ref{TheoreticalMeasuresAlternates}}%
\begin{align*}
\sigma^2 & = E[X^2] - \mu^2\\
& = \int_{-1}^2 x^2 \cdot x^2/3 dx - \mu^2\\
& = \int_{-1}^2 x^4/3 dx - \left ( \frac{5}{4} \right\\
& = \frac{2^5}{15} - \frac{(-1)^5}{15} - \frac{25}{16}\\
& = \frac{33}{15} - \frac{25}{16} = \frac{51}{80}
\end{align*}
which gives%
\begin{equation*}
\sigma = \sqrt{\frac{51}{80}} \approx 0.7984.
\end{equation*}
%
\par
\hypertarget{p-745}{}%
For skewness, note that in computing the variance above you also found that%
\begin{equation*}
E[X^2] = \frac{11}{5}.
\end{equation*}
So, once again by using \hyperref[TheoreticalMeasuresAlternates]{the alternate forumulas~\ref{TheoreticalMeasuresAlternates}}%
\begin{equation*}
E[X^3] = \int_{-1}^2 x^3 \cdot x^2/3 dx = \frac{x^6}{18} |_{-1}^2 = \frac{7}{2}
\end{equation*}
and so%
\begin{equation*}
\gamma_1 = \frac{\frac{7}{2} - 3 \cdot \frac{5}{4} \cdot \frac{11}{5} + 2 \cdot (\frac{5}{4})^3}{\sqrt{\frac{51}{80}}^3}
\end{equation*}
%
\par
\hypertarget{p-746}{}%
For kurtosis, you can reuse \(E[X^3] = \frac{7}{2}\) and \(E[X^2] = \frac{11}{5}\) and \hyperref[TheoreticalMeasuresAlternates]{the alternate forumulas~\ref{TheoreticalMeasuresAlternates}} to determine%
\begin{equation*}
E[(X-\mu)^4] = E[X^4] - 4 \mu \cdot E[X^3] + 6 \mu^2 \cdot E[X^2] - 3 \mu^4
\end{equation*}
which is the numerator for the kurtosis.%
\end{example}
%
\par
\hypertarget{p-747}{}%
\begin{example}{Roulette.}{example-30}%
\hypertarget{p-748}{}%
Roulette is a gambling game popular in may casinos in which a player attempts to win money from the casino by predicting the location that a ball lands on in a spinning wheel.  There are two variations of this game...the American version and the European version. The difference being that the American version has one additional numbered slot on the wheel. The American version of the game will be used for the purposes of this example.%
\par
\hypertarget{p-749}{}%
A Roulette wheel consists of 38 equally-sized sectors identified with the numbers 1 through 36 plus 0 and 00. The 0 and 00 sectors are colored green and half of the remaining numbers are in sectors colored red with the remainder colored black.  A steel ball is dropped onto a spinning wheel and as the wheel comes to rest the sector in which it comes to rest is noted.  It is easy to determine that the probability of landing on any one of the 38 sectors is 1/38. A picture of a typical American-style wheel and betting board is given by \begin{figure}\centering\includegraphics[width=1\linewidth]{images/6-1-American-Roulette-1024x463.png}
\end{figure}. (Found at BigFishGames.com.)%
\par
\hypertarget{p-750}{}%
Since this is a game in a casino, there must be the opportunity to bet (and likely lose) money. For the remainder of this example we will assume that you are betting 1 dollar each time. If you were to bet more then the values would scale correspondingly. However, if you place your bet on any single number and the ball ends up on the sector corresponding to that number, you win a net of 35 dollars.  If the ball lands elsewhere you lose your dollar. Therefore the expected value of winning if you bet on one number is%
\begin{equation*}
E[\text{win on one}] = 35 \cdot \frac{1}{38} - 1 \cdot \frac{37}{38} = - \frac{2}{38}
\end{equation*}
which is a little more than a nickel loss on average.%
\par
\hypertarget{p-751}{}%
You can bet on two numbers as well and if the ball lands on either of the two then you win a payout in this case of 17 dollars.  Therefore the expected value of winning if you bet on two numbers is%
\begin{equation*}
E[\text{win on two numbers}] = 17 \cdot \frac{2}{38} - 1 \cdot \frac{36}{38} = - \frac{2}{38}.
\end{equation*}
%
\par
\hypertarget{p-752}{}%
Continuing, you can bet on three numbers and if the ball lands on any of the three then you win a payout of 11 dollars.  Therefore the expected value of winning if you bet on three numbers is%
\begin{equation*}
E[\text{win on three numbers}] = 11 \cdot \frac{3}{38} - 1 \cdot \frac{35}{38} = - \frac{2}{38}.
\end{equation*}
%
\par
\hypertarget{p-753}{}%
You can bet on all reds, all blacks, all evens (ignoring 0 and 00), or all odds and get your dollar back. The expected value for any of these options is%
\begin{equation*}
E[\text{win on eighteen numbers}] = 1 \cdot \frac{18}{38} - 1 \cdot \frac{20}{38} = - \frac{2}{38}.
\end{equation*}
%
\par
\hypertarget{p-754}{}%
There is one special way to bet which uses the the 5 numbers {0, 00, 1, 2, 3} and pays 6 dollars. This is called the "top line of basket".  Notice that the use of five numbers will make getting the same expected value as the other cases impossible using regular dollars and cents. The expected value of winning in this case us%
\begin{equation*}
E[\text{win on top line of basket}] = 6 \cdot \frac{5}{38} - 1 \cdot \frac{33}{38} = - \frac{3}{38}
\end{equation*}
which is of course worse and is the only normal way to bet on roulette which has a different expected value.%
\par
\hypertarget{p-755}{}%
There are other possible ways to bet on roulette but none provide a better expected value of winning.  The moral of this story is that you should never bet on the 5 number option and if you ever get ahead by winning on roulette using any of the possible options then you should probably stop quickly since over a long period of time it is expected that you will lose an average of \(\frac{1}{19}\) dollars per game.%
\end{example}
%
\par
\hypertarget{p-756}{}%
Going back to Pascal's wager, let \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}X = 0 represent disbelief when God doesn't exist%
\item{}X = 1 represent disbelief when God does exist%
\item{}X = 2 represent belief when God does exist%
\item{}X = 3 represent belief when God does not exist%
\end{itemize}
 Presume that p is the likelihood that God exists. Then you can compute the expected value of disbelief and the expect value of belief by first creating a value function. Below, for argument sake we are somewhat randomly assign a value of one million to disbelief if God doesn't exist. The conclusions are the same if you choose any other finite number...%
\begin{gather*}
v(0) = 1,000,000, f(0) = 1-p\\
v(1) = -\infty, f(1) = p\\
v(2) = \infty, f(2) = p\\
v(3) = 0, f(3) = 1-p
\end{gather*}
Then,%
\begin{align*}
E[\text{disbelief}] & = v(0)f(0) + v(1)f(1)\\
& = 1000000 \times (1-p) - \infty \times p\\
& = -\infty
\end{align*}
if p>0. On the other hand,%
\begin{align*}
E[\text{belief}] & = v(2)f(2) + v(3)f(3)\\
& = \infty \times p + 0 \times (1-p)\\
& = \infty
\end{align*}
if p>0. So Pascal's conclusion is that if there is even the slightest chance that God exists then belief is the smart and scientific choice.%
%
%
\typeout{************************************************}
\typeout{Section 5.5 Standard Units}
\typeout{************************************************}
%
\section[{Standard Units}]{Standard Units}\label{section-36}
\hypertarget{p-757}{}%
Any distribution variable can be converted to “standard units” using the linear translation%
\begin{equation*}
\displaystyle z = \frac{x-\mu}{\sigma}.
\end{equation*}
In doing so, values of z will always represent the number of standard deviations x is from the mean and will provide “dimensionless” comparisons.%
\par
\hypertarget{p-758}{}%
\begin{example}{}{example-31}%
\hypertarget{p-759}{}%
Consider our earlier \hyperref[ContinuousEV-example]{continuous example~\ref{ContinuousEV-example}} in which we found \(\mu = \frac{5}{4}\) and \(\sigma = \sqrt{\frac{51}{80}}\).  Then,%
\begin{equation*}
P(0 < X < 1) = P \left ( \frac{0-\frac{5}{4}}{\sqrt{\frac{51}{80}}} < \frac{X - \frac{5}{4}}{\sqrt{\frac{51}{80}}} < \frac{1-\frac{5}{4}}{\sqrt{\frac{51}{80}}} \right )
\end{equation*}
gives the middle term is Z and the other endpoints are now in standard units that indicate the number of standard deviations from the mean rather than actual problem units.%
\end{example}
%
%
%
\typeout{************************************************}
\typeout{Section 5.6 Summary}
\typeout{************************************************}
%
\section[{Summary}]{Summary}\label{section-37}
\hypertarget{p-760}{}%
TBA%
%
%
\typeout{************************************************}
\typeout{Section 5.7 Exercises}
\typeout{************************************************}
%
\section[{Exercises}]{Exercises}\label{section-38}
\begin{inlineexercise}{Flipping A Fixed Number of Coins.}{exercise-47}%
\hypertarget{p-761}{}%
Consider the random variable from the previous section where you flip three coins and measure the number of heads obtained. Determine f(0), f(1), f(2), and f(3) and the corresponding distribution function F(x). These can be expressed in a table format. Generalize your answer to the case when you flip a n coins where n is a fixed natural number.%
\end{inlineexercise}
\begin{inlineexercise}{Later.}{exercise-48}%
\hypertarget{p-762}{}%
B.%
\end{inlineexercise}
\begin{inlineexercise}{Flipping Fixed Number of Coins.}{exercise-49}%
\hypertarget{p-763}{}%
You flip three coins and measure the number of heads obtained. Determine the space R for the corresponding random variable X. From the eight possible outcomes, determine all outcomes corresponding to X=2. Identify the random variable as discrete or continuous.%
\end{inlineexercise}
\begin{inlineexercise}{Flipping Coins till success.}{exercise-50}%
\hypertarget{p-764}{}%
You flip one coin repeatedly until you get a second head. Determine the space R for the corresponding random variable X. From the possibilities, determine all outcomes corresponding to X=4. Identify the random variable as discrete or continuous.%
\end{inlineexercise}
\begin{inlineexercise}{Time between Accidents.}{exercise-51}%
\hypertarget{p-765}{}%
Now you want to measure the time between accidents at a particular intersection in town. Determine the space R for the corresponding random variable X. Describe all outcomes corresponding to \(X \lt 1\). Be purposeful in the problem to describe the units you are using to measure time. Identify the random variable as discrete or continuous.%
\end{inlineexercise}
%
%
\typeout{************************************************}
\typeout{Chapter 6 Distributions based upon Equally likely Outcomes}
\typeout{************************************************}
%
\chapter[{Distributions based upon Equally likely Outcomes}]{Distributions based upon Equally likely Outcomes}\label{UniformHypergeometric}
%
%
\typeout{************************************************}
\typeout{Section 6.1 Introduction}
\typeout{************************************************}
%
\section[{Introduction}]{Introduction}\label{section-39}
\hypertarget{p-766}{}%
When motivating our definition of probability you may have noticed that we modeled our definition on the relative frequency of equally-likely outcomes. In this chapter you will develop the theoretical formulas which can be used to model equally-likely outcomes.%
\par
\hypertarget{p-767}{}%
In this chapter, you will investigate the following distributions: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-205}{}Discrete Uniform - each of a finite collection of outcomes is equally likely and prescribed a "position" and X measures the position of an item selected randomly from the outcomes.%
\item\hypertarget{li-206}{}Continuous Uniform - an interval of values is possible with sub-intervals of equal length having equal probabilities and X measures a location inside that interval.%
\item\hypertarget{li-207}{}Hypergeometric - each of a finite collection of values are equally likely and grouped into two classes (successes vs failures) and a subset of that collection is extracted with X measuring the number of successes in the sample.%
\end{enumerate}
%
%
%
\typeout{************************************************}
\typeout{Section 6.2 Discrete Uniform Distribution}
\typeout{************************************************}
%
\section[{Discrete Uniform Distribution}]{Discrete Uniform Distribution}\label{section-40}
\hypertarget{p-768}{}%
In this section, you will investigate distributions that begin with individual outcomes that are equally likely and expand into more general settings.%
\par
\hypertarget{p-769}{}%
\begin{theorem}{Discrete Uniform Distribution.}{}{theorem-30}%
\hypertarget{DiscreteUniformFunction}{}%
Assume outcomes in R = {1, 2, 3, ..., n} are equally likely.  Then, the probability function for the discrete uniform variable X is%
\begin{equation*}
f(x) = \frac{1}{n}
\end{equation*}
for \(x \in R\).%
\end{theorem}
\begin{proof}\hypertarget{proof-33}{}
\hypertarget{p-771}{}%
Assume that you have a variable with space R = {1, 2, 3, ..., n} so that the likelihood of each value is equally likely. Then, the probability function satisfies \(f(x) = c\) for any \(x \in R\).  As before, since \(\sum_{x \in R} f(x) = 1\), then%
\begin{equation*}
f(x) = \frac{1}{n}.\qedhere
\end{equation*}
%
\end{proof}
%
\par
\hypertarget{p-772}{}%
\leavevmode%
\begin{sageinput}
# Uniform distribution over 1 .. n
pretty_print("Discrete Uniform Distribution over the set 1, 2, ..., n")
var('x')
@interact
def _(n=slider(2,10,1,2)):
    np1 = n+1
    R = range(1,np1)
    f(x) = 1/n
    pretty_print(html('Density Function: $f(x) =%s$'%str(latex(f(x)))+' over the space $R = %s$'%str(R)))
    points((k,f(x=k)) for k in R).show()
    for k in R:
        pretty_print(html('$f(%s'%k+') = %s'%f(x=k)+' \\approx %s$'%f(x=k).n(digits=5)))
\end{sageinput}
%
\par
\hypertarget{p-773}{}%
\begin{theorem}{Properties of the Discrete Uniform Probability Function.}{}{theorem-31}%
\hypertarget{DiscreteUniformProperties}{}%
\(f(x) = \frac{1}{n}\) over R = {1, 2, 3, ..., n} satisfies the properties of a discrete probability function and \leavevmode%
\begin{enumerate}
\item\hypertarget{li-208}{}\(\mu = \frac{1+n}{2}\)%
\item\hypertarget{li-209}{}\(\sigma^2 = \frac{n^2-1}{12}\)%
\item\hypertarget{li-210}{}\(\gamma_1 = 0\)%
\item\hypertarget{li-211}{}\(\gamma_2 = \frac{6}{5}\frac{1+n^2}{1-n^2}\)%
\item\hypertarget{li-212}{}Distribution function F(x) = frac{x}{n} for \(x \in R\).%
\end{enumerate}
%
\end{theorem}
\begin{proof}\hypertarget{proof-34}{}
\hypertarget{p-775}{}%
Trivially, by construction you get%
\begin{equation*}
\sum_{k=1}^n \frac{1}{n} = 1
\end{equation*}
Also, 1/n is positive for all x values.%
\par
\hypertarget{p-776}{}%
To determine the mean,%
\begin{align*}
\mu & = \sum_{k=1}^n x \cdot \frac{1}{n}\\
& = \frac{1}{n}\sum_{k=1}^n x \\
& = \frac{1}{n} \frac{n(n+1)}{2}\\
& = \frac{1+n}{2}
\end{align*}
%
\par
\hypertarget{p-777}{}%
To determine the variance,%
\begin{align*}
\sigma^2 & = \sum_{k=1}^n x^2 \cdot \frac{1}{n} - \mu^2\\
& = \frac{1}{n}\sum_{k=1}^n x^2 - \left ( \frac{1+n}{2}\right )^2 \\
& = \frac{1}{n} \frac{n(n+1)(2n+1)}{6} - \frac{1+2n+n^2}{4}\\
& = \frac{(2n^2+3n+1)}{6} - \frac{1+2n+n^2}{4}\\
& = \frac{(4n^2+6n+2)}{12} - \frac{3+6n+3n^2}{12}\\
& = \frac{(n^2-1)}{12}
\end{align*}
%
\par
\hypertarget{p-778}{}%
For skewness,%
\begin{align*}
\gamma_1 = & \sum_{k=1}^n x^3 \cdot \frac{1}{n} - 3 \mu \sum_{k=1}^n x^2 \cdot \frac{1}{n}  + 2\mu^3\\
& = \frac{n^2(n+1)^2}{4n} - 3\frac{(n(n+1))}{2} \frac{1+n}{2} + 2 \left ( \frac{1+n}{2}\right )^3 \\
& = 
\end{align*}
%
\par
\hypertarget{p-779}{}%
For Kurtosis, use the fourth moment and simplify...the algebra is performed using Sage in the active cell below this proof.%
\end{proof}
%
\par
\hypertarget{p-780}{}%
Sage can also do the algebra for you to determine each of these measures. Notice, as n increases the Kurtosis approaches \(\frac{6}{5}\) which indicates that there is (obviously) no tend toward central tendency over time.%
\par
\hypertarget{p-781}{}%
\leavevmode%
\begin{sageinput}
var('x,n')
f = 1/n
mu = sum(x*f,x,1,n).factor()
pretty_print('Mean = ',mu)
mu = (1+n)/2
v = sum((x-mu)^2*f, x, 1, n)
pretty_print('Variance = ',v.factor())
stand = sqrt(v)
pretty_print('Skewness =  ',(sum((x-mu)^3*f, x, 1, n)/stand^3))
kurt = sum((x-mu)^4*f, x, 1, n)/stand^4
pretty_print('Kurtosis = ',(kurt-3).factor(),' + 3')
\end{sageinput}
%
\par
\hypertarget{p-782}{}%
\begin{example}{Rolling one die.}{example-32}%
\hypertarget{p-783}{}%
When you consider rolling a regular, fair, single 6-sided die, each side is equally likely. The sample space consists of the 6 sides, each with a unique number of physical dots. Let the random variable X correspond each side with the number corresponding to the number of dots. Then, R = {1, 2, 3, 4, 5, 6}.  Since each side is equally likely then f(x) = 1/6.%
\par
\hypertarget{p-784}{}%
Further, the probability of getting an outcome in A={2,3} would be f(2)+f(3) = 1/6 + 1/6 = 2/6.%
\end{example}
%
%
%
\typeout{************************************************}
\typeout{Section 6.3 Continuous Uniform Distribution}
\typeout{************************************************}
%
\section[{Continuous Uniform Distribution}]{Continuous Uniform Distribution}\label{section-41}
\hypertarget{p-785}{}%
Modeling the idea of "equally-likely" in a continuous world requires a slightly different perspective since there are obviously infinitely many outcomes to consider. Instead, you should consider requiring that intervals in the domain which are of equal width should have the same probability regardless of where they are in that domain. This behaviour suggests \(P(u \lt X \lt v) = P(u + w \lt X \lt v + w)\).  We use this below...%
\par
\hypertarget{p-786}{}%
\begin{theorem}{}{}{theorem-32}%
\hypertarget{ContinuousUniformFunction}{}%
For R = [a,b], with a < b, the continuous uniform probability function is given by%
\begin{equation*}
f(x) = \frac{1}{b-a}.
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-35}{}
\hypertarget{p-788}{}%
From before, for X a continuous uniform variable, we get%
\begin{gather*}
\int_u^v f(x) dx = \int_{u+w}^{v+w} f(x) dx\\
F(v)-F(u) = F(v+w)-F(u+w)\\
F(u+w)-F(u) = F(v+w)-F(v)
\end{gather*}
which is true regardless of w so long as you stay in the domain of interest. This only happens if F is linear and therefore f must be constant....say, f(x)=c. If the space of X is a single interval with \(R = [a,b]\) then%
\begin{equation*}
1 = \int_a^b c dx = c(b-a)
\end{equation*}
which yields c = \frac{1}{b-a} as desired.%
\end{proof}
%
\par
\hypertarget{p-789}{}%
\begin{example}{Basic Continuous Uniform.}{example-33}%
\hypertarget{p-790}{}%
On \(R = [1,2 \pi]\),%
\begin{equation*}
f(x) = \frac{1}{2 \pi - 1}.
\end{equation*}
Then, if you want to compute something like \(P(2 < X < 4.5)\) integrate%
\begin{equation*}
P(2 < X < 4.5) = \int_2^{4.5} \frac{1}{2 \pi -1} dx = \frac{2.5}{2 \pi - 1}
\end{equation*}
%
\end{example}
%
\par
\hypertarget{p-791}{}%
\begin{example}{Continuous Uniform over two disjoint intervals.}{example-34}%
\hypertarget{p-792}{}%
Suppose \(R = [0,2] \cup [5,7]\).  Then, as in the theorem proof%
\begin{equation*}
1 = \int_R c dx = \int_0^2 c dx + \int_5^7 c dx = 4.
\end{equation*}
Thus, \(f(x) = \frac{1}{4}\). For computing probabilities, you will want to break up any resulting integrals in a similar manner.%
\end{example}
%
\par
\hypertarget{p-793}{}%
\begin{theorem}{Properties of the Continuous Uniform Probability Function.}{}{theorem-33}%
\hypertarget{ContinuousUniformProperties}{}%
For the Continuous Uniform Distribution over \(R = [a,b]\), with a < b, \leavevmode%
\begin{enumerate}
\item\hypertarget{li-213}{}\(f(x) = \frac{1}{b-a}\) satisfies the properties of a probability function over R = [a,b].%
\item\hypertarget{li-214}{}\(\mu = \frac{a+b}{2}\)%
\item\hypertarget{li-215}{}\(\sigma^2 = \frac{b^2-a^2}{12}\)%
\item\hypertarget{li-216}{}\(\gamma_1 = 0\)%
\item\hypertarget{li-217}{}\(\gamma_2 = \frac{9 \, {\left(a^{5} - 5 \, a^{4} b + 10 \, a^{3} b^{2} - 10 \, a^{2} b^{3} + 5 \, a b^{4} - b^{5}\right)} {\left(a - b\right)}}{5 \, {\left(a^{3} - 3 \, a^{2} b + 3 \, a b^{2} - b^{3}\right)}^{2}}\)%
\end{enumerate}
%
\end{theorem}
%
\par
\hypertarget{p-795}{}%
\leavevmode%
\begin{sageinput}
# Continous uniform distribution statistics derivation
reset()
var('x,a,b')


f = 1/(b-a)

mu = integrate(x*f,x,a,b).factor()
pretty_print('Mean = ',mu)

v = integrate((x-mu)^2*f, x, a, b)

pretty_print('Variance = ',v.factor())
stand = sqrt(v)
sk = (integrate((x-mu)^3*f, x, a, b)/stand^3)
pretty_print('Skewness =  ',sk)
kurt = (integrate((x-mu)^4*f, x, a, b)/stand^4)
pretty_print('Kurtosis = ',kurt)

pretty_print('Several Examples')
a1=0
for b1 in range(2,7):
    pretty_print('Using [',a1,',',b1,']:')
    pretty_print('    mean = ',mu(a=a1,b=b1))
    pretty_print('variance = ',v(a=a1,b=b1))
    pretty_print('skewness = ',sk(a=a1,b=b1))
    pretty_print('kurtosis = ',kurt(a=a1,b=b1))
\end{sageinput}
%
\par
\hypertarget{p-796}{}%
\begin{example}{Occurence of exactly one event randomly in a given interval.}{example-35}%
\hypertarget{p-797}{}%
Suppose you know that only one person showed up at the counter of a local business in a given 30 minute interval of time. Then, R=[0,30] given f(x) = 1/30.%
\par
\hypertarget{p-798}{}%
Further, the probability that the person arrived within the first 6 minutes would be \(\int_0^6 \frac{1}{30} dx = 0.2\).%
\end{example}
%
\par
\hypertarget{p-799}{}%
\begin{theorem}{Distribution Function for Continuous Uniform.}{}{theorem-34}%
\hypertarget{p-800}{}%
For \(x \in [a,b], F(x) = \frac{x-a}{b-a}\)%
\end{theorem}
\begin{proof}\hypertarget{proof-36}{}
\hypertarget{p-801}{}%
For x in this range,%
\begin{equation*}
F(x) = \int_a^x \frac{1}{b-a} du = \frac{u}{b-a} \big |_a^x = \frac{x-a}{b-a}.\qedhere
\end{equation*}
%
\end{proof}
%
%
%
\typeout{************************************************}
\typeout{Section 6.4 Hypergeometric Distribution}
\typeout{************************************************}
%
\section[{Hypergeometric Distribution}]{Hypergeometric Distribution}\label{section-42}
\hypertarget{p-802}{}%
For the discrete uniform distribution, the presumption is that you will be making a selection one time from the collection of items. However, if you want to take a larger sample without replacement from a distribution in which originally all are equally likely then you will end up with something which will not be uniform.%
\par
\hypertarget{p-803}{}%
Indeed, consider a collection of n items from which you want to take a sample of size r without replacement. If \(n_1\) of the items are "desired" and the remainder are not, let the random variable X measure the number of items from the first group in your sample with \(R = \{0, 1, ..., r \}\). The resulting collection of probabilities is called a Hypergeometric Distribution.%
\par
\hypertarget{p-804}{}%
\begin{theorem}{Hypergeometric Probability Function.}{}{theorem-35}%
\hypertarget{HypergeometricFunction}{}%
For a Hypergeometric random variable with R = {0, 1, ..., r} and assuming \(n_1 \ge r\) and \(n-n_1 \ge r\),%
\begin{equation*}
f(x) = \frac{\binom{n_1}{x} \binom{n-n_1}{r-x}}{\binom{n}{r}}
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-37}{}
\hypertarget{p-806}{}%
Since you are sampling without replacement and trying only measure the number of items from your desired group in the sample, then the space of X will include R = {0, 1, ..., r} assuming \(n_1 \ge r\) and \(n-n_1 \ge r\). In the case when r is too large for either of these, the formulas below will follow noting that binomial coefficients are zero if the top is smaller than the bottom or if the bottom is negative.%
\par
\hypertarget{p-807}{}%
So f(x) = P(X = x) = P(x from the sample are from the target group and the remainder are not). Breaking these up gives%
\begin{equation*}
f(x) = \frac{\binom{n_1}{x} \binom{n-n_1}{r-x}}{\binom{n}{r}}\qedhere
\end{equation*}
%
\end{proof}
%
\par
\hypertarget{p-808}{}%
\begin{theorem}{Properties of the Hypergeometric Distribution.}{}{theorem-36}%
\hypertarget{HypergeometricProperties}{}%
\leavevmode%
\begin{enumerate}
\item\hypertarget{li-218}{}\(f(x) = \frac{\binom{n_1}{x} \binom{n-n_1}{r-x}}{\binom{n}{r}}\) satisfies the properties of a probability function.%
\item\hypertarget{li-219}{}\(\mu = r \frac{n_1}{n}\)%
\item\hypertarget{li-220}{}\(\sigma^2 = r \frac{n_1}{n} \frac{n_2}{n} \frac{n-r}{n-1}\)%
\item\hypertarget{li-221}{}\(\gamma_1 = \frac{(n - 2 n_1)\sqrt{n-1}(n - 2r)}{r n_1 (n - n_1) \sqrt{n-r}(n-2)}\)%
\item\hypertarget{li-222}{}\(\gamma_2 = \frac{n(n+1)-6n(n-r)}{n_1(n-n_1)} + \frac{3r(n-r)(n+6)}{n^2} - 6\)%
\end{enumerate}
%
\end{theorem}
\begin{proof}\hypertarget{proof-38}{}
\hypertarget{p-810}{}%
\leavevmode%
\begin{enumerate}
\item\hypertarget{li-223}{}\hypertarget{p-811}{}%
%
\begin{align*}
\sum_{x=0}^n \binom{n}{x} y^x & = (1+y)^n, \text{ by the Binomial Theorem}\\
& = (1+y)^{n_1} \cdot (1+y)^{n_2} \\
& = \sum_{x=0}^{n_1} \binom{n_1}{x} y^x \cdot \sum_{x=0}^{n_2} \binom{n_2}{x} y^x \\
& = \sum_{x=0}^n \sum_{t=0}^r \binom{n_1}{r} \binom{n_2}{r-t} y^x
\end{align*}
Equating like coefficients for the various powers of y gives%
\begin{equation*}
\binom{n}{r} = \sum_{t=0}^r \binom{n_1}{r} \binom{n_2}{r-t}.
\end{equation*}
Dividing gives%
\begin{equation*}
1 = \sum_{x=0}^r f(x).
\end{equation*}
%
\item\hypertarget{li-224}{}\hypertarget{p-812}{}%
For the mean%
\begin{align*}
\sum_{x=0}^n x \frac{\binom{n_1}{x} \binom{n-n_1}{r-x}}{\binom{n}{r}} & = 
\frac{1}{\binom{n}{r}} \sum_{x=1}^n  \frac{n_1(n_1-1)!}{(x-1)!(n_1-x)!}  \binom{n-n_1}{r-x}				\\
& = \frac{n_1}{\binom{n}{r}} \sum_{x=1}^n  \frac{(n_1-1)!}{(x-1)!((n_1-1)-(x-1))!}  \binom{n-n_1}{r-x} \\
& = \frac{n_1}{\frac{n(n-1)!}{r!(n-r)!}} \sum_{x=1}^n  \binom{n_1-1}{x-1}  \binom{n-n_1}{r-x} 
\end{align*}
Consider the following change of variables for the summation:%
\begin{gather*}
y = x-1\\
n_3 = n_1-1\\
s = r-1\\
m = n-1
\end{gather*}
Then, this becomes%
\begin{align*}
\mu = \sum_{x=0}^n x \frac{\binom{n_1}{x} \binom{n-n_1}{r-x}}{\binom{n}{r}} & = r \frac{n_1}{n} \sum_{y=0}^m  \frac{\binom{n_3}{y} \binom{m-n_3}{s-y}}{\binom{m}{s}}\\
& = r \frac{n_1}{n} \cdot 1
\end{align*}
noting that the summation is in the same form as was show yields 1 above.%
\item\hypertarget{li-225}{}\hypertarget{p-813}{}%
The proof of the variance formula is similar and uses E(X(X-1)+ μ - μ2. The proof of skewness and kurtosis are messy and we won’t bother with them for this distribution!%
\end{enumerate}
%
\end{proof}
%
\par
\hypertarget{p-814}{}%
Note, if r=1 then you are back at a regular discrete uniform model. Indeed,%
\begin{equation*}
P(\text{desired item}) = 1 \cdot \frac{n_1}{n} = \mu .
\end{equation*}
which is indeed what you might expect when selecting once.%
\par
\hypertarget{p-815}{}%
Consider the Hypergeometric distribution for various values of \(n_1, n_2,\) and r using the interactive cell below. Notice what happens when you start with relatively small values of \(n_1, n_2,\) and r (say, start with \(n_1 = 5, n_2 = 8,\) and r = 4 and then doubling then all again and again. Consider the likely skewness and kurtosis of the graph as the values get larger.%
\begin{sageinput}
# Hypergeometric distribution over 0 .. N
# Size of classes N1 and N2 must be given as well as subset size r
var('x')
@interact
def _(N1=slider(1,40,1,10,label='$N_1$'),
    N2=slider(1,40,1,10,label='$N_2$'),
    r=slider(1,40,1,10,label='$r$')):
    N = N1 + N1
    R = range(r+1)
    if (r > N1)|(r > N2):
        pretty_print('When r is bigger than N1 or N2, special consideration must be made')
    else:
        f(x) = binomial(N1,x)*binomial(N2,r-x)/binomial(N,r)
        pretty_print(html('Density Function: $f(x) =%s$'%str(latex(f(x)))))
        pretty_print(html('over the space $R = %s$'%str(R)))
        points((k,f(x=k)) for k in R).show()
        for k in R:
           print (html('$f(%s'%k+') = %s'%latex(f(x=k))+' \\approx %s$'%f(x=k).n(digits=5)))
\end{sageinput}
%
%
\typeout{************************************************}
\typeout{Section 6.5 Summary}
\typeout{************************************************}
%
\section[{Summary}]{Summary}\label{section-43}
\hypertarget{p-816}{}%
\hypertarget{p-817}{}%
Here is a summary of the major formulas from this chapter:%
%
\par
\hypertarget{p-818}{}%
\hyperlink{DiscreteUniformFunction}{Discrete Uniform f(x)~}%
\par
\hypertarget{p-819}{}%
\hyperlink{DiscreteUniformProperties}{Discrete Uniform statistics~}%
\par
\hypertarget{p-820}{}%
\hyperlink{ContinuousUniformFunction}{Continuous Uniform f(x)~}%
\par
\hypertarget{p-821}{}%
\hyperlink{ContinuousUniformProperties}{Continuous Uniform statistics~}%
\par
\hypertarget{p-822}{}%
\hyperlink{HypergeometricFunction}{Hypergeometric f(x)~}%
\par
\hypertarget{p-823}{}%
\hyperlink{HypergeometricProperties}{Hypergeometric statistics~}%
%
%
\typeout{************************************************}
\typeout{Section 6.6 Exercises}
\typeout{************************************************}
%
\section[{Exercises}]{Exercises}\label{section-44}
\hypertarget{p-824}{}%
\begin{inlineexercise}{- The Proverbial Urn Problem.}{exercise-52}%
\hypertarget{p-825}{}%
You have an urn with 10 marbles of which \(n_1 = 6\) are red and \(N_2 = 4\) are blue. You select randomly r = 3 of the marbles without replacement and let X represent the number of red marbles in your sample. With R = {0, 1, 2, 3}, determine: \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}f(x)%
\item{}P(2 of the 3 are red) = f(2)%
\item{}P(at most 2 of the 3 are red) = f(0) + f(1) + f(2)%
\end{itemize}
%
\end{inlineexercise}
%
\par
\hypertarget{p-826}{}%
\begin{inlineexercise}{- Playing Cards.}{exercise-53}%
\hypertarget{p-827}{}%
You randomly select a hand of five cards without replacement from an ordinary deck of playing cards. \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}Determine the probability that four of the five are spades.%
\item{}Determine the probability that three of the five are face cards (ie, Jacks, Queens, Kings, or Aces).%
\end{itemize}
%
\end{inlineexercise}
%
\par
\hypertarget{p-828}{}%
\begin{inlineexercise}{- Starting Seniors.}{exercise-54}%
\hypertarget{p-829}{}%
You are picking an eleven member football starting team by picking randomly from a group with 15 seniors and 35 others.  Using a hypergeometric distribution, determine: \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}P(all seniors)%
\item{}P(exactly 6 seniors)%
\item{}the expected number of seniors on the team%
\item{}If your team has all seniors, explain whether someone could suggest that your decision on members was unfair%
\end{itemize}
%
\end{inlineexercise}
%
\par
\hypertarget{p-830}{}%
\begin{inlineexercise}{- Old Faithful.}{exercise-55}%
\hypertarget{p-831}{}%
Ole Faithful geyser in Yellowstone National Park erupts every 91 minutes. You show up at some random time in the eruption cycle and your tour bus plans to stay for 25 minutes. Determine the likelihood that you will be able to see it erupt.  Express your answer in terms of the continuous uniform distribution by giving f(x), F(x), and the specific answer to this question.%
\end{inlineexercise}
%
\par
\hypertarget{p-832}{}%
\begin{inlineexercise}{- Continuous Uniform Random Variable Scenarios.}{exercise-56}%
\hypertarget{p-833}{}%
Explain how the following situations can be modeled using a continuous uniform distribution by identifying the space R and the corresponding f(x) for each situation. \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}The location on a prize wheel where the spun wheel will stop.%
\item{}Given a clock with only a minute hand, the current one second interval.%
\item{}The location on a automobile tire where the next puncture will occur.%
\end{itemize}
%
\end{inlineexercise}
%
\par
\hypertarget{p-834}{}%
\begin{inlineexercise}{- Continous Uniform on a different space.}{exercise-57}%
\hypertarget{p-835}{}%
Determine an explicit formula for f(x) and the resulting mean and variance for a continuous uniform distribution over \(R = [-2,3] \cup [5,6] \cup [9,15]\).%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-23}{}\quad%
\hypertarget{p-836}{}%
Since you must have%
\begin{equation*}
\int_{x \in R} f(x) dx = 1
\end{equation*}
and since f(x) must be constant than all you must do is measure the accumulated width of the intervals in R. This is 5 + 1 + 6 = 12 and so%
\begin{equation*}
f(x)=\left\{\begin{matrix}
\frac{1}{12}, & -2 \le x \le 3
\\ \frac{1}{12}, & 5 \le x \le 6
\\ \frac{1}{12}, & 9 \le x \le 15
\\ 0, & \text{otherwise}
\end{matrix}\right.
\end{equation*}
For the mean,%
\begin{align*}
\int_{x \in R} x \frac{1}{12} dx & = \int_{-2}^3 \frac{x}{12} dx + \int_5^6 \frac{x}{12} dx + \int_9^{15} \frac{x}{12} dx\\
& = \frac{9-4}{24} + \frac{36-25}{24} + \frac{225-81}{24}\\
& = \frac{5+11+144}{24} = \frac{160}{24} = \frac{20}{3}.
\end{align*}
For the variance,%
\begin{align*}
\int_{x \in R} x^2 \frac{1}{12} dx - \mu^2 & = \int_{-2}^3 \frac{x^2}{12} dx + \int_5^6 \frac{x^2}{12} dx + \int_9^{15} \frac{x^2}{12} dx - \mu^2\\
& = \frac{81+8}{36} + \frac{216-125}{36} + \frac{3375-729}{36} - \big ( \frac{20}{3} \big )^2\\
& = \frac{89+91+2646}{36} - \frac{400}{9} = \frac{2826-1600}{36} \\
& = \frac{1226}{36} \approx	34.055.
\end{align*}
%
\end{inlineexercise}
%
\par
\hypertarget{p-837}{}%
\begin{inlineexercise}{- Louisiana Mega Millions Lottery.}{exercise-58}%
\hypertarget{p-838}{}%
To play the Mega Millions Louisiana Lottery consists of picking five numbers from 1 to 75 and one yellow Mega Ball number from 1 through 15. (You can play up to five different sets of numbers on each playslip but we will just assume one play per ticket to keep things straight.) Each play costs $1 and you can pay an additional $1 to apply a "multiplier" which multiplies any non-Jackpot prize by the Multiplier number (2, 3, 4, or 5) randomly selected at the time of the drawing.%
\par
\hypertarget{p-839}{}%
On October 10, 2016 the jackpots listed were \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}Match 5 plus Mega ball = Jackpot of $49,000,000 with cash value of $32,600,000%
\item{}Match only 5 = $1,000,000%
\item{}Match 4 plus Mega ball = $5,000%
\item{}Match only 4 =$500%
\item{}Match 3 plus Mega ball = $50%
\item{}Match only 3 = $5%
\item{}Match 2 plus Mega ball = $5%
\item{}Match 1 plus Mega ball = $2%
\item{}Match only the Mega ball = $1%
\end{itemize}
%
\par
\hypertarget{p-840}{}%
Verify the posted odds \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}Match 5 plus Mega ball = 1 in 258,890,850%
\item{}Match only 5 = 1 in 18,492,204%
\item{}Match 4 plus Mega ball = 1 in 739,688%
\item{}Match only 4 = 1 in 52,835%
\item{}Match 3 plus Mega ball = 1 in 10,720%
\item{}Match only 3 = 1 in 766%
\item{}Match 2 plus Mega ball = 1 in 473%
\item{}Match 1 plus Mega ball = 1 in 56%
\item{}Match only the Mega ball = 1 in 21%
\end{itemize}
%
\par
\hypertarget{p-841}{}%
Determine the expected payout for each ticket purchased. Also, determine what the Jackpot would need to be in order for the game to be considered "fair" with an expected value of zero.%
\par\smallskip%
\noindent\textbf{Solution}.\hypertarget{solution-24}{}\quad%
\hypertarget{p-842}{}%
Throughout these calculations, you can presume that the first five numbers are selected independently from the Mega Ball number. However, the first five numbers are selected without replacement so computing probabilities with those does not allow for independence. This part is hypergeometric with the \(n_1 = 5\) numbers you selected being the "desired" numbers and the Lottery Commission picking a subset of size r = 5 from the 75 possible numbers.  So, your likelihood of matching all five would be%
\begin{equation*}
\frac{\binom{5}{5} \cdot \binom{70}{0}}{\binom{75}{5}} = \frac{1}{17259390}.
\end{equation*}
Multiplying this by the 1 chance in 15 that you also match the Mega Ball gives%
\begin{equation*}
P(\text{Match 5 plus Mega Ball}) = \frac{1}{17259390} \cdot \frac{1}{15} = \frac{1}{258,890,850}.
\end{equation*}
To match only 5 means you also MUST miss the Mega Ball which has probability 14/15 to give%
\begin{equation*}
\frac{1}{17259390} \cdot \frac{14}{15} = \frac{1}{17259390 \cdot \frac{15}{14}} \approx \frac{1}{18492204}.
\end{equation*}
Continue in this manner to determine the other odds.%
\par
\hypertarget{p-843}{}%
For the expected earnings, first determine a value function corresponding to each outcome and apply the discrete expected value process. This gives%
\begin{align*}
& \$32600000 \cdot \frac{1}{258,890,850} + \$1000000 \cdot \frac{1}{18,492,204} \\
& + \$5000 \cdot \frac{1}{739,688} + \$500 \cdot \frac{1}{52,835}\\
& + \$50 \cdot \frac{1}{10,720} + \$5 \cdot \frac{1}{766} \\
& + \$5 \cdot \frac{1}{473} + \$2 \cdot \frac{1}{56} + \$1 \cdot \frac{1}{21}\\
& \approx \$0.3013.
\end{align*}
So, the expected payout is approximately 30 cents. Subtracting the cost of playing ($1) indicates that the average winnings per play of the Louisiana Lottery would be -70 cents. So, you would be better off to take, say, 50 cents and just give it to the local school system every time you consider playing this game rather than actually playing.%
\par
\hypertarget{p-844}{}%
To determine the Jackpot A needed to make this a fair game means to solve the equation%
\begin{align*}
& A \cdot \frac{1}{258,890,850} + \$1000000 \cdot \frac{1}{18,492,204} \\
& + \$5000 \cdot \frac{1}{739,688} + \$500 \cdot \frac{1}{52,835}\\
& + \$50 \cdot \frac{1}{10,720} + \$5 \cdot \frac{1}{766} \\
& + \$5 \cdot \frac{1}{473} + \$2 \cdot \frac{1}{56} + \$1 \cdot \frac{1}{21}\\
& = 1
\end{align*}
for A.%
\par
\hypertarget{p-845}{}%
Finally, to deal with the multiplier, note that all but the Jackpot payouts would be increased by the multiplier m where \(m \in \{1,2,3,4,5\}\).  For the cost of an extra $1 (total cost of $2 per bet) the expected payout increases as the multiplier increases but each of these decreases likelihood of winning that payout by a factor of 1/5.  In general, let x = 1, 2, ..., 9 indicate the various winning options in order listed above, f(x) the corresponding probabilities listed for each option, and  u(x) the listed payouts. Then the expected payout is given by%
\begin{equation*}
\$32600000 \cdot \frac{1}{258,890,850} + \sum_{m=1}^5
\sum_{x=2}^9 m \cdot u(x) f(x)/5 
\end{equation*}
or%
\begin{align*}
\$32600000 \cdot \frac{1}{258,890,850} & + \sum_{m=1}^5 \frac{m}{5} \sum_{x=2}^9 u(x) f(x) \\
& = \frac{\$32600000}{258890850} + \sum_{m=1}^5 \frac{m}{5} 0.17539\\
& = 0.12592 + 3 \cdot 0.17539\\
& = 0.65209
\end{align*}
Therefore, the expect value of spending another dollar to get the multiplier effect is about -$1.35.  Since this is slightly less than doubling the expected loss of 70 cents for playing without the multiplier with $1, it make more sense to bet $2 once rather than betting $1 twice.  Or, you can send the extra nickel to this author of this text and call it quits.%
\end{inlineexercise}
%
%
%
\typeout{************************************************}
\typeout{Chapter 7 Distributions based upon Bernoulli Trials}
\typeout{************************************************}
%
\chapter[{Distributions based upon Bernoulli Trials}]{Distributions based upon Bernoulli Trials}\label{BinomialGeometricNegBinomial}
%
%
\typeout{************************************************}
\typeout{Section 7.1 Introduction}
\typeout{************************************************}
%
\section[{Introduction}]{Introduction}\label{section-45}
\hypertarget{p-846}{}%
Many practical problems involve measuring simply whether something was a success or a failure. In these situations, "success" should not be interpreted as having any moral or subjective meaning but only construed to mean that something you are looking for actually occurs.%
\par
\hypertarget{p-847}{}%
In situation where a single trial is performed and the result is determined only to be a success or failure is called a Bernoulli event. Indeed, one could create a corresponding probability function using a random variable X over the space \(R = \{0, 1 \}\) mapping X(success) = 1 and X(failure)=0. If p = P(success) then%
\begin{equation*}
f(x) = p^x \cdot (1-p)^{1-x}
\end{equation*}
would be a formula but which only related to two values P(failure) = f(0) = (1-p) and P(Success) = f(1) = p.%
\par
\hypertarget{p-848}{}%
Notice that p=0 means that you will always get a failure and that p=1 means that you will always get a success. In these cases, X would no longer be a random variable since the outcome for X could be predicted with certainty. Therefore, we will always assume that \(0 < p < 1\).%
\par
\hypertarget{p-849}{}%
The Bernoulli distribution on its own is not extremely useful but serves as a starting point for several others that are useful.  Indeed, in this chapter you will investigate distributions that relate some number of successes in multiple trials to some number of independent trials. The difference between these distributions will be that one of these variables will be fixed and the other one will be variable.%
\par
\hypertarget{p-850}{}%
In this chapter, you will investigate the following distributions: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-256}{}Binomial - the number of trials is fixed and X measures the variable number of successes%
\item\hypertarget{li-257}{}Geometric - the number of successes is fixed--at 1--and X measures the variable number of trials%
\item\hypertarget{li-258}{}Negative Binomial - the number of successes is fixed and X measures the variable number of trials%
\end{enumerate}
%
%
%
\typeout{************************************************}
\typeout{Section 7.2 Binomial Distribution}
\typeout{************************************************}
%
\section[{Binomial Distribution}]{Binomial Distribution}\label{section-46}
\hypertarget{p-851}{}%
Consider a sequence  of n independent Bernoulli trials with the likelihood of a success p on each individual trial stays constant from trial to trial with \(0 \lt p \lt 1 \). If we let the variable \(X\) measure the number of successes obtained when doing a fixed number of trials n with \(R = \{ 0, 1, ..., n \}\), then the resulting distribution of probabilities is called a Binomial Distribution.%
\par
\hypertarget{p-852}{}%
\leavevmode%
\begin{sageinput}
# Binomial distribution over 0 .. n
# Probability of success on one independent trial = p must also be given
var('x')
@interact
def _(n=slider(3,50,1,3),p=slider(1/20,19/20,1/20,1/2)):
    np1 = n+1
    R = range(np1)
    f(x) = factorial(n)/(factorial(x)*factorial(n-x))*p^x*(1-p)^(n-x)
    pretty_print(html('Density Function: $f(x) =%s$'%str(latex(f(x)))))
    pretty_print(html('over the space $R = %s$'%str(R)))
    G = points((k,f(x=k)) for k in R)
    G.show()
    R = [k for k in R]
    probs = [f(x=k) for k in R]
#    H = histogram( R, weights = probs, align="mid", linewidth=2, edgecolor="blue", color="yellow")
#    H.show()
    for k in R:
        pretty_print(html('$f(%s'%k+') = %s'%latex(f(x=k))+' \\approx %s$'%f(x=k).n(digits=5)))
\end{sageinput}
%
\par
\hypertarget{p-853}{}%
You can of course get specific values and graph the Binomial Distribution using R as well...%
\par
\hypertarget{p-854}{}%
\leavevmode%
\begin{sageinput}
n <- 10
p <- 0.3

paste('Probability Function')
dbinom(0:n, n, p)   # gives the probability function
paste('Distribution function')
pbinom(0:n, n, p)   # gives the distribution function
paste('A random sample')
rbinom(15, n, p)    # gives a random sample of 15 items from b(n,p)

x <- dbinom(0:n, size=n, prob=p)
barplot(x,names.arg=0:n, main=sprintf(paste('n=',n,' and p= ',p)))
\end{sageinput}
%
\par
\hypertarget{p-855}{}%
\begin{theorem}{Derivation of Binomial Probability Function.}{}{BinomialProbabilityFunctionTheorem}%
\hypertarget{BinomialProbabilityFunction}{}%
For R = {0, 1, ..., n},%
\begin{equation*}
f(x) = \binom{n}{x}p^x(1-p)^{n-x}
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-39}{}
\hypertarget{p-857}{}%
Since successive trials are independent, then the probability of X successes occurring within n trials is given by%
\begin{equation*}
P(X=x) = \binom{n}{x}P(SS...SFF...F) = 		
\binom{n}{x}p^x(1-p)^{n-x}\qedhere
\end{equation*}
%
\end{proof}
%
\par
\hypertarget{p-858}{}%
\begin{theorem}{Verification of Binomial Distribution Formula.}{}{theorem-38}%
\hypertarget{p-859}{}%
%
\begin{equation*}
\sum_{x \in R} f(x) = \sum_{x=0}^n \binom{n}{x}p^x(1-p)^{n-x} = 1.
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-40}{}
\hypertarget{p-860}{}%
Using the Binomial Theorem with a = p and b = 1-p yields%
\begin{equation*}
\sum_{x=0}^n \binom{n}{x}p^x(1-p)^{n-x} = (p + (1-p))^n = 1\qedhere
\end{equation*}
%
\end{proof}
%
\par
\hypertarget{p-861}{}%
Utilize the interactive cell below to compute f(x) and F(x) for the Binomial distribution%
\par
\hypertarget{p-862}{}%
\leavevmode%
\begin{sageinput}
# Binomial calculator
@interact
def _(p=input_box(0.3,width=15),n=input_box(10,width=15)):
    R = range(n+1)
    f(x) = binomial(n,x)*p^x*(1-p)^(n-x)
    acc = 0
    for k in R:
        prob = f(x=k)
        acc = acc+prob
        pretty_print('f(%s) = '%k,' %.8f'%prob,' and F(%s) = '%k,' %.8f'%acc)
\end{sageinput}
%
\par
\hypertarget{p-863}{}%
\begin{theorem}{Binomial Distribution Statistics.}{}{BinomialStatisticsTheorem}%
\hypertarget{BinomialStatistics}{}%
For the Binomial Distribution%
\begin{equation*}
\mu = np
\end{equation*}
%
\begin{equation*}
\sigma^2 = np(1-p)
\end{equation*}
%
\begin{equation*}
\gamma_1 = \frac{1-2p}{\sqrt{np(1-p)}}
\end{equation*}
%
\begin{equation*}
\gamma_2 = \frac{1-6p(1-p)}{np(1-p)} + 3
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-41}{}
\hypertarget{p-865}{}%
For the mean,%
\begin{align*}
\mu & = E[X] \\
& = \sum_{x=0}^{n} {x \binom{n}{x} p^x (1-p)^{n-x}}\\
& = \sum_{x=1}^{n} {x \frac{n(n-1)!}{x(x-1)!(n-x)!} p^x (1-p)^{n-x}}\\
& = np \sum_{x=1}^{n} {\frac{(n-1)!}{(x-1)!((n-1)-(x-1))!} p^{x-1} (1-p)^{(n-1)-(x-1)}}
\end{align*}
Using the change of variables \(k=x-1\) and \(m = n-1\) yields a binomial series%
\begin{align*}
& = np \sum_{k=0}^{m} {\frac{m!}{k!(m-k)!} p^k (1-p)^{m-k}}\\
& = np (p + (1-p))^m = np
\end{align*}
%
\par
\hypertarget{p-866}{}%
For the variance,%
\begin{align*}
\sigma^2 & = E[X(X-1)] + \mu - \mu^2 \\
& = \sum_{x=0}^{n} {x(x-1) \binom{n}{x} p^x (1-p)^{n-x}} + np - n^2p^2\\
& = \sum_{x=2}^{n} {x(x-1) \frac{n(n-1)(n-2)!}{x(x-1)(x-2)!(n-x)!} p^x (1-p)^{n-x}}  + np - n^2p^2\\
& = n(n-1)p^2 \sum_{x=2}^{n} {\frac{(n-2)!}{(x-2)!((n-2)-(x-2))!} p^{x-2} (1-p)^{(n-2)-(x-2)}} + np - n^2p^2
\end{align*}
%
\par
\hypertarget{p-867}{}%
Using the change of variables \(k=x-2\) and \(m = n-2\) yields a binomial series%
\begin{align*}
& = n(n-1)p^2  \sum_{k=0}^{m} {\frac{m!}{k!(m-k)!} p^k (1-p)^{m-k}} + np - n^2p^2\\
& = n(n-1)p^2 + np - n^2p^2 = np - np^2 = np(1-p)
\end{align*}
%
\par
\hypertarget{p-868}{}%
The skewness and kurtosis can be found similarly using formulas involving E[X(X-1)(X-2)] and E[X(X-1)(X-2)(X-3)]. The complete determination is performed using Sage below.%
\end{proof}
%
\par
\hypertarget{p-869}{}%
The following uses Sage to symbolically confirm the general formulas for the Binomial distribution.%
\par
\hypertarget{p-870}{}%
\leavevmode%
\begin{sageinput}
var('x,n,p')
assume(x,'integer')
f(x) = binomial(n,x)*p^x*(1-p)^(n-x)
mu = sum(x*f,x,0,n)
M2 = sum(x^2*f,x,0,n)
M3 = sum(x^3*f,x,0,n)
M4 = sum(x^4*f,x,0,n)

pretty_print('Mean = ',mu)

v = (M2-mu^2).factor()
pretty_print('Variance = ',v)
stand = sqrt(v)

sk = ((M3 - 3*M2*mu + 2*mu^3)).factor()/stand^3
pretty_print('Skewness = ',sk)

kurt = (M4 - 4*M3*mu + 6*M2*mu^2 -3*mu^4).factor()/stand^4
pretty_print('Kurtosis = ',(kurt-3).factor(),'+3')
\end{sageinput}
%
\par
\hypertarget{p-871}{}%
Flipping Coins%
\par
\hypertarget{p-872}{}%
Suppose you flip a coin exactly 20 times. Determine the probability of getting exactly 10 heads and then determine the probability of getting 10 or fewer heads.%
\textbf{Solution}.\quad%
\hypertarget{p-873}{}%
This is binomial with n = 20, p = 1/2 and you are looking for f(10). With these values%
\begin{equation*}
f(10) = \binom{20}{10} \cdot \left ( \frac{1}{2} \right )^{10} \cdot \left ( \frac{1}{2} \right 	)^{20-10} = \frac{46189}{262144} \approx 0.176
\end{equation*}
%
\par
\hypertarget{p-874}{}%
Notice, the mean for this distribution is also 10 so one might expect 10 heads in general.  Next, to determine the probability for 10 or fewer heads requires F(10) = f(0) + f(1) + ... + f(10). There is no "nice" formula for F but this calculation can be performed using a graphing calculator, such as the TI-84 with F(x) = binomcdf(n,p,x). In this case, F(10) = binomcdf(20,1/2,10) = 0.588.%
%
%
\typeout{************************************************}
\typeout{Section 7.3 Geometric Distribution}
\typeout{************************************************}
%
\section[{Geometric Distribution}]{Geometric Distribution}\label{section-47}
\hypertarget{p-875}{}%
Consider the situation where one can observe a sequence  of independent trials where the likelihood of a success on each individual trial stays constant from trial to trial. Call this likelihood the probably of "success" and denote its value by \(p\) where \(0 \lt p \lt 1 \). If we let the variable \(X\) measure the number of trials needed in order to obtain the first success with \(R = \{1, 2, 3, ... \}\), then the resulting distribution of probabilities is called a Geometric Distribution.%
\par
\hypertarget{p-876}{}%
\begin{theorem}{}{}{GeometricProbabilityFunctionTheorem}%
\hypertarget{GeometricProbabilityFunction}{}%
For a Geometric variable X with \(R = \{1, 2, 3, ... \}\),%
\begin{equation*}
f(x) = (1-p)({x-1} \cdot p
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-42}{}
\hypertarget{p-878}{}%
Since successive trials are independent, then the probability of the first success occurring on the mth trial presumes that the previous m-1 trials were all failures.  Therefore the desired probability is given by%
\begin{equation*}
f(x) = P(X = x) = P(FF...FS) = (1-p)^{x-1}p\qedhere
\end{equation*}
%
\end{proof}
%
\par
\hypertarget{p-879}{}%
\begin{theorem}{Geometric Distribution sums to 1.}{}{theorem-41}%
\hypertarget{p-880}{}%
%
\begin{equation*}
f(x) = (1-p)^{x-1}p
\end{equation*}
sums to 1 over \(R = \{ 1, 2, ... \}\)%
\end{theorem}
\begin{proof}\hypertarget{proof-43}{}
\hypertarget{p-881}{}%
%
\begin{equation*}
\sum_{x=1}^{\infty} {f(x)} = \sum_{x=1}^{\infty} {(1-p)^{x-1} p} \\ = p \sum_{j=0}^{\infty} {(1-p)^j} = p \frac{1}{1-(1-p)} = 1\qedhere
\end{equation*}
%
\end{proof}
%
\par
\hypertarget{p-882}{}%
\leavevmode%
\begin{sageinput}
# Geometric distribution over 0 .. n
# Probability of success on one independent trial = p must also be given
var('x')
# n = 50 by default. actually should be infinite
@interact
def _(p=input_box(0.1,label='p = '),n=[25,50,75,100,200]):
    np1 = n+1
    R = range(1,np1)
    f(x) = (1-p)^(x-1)*p
    F(x) = 1 - (1-p)^x
    pretty_print(html('Density Function: $f(x) =%s$'%str(latex(f(x)))+' over the space $R = %s$'%str(R)))
    points((k,f(x=k)) for k in R).show(title="Probability Function")
    print
    points((k,F(x=k)) for k in R).show(title="Distribution Function")
    if (n == 25):
        for k in R:
            pretty_print(html('$f(%s'%k+') = %s'%latex(f(x=k))+' \\approx %s$'%f(x=k).n(digits=5)))
\end{sageinput}
%
\par
\hypertarget{p-883}{}%
\begin{theorem}{Geometric Statistics Theorem.}{}{theorem-42}%
\hypertarget{GeometricStatistics}{}%
For the geometric distribution,%
\begin{equation*}
\mu = 1/p
\end{equation*}
%
\begin{equation*}
\sigma^2  = \frac{1-p}{p^2}
\end{equation*}
%
\begin{equation*}
\gamma_1 = ADD
\end{equation*}
%
\begin{equation*}
\gamma_2 = ADD + 3
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-44}{}
\hypertarget{p-885}{}%
For the mean,%
\begin{align*}
\mu & = E[X] = \sum_{k=0}^{\infty} {k(1-p)^{k-1}p}\\
& = p \sum_{k=1}^{\infty} {k(1-p)^{k-1}}\\
& = p \frac{1}{(1-(1-p))^2}\\
& = p \frac{1}{p^2} = \frac{1}{p}
\end{align*}
%
\par
\hypertarget{p-886}{}%
For the variance,%
\begin{align*}
\sigma^2 & = E[X(X-1)] + \mu - \mu^2 \\
& = \sum_{k=0}^{\infty} {k(k-1)(1-p)^{k-1}p} + \mu - \mu^2 \\
& = (1-p)p \sum_{k=2}^{\infty} {k(k-1)(1-p)^{k-2}} + \frac{1}{p} - \frac{1}{p^2}\\
& = (1-p)p \frac{2}{(1-(1-p))^3} + \frac{1}{p} - \frac{1}{p^2}\\
& = \frac{1-p}{p^2}\qedhere
\end{align*}
%
\end{proof}
%
\par
\hypertarget{p-887}{}%
\begin{theorem}{Geometric Distribution Function.}{}{theorem-43}%
\hypertarget{p-888}{}%
%
\begin{equation*}
F(x) =  1- (1-p)^{x}
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-45}{}
\hypertarget{p-889}{}%
Consider the accumulated probabilities over a range of values...%
\begin{align*}
P(X \le x) & = 1 - P(X \gt x)\\
& = 1- \sum_{k={x+1}}^{\infty} {(1-p)^{k-1}p}\\
& = 1- p \frac{(1-p)^{x}}{1-(1-p)}\\
& = 1- (1-p)^{x}\qedhere
\end{align*}
%
\end{proof}
%
\par
\hypertarget{p-890}{}%
\begin{theorem}{Statistics for Geometric Distribution.}{}{theorem-44}%
\hypertarget{p-891}{}%
Mean, Variance, Skewness, Kurtosis computed by Sage.%
\end{theorem}
\begin{proof}\hypertarget{proof-46}{}
\hypertarget{p-892}{}%
\leavevmode%
\begin{sageinput}
var('x,n,p')
assume(x,'integer')
f(x) = p*(1-p)^(x-1)
mu = sum(x*f,x,0,oo).full_simplify()
M2 = sum(x^2*f,x,0,oo).full_simplify()
M3 = sum(x^3*f,x,0,oo).full_simplify()
M4 = sum(x^4*f,x,0,oo).full_simplify()

pretty_print('Mean = ',mu)

v = (M2-mu^2).factor().full_simplify()
pretty_print('Variance = ',v)
stand = sqrt(v)

sk = (((M3 - 3*M2*mu + 2*mu^3))/stand^3).full_simplify()
pretty_print('Skewness = ',sk)

kurt = (M4 - 4*M3*mu + 6*M2*mu^2 -3*mu^4).factor()/stand^4
pretty_print('Kurtosis = ',(kurt-3).factor(),'+3')
\end{sageinput}
%
\end{proof}
%
\par
\hypertarget{p-893}{}%
\begin{theorem}{The Geometric Distribution yields a memoryless model..}{}{theorem-45}%
\hypertarget{p-894}{}%
If X has a geometric distribution and a and b are nonnegative integers, then%
\begin{equation*}
P( X > a + b | X > b ) = P( X > a)
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-47}{}
\hypertarget{p-895}{}%
Using the definition of conditional probability,%
\begin{align*}
P( X > a + b | X > b ) & = P( X > a + b \cap X > b ) \ P( X > b)\\
& = P( X > a + b ) / P( X > b)\\
& = (1-p)^{a+b} / (1-p)^b\\
& = (1-p)^a\\
& = P(X > a)\qedhere
\end{align*}
%
\end{proof}
%
%
%
\typeout{************************************************}
\typeout{Section 7.4 Negative Binomial}
\typeout{************************************************}
%
\section[{Negative Binomial}]{Negative Binomial}\label{section-48}
\hypertarget{p-896}{}%
Consider the situation where one can observe a sequence  of independent trials where the likelihood of a success on each individual trial stays constant from trial to trial. Call this likelihood the probably of "success" and denote its value by \(p\) where \(0 \lt p \lt 1 \). If we let the variable \(X\) measure the number of trials needed in order to obtain the rth success, \(r \ge 1\), with \(R = \{r, r+1, r+2, ... \}\) then the resulting distribution of probabilities is called a Geometric Distribution.%
\par
\hypertarget{p-897}{}%
Note that r=1 gives the Geometric Distribution.%
\par
\hypertarget{p-898}{}%
\begin{theorem}{Negative Binomial Series.}{}{theorem-46}%
\hypertarget{p-899}{}%
%
\begin{equation*}
\displaystyle \frac{1}{(a+b)^n} = \sum_{k=0}^{\infty} {(-1)^k \binom{n + k - 1}{k} a^k b^{-n-k}}
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-48}{}
\hypertarget{p-900}{}%
First, convert the problem to a slightly different form: \(\frac{1}{(a+b)^n} = \frac{1}{b^n} \frac{1}{(\frac{a}{b}+1)^n} 
= \frac{1}{b^n} \sum_{k=0}^{\infty} {(-1)^k \binom{n + k - 1}{k} \left ( \frac{a}{b} \right ) ^k}\)%
\par
\hypertarget{p-901}{}%
So, let's replace \(\frac{a}{b} = x\) and ignore for a while the term factored out. Then, we only need to show%
\begin{gather*}
\sum_{k=0}^{\infty} {(-1)^k \binom{n + k - 1}{k} x^k} = \left ( \frac{1}{1+x} \right )^n 
\end{gather*}
However%
\begin{align*}
\left ( \frac{1}{1+x} \right )^n & = \left ( \frac{1}{1 - (-x)} \right )^n \\
& = \left ( \sum_{k=0}^{\infty} {(-1)^k x^k} \right )^n
\end{align*}
%
\par
\hypertarget{p-902}{}%
This infinite sum raised to a power can be expanded by distributing terms in the standard way. In doing so, the various powers of x multiplied together will create a series in powers of x involving \(x^0, x^1, x^2, ...\). To detemine the final coefficients notice that the number of time \(m^k\) will appear in this product depends upon the number of ways one can write k as a sum of nonnegative integers.%
\par
\hypertarget{p-903}{}%
For example, the coefficient of \(x^3\) will come from the n ways of multiplying the coefficients \(x^3, x^0, ..., x^0\) and \(x^2, x^1, x^0, ..., x^0\) and \(x^1, x^1, x^1, x^0,..., x^0\). This is equivalent to finding the number of ways to write the number k as a sum of nonnegative integers. The possible set of nonnegative integers is {0,1,2,...,k} and one way to count the combinations is to separate k *'s by n-1 |'s.  For example, if k = 3 then *||** means \(x^1 x^0 x^2 = x^3\). Similarly for k = 5 and |**|*|**| implies \(x^0 x^2 x^1 x^2 x^0 = x^5\). The number of ways to interchange the identical *'s among the idential |'s is \(\binom{n+k-1}{k}\).%
\par
\hypertarget{p-904}{}%
Furthermore, to obtain an even power of x will require an even number of odd powers and an odd power of x will require an odd number of odd powers. So, the coefficient of the odd terms stays odd and the coefficient of the even terms remains even. Therefore,%
\begin{gather*}
\left ( \frac{1}{1+x} \right )^n = \sum_{k=0}^{\infty} {(-1)^k \binom{n + k - 1}{k} x^k}
\end{gather*}
Similarly, \(\left ( \frac{1}{1-x} \right )^n = \left ( \sum_{k=0}^{\infty} {x^k} \right )^n = \sum_{k=0}^{\infty} {\binom{n + k - 1}{k} x^k}\)%
\end{proof}
%
\par
\hypertarget{p-905}{}%
Consider the situation where one can observe a sequence  of independent trials with the likelihood of a success on each individual trial \(p\) where \(0 \lt p \lt 1 \). For a positive integer r, let the variable \(X\) measure the number of trials needed in order to obtain the rth success. Then the resulting distribution of probabilities is called a Negative Binomial Distribution.%
\par
\hypertarget{p-906}{}%
\begin{theorem}{Negative Binomial Probability Function.}{}{theorem-47}%
\hypertarget{NegativeBinomialProbabilityFunction}{}%
%
\begin{equation*}
f(x) = \binom{x - 1}{r-1}(1-p)^{x-r}p^r,
\end{equation*}
for \(x \in R = \{r, r+1, ... \}\).%
\end{theorem}
\begin{proof}\hypertarget{proof-49}{}
\hypertarget{p-908}{}%
Since successive trials are independent, then the probability of the rth success occurring on the x-th trial presumes that in the previous x-1 trials were r-1 successes and x-r failures. You can arrange these indistinguishable successes (and failures) in \(\binom{x-1}{r-1}\) unique ways. Therefore the desired probability is given by%
\begin{gather*}
P(X=x) = \binom{x - 1}{r-1}(1-p)^{x-r}p^r\qedhere
\end{gather*}
%
\end{proof}
%
\par
\hypertarget{p-909}{}%
\begin{theorem}{Negative Binomial Distribution Sums to 1.}{}{theorem-48}%
\hypertarget{p-910}{}%
%
\begin{equation*}
\sum_{x=r}^{\infty} {\binom{x - 1}{r-1}(1-p)^{x-r}p^r} = 1
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-50}{}
\hypertarget{p-911}{}%
%
\begin{equation*}
\sum_{x=r}^{\infty} {\binom{x - 1}{r-1}(1-p)^{x-r}p^r} = p^r \sum_{x=r}^{\infty} {\binom{x - 1}{r-1}(1-p)^{x-r}}
\end{equation*}
\hypertarget{p-912}{}%
and by using \(k = x-r\)%
%
\begin{align*}
& = p^r \sum_{k=0}^{\infty} {\binom{r + k - 1}{k}(1-p)^k}\\
& = p^r \frac{1}{(1-(1-p))^r}\\
& = 1\qedhere
\end{align*}
%
\end{proof}
%
\par
\hypertarget{p-913}{}%
Below, the interactive cell symbolically computes f(x) and F(x) for the negative binomial distribution.%
\par
\hypertarget{p-914}{}%
\leavevmode%
\begin{sageinput}
# Negative Binomial calculator
@interact
def _(p=input_box(0.3,width=15),r=slider(1,10,1,2)):
    n = 4*(floor(r/p)+1)
    np1 = n+1
    R = range(r,np1)
    f(x) = (factorial(x-1)/(factorial(r-1)*factorial(x-r)))*(1-p)^(x-r)*p^r
    acc = 0
    for k in R:
        prob = f(x=k)
        acc = acc+prob
        pretty_print('f(%s) = '%k,' %.8f'%prob,' and F(%s) = '%k,' %.8f'%acc)
\end{sageinput}
%
\par
\hypertarget{p-915}{}%
\begin{theorem}{Statistics for Negative Binomial Distribution.}{}{theorem-49}%
\hypertarget{NegativeBinomialStatistics}{}%
For the Negative Binomial Distribution,%
\begin{equation*}
\mu = \frac{r}{p}
\end{equation*}
%
\begin{equation*}
\sigma^2 = r \frac{1-p}{p^2}
\end{equation*}
%
\begin{equation*}
\gamma_1 = \frac{2-p}{\sqrt{r(1-p)}}
\end{equation*}
%
\begin{equation*}
\gamma_2 = \frac{p^2-6p+6}{r(1-p)} + 3
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-51}{}
\leavevmode%
\begin{sageinput}
# Negative Binomial
var('x,n,p,r,alpha')
assume(x,'integer')
assume(alpha,'integer')
assume(alpha > 2)
assume(0 < p < 1)
@interact
def _(r=[2,5,10,15,alpha]):
    f(x) = binomial(x-1,r-1)*p^r*(1-p)^(x-r)
    mu = sum(x*f,x,r,oo).full_simplify()
    M2 = sum(x^2*f,x,r,oo).full_simplify()
    M3 = sum(x^3*f,x,r,oo).full_simplify()
    M4 = sum(x^4*f,x,r,oo).full_simplify()
        
    pretty_print('Mean = ',mu)
    
    v = (M2-mu^2).full_simplify()
    pretty_print('Variance = ',v)
    stand = sqrt(v)
    
    sk = (((M3 - 3*M2*mu + 2*mu^3)).full_simplify()/stand^3).factor()
    pretty_print('Skewness = ',sk)
    
    kurt = ((M4 - 4*M3*mu + 6*M2*mu^2 -3*mu^4)/v^2).full_simplify()
    pretty_print('Kurtosis = ',(kurt-3).factor(),'+3')
\end{sageinput}
\end{proof}
%
\par
\hypertarget{p-917}{}%
This is the symbolic derivation component that should be the proof of the theorem above.%
\begin{sageinput}
# Negative Binomial
var('x,n,p,r,alpha')
assume(x,'integer')
assume(alpha,'integer')
assume(alpha > 2)
assume(0 < p < 1)
@interact
def _(r=[2,5,10,15,alpha]):
    f(x) = binomial(x-1,r-1)*p^r*(1-p)^(x-r)
    mu = sum(x*f,x,r,oo).full_simplify()
    M2 = sum(x^2*f,x,r,oo).full_simplify()
    M3 = sum(x^3*f,x,r,oo).full_simplify()
    M4 = sum(x^4*f,x,r,oo).full_simplify()
        
    pretty_print('Mean = ',mu)
    
    v = (M2-mu^2).full_simplify()
    pretty_print('Variance = ',v)
    stand = sqrt(v)
    
    sk = (((M3 - 3*M2*mu + 2*mu^3)).full_simplify()/stand^3).factor()
    pretty_print('Skewness = ',sk)
    
    kurt = ((M4 - 4*M3*mu + 6*M2*mu^2 -3*mu^4)/v^2).full_simplify()
    pretty_print('Kurtosis = ',(kurt-3).factor(),'+3')
\end{sageinput}
%
%
\typeout{************************************************}
\typeout{Section 7.5 Summary}
\typeout{************************************************}
%
\section[{Summary}]{Summary}\label{section-49}
\hypertarget{p-918}{}%
\hyperlink{BinomialProbabilityFunction}{Binomial Distribution~}%
\par
\hypertarget{p-919}{}%
\hyperlink{BinomialStatistics}{Binomial Distribution Statistics~}%
\par
\hypertarget{p-920}{}%
\hyperlink{GeometricProbabilityFunction}{Geometric Distribution~}%
\par
\hypertarget{p-921}{}%
\hyperlink{GeometricStatistics}{Geometric Distribution Statistics~}%
\par
\hypertarget{p-922}{}%
\hyperlink{NegativeBinomialProbabilityFunction}{Negative Binomial Distribution~}%
\par
\hypertarget{p-923}{}%
\hyperlink{NegativeBinomialStatistics}{Negative Binomial Distribution Statistics~}%
%
%
\typeout{************************************************}
\typeout{Section 7.6 Exercises}
\typeout{************************************************}
%
\section[{Exercises}]{Exercises}\label{section-50}
\hypertarget{p-924}{}%
\begin{inlineexercise}{- Gallup Consumer Confidence Polling.}{exercise-59}%
\hypertarget{p-925}{}%
A January 2008 Gallup poll on consumer confidence asked the question "How would you rate economic conditions in this country today" and 22% responded "Excellent" or "Good", 45% responded "Only Fair", and 33% responded "Poor". If you pick a representative sample of 25 people and ask them the same question, if X measures the number of responses that are "Excellent" or "Good", determine \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}P(X is at most 5).%
\item{}P(X is at least 5).%
\item{}the expected number of Excellent or Good responses.%
\end{itemize}
%
\textbf{Solution}.\quad%
\hypertarget{p-926}{}%
This is a Binomial distribution with n=25 and p = 0.22. \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}P(X is at most 5) = F(5) = f(0)+f(1)+f(2)+f(3)+f(4)+f(5) = 0.51843%
\item{}P(X is at least 5) = 1 - F(4) = 0.67183%
\item{}\(\mu = np = 25 \cdot .22 = 5.5\)%
\end{itemize}
%
\end{inlineexercise}
%
\par
\hypertarget{p-927}{}%
\begin{inlineexercise}{- Rolling Dice.}{exercise-60}%
\hypertarget{p-928}{}%
You keep on rolling a pair of dice and let X be the number of rolls needed until you get a sum of 7 or 11 for the second time. Determine: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-265}{}P(7 or 11 on one roll)%
\item\hypertarget{li-266}{}The expected number of rolls until you get the second 7 or 11 sum.%
\item\hypertarget{li-267}{}P(X = 12)%
\item\hypertarget{li-268}{}\(P(X \ge 4)\)%
\end{enumerate}
%
\textbf{Solution}.\quad%
\hypertarget{p-929}{}%
For this problem, use the Negative Binomial Distribution when looking for the number of trials till the 2nd success. p is determined in the first answer. \leavevmode%
\begin{enumerate}
\item\hypertarget{li-269}{}P(7 or 11 on one roll) = 8/36 = 2/9, using equally likely outcomes.%
\item\hypertarget{li-270}{}\(\mu = \frac{r}{p} = \frac{2}{2/9} = 9\)%
\item\hypertarget{li-271}{}\(P( X = 12) = \binom{11}{1} (7/9)^10 \cdot (2/9)^2 = 0.044\)%
\item\hypertarget{li-272}{}%
\begin{align*}
P(X \ge 4) & = 1- P(x \le 3) \\
& = 1 - [ f(2) + f(3) ]\\
& = 1 - \left[ \binom{1}{1} (2/9)^2 + \binom{2}{1} (7/9)^1 \cdot (2/9)^2 \right ]\\
& = 1 - 0.1262 = 0.8738.
\end{align*}
%
\end{enumerate}
%
\end{inlineexercise}
%
\par
\hypertarget{p-930}{}%
\begin{inlineexercise}{- Collecting Kids Meal Prizes.}{exercise-61}%
\hypertarget{p-931}{}%
You love to eat at Chick-Fil-A with your kids and want to collect all of the five new book titles that come randomly included with each kids meal.  If the promotion with these books starts today, determine: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-273}{}The probability that you get a book you don't have when purchasing the first children's meal.%
\item\hypertarget{li-274}{}The probability that you it takes more than four purchases in order to get a second title.%
\item\hypertarget{li-275}{}The expected total number of children's meals you would expect to purchase in order to get all five titles.%
\end{enumerate}
%
\textbf{Solution}.\quad%
\hypertarget{p-932}{}%
\leavevmode%
\begin{enumerate}
\item\hypertarget{li-276}{}One. The first meal will certainly have a book that you have not received yet.%
\item\hypertarget{li-277}{}This is a geometric distribution with p=4/5.  P(X > 4) = 1 - F(4) = \((1 - 4/5)^4 = \frac{1}{625}\) which is very small. Note, in this case you would have needed to receive the same title randomly for all of the first four kids meal purchases. If this were to ever happen, please let the people at the counter know and it will be their pleasure to swap out for a new title.%
\item\hypertarget{li-278}{}Use the geometric distribution five times with changing values for p. For the first book p = 1 means you are certain to get a new title. For the second book title the probability of success is p=4/5; for the third book title the probability of success is p=3/5; for the fourth the probability is p=2/5; and for the last the probability is p = 1/5. Using the mean as 1/p in each case and accumulating these gives the total expected number of meals to purchase as%
\begin{align*}
& 1 + \frac{1}{\frac{4}{5}} + \frac{1}{\frac{3}{5}} + \frac{1}{\frac{2}{5}} + \frac{1}{\frac{1}{5}} \\
& = 1 + \frac{5}{4} + \frac{5}{3} + \frac{5}{2} + \frac{5}{1} \\
& = \frac{12 + 15 + 25 + 30 + 60}{12} \\
& = \frac{142}{12} = 11.833
\end{align*}
and so you would need 12 kids meals.  If this were to happen, please be certain to donate the "extra" books to an organization that works with kids or directly to some kids that you might know.%
\end{enumerate}
%
\end{inlineexercise}
%
\par
\hypertarget{p-933}{}%
\begin{inlineexercise}{- Rolling Dice.}{exercise-62}%
\hypertarget{p-934}{}%
Suppose you roll a standard pair of 6-sided dice 20 times and let X measure the number of outcomes which result in a sum of 7 or 11. Determine: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-279}{}the expected number of rolls which have a sum of 7 or 11%
\item\hypertarget{li-280}{}P(X=5)%
\item\hypertarget{li-281}{}P( X > 5)%
\item\hypertarget{li-282}{}P( X < 5)%
\end{enumerate}
%
\end{inlineexercise}
%
\par
\hypertarget{p-935}{}%
\begin{inlineexercise}{- Rolling Dice yet again.}{exercise-63}%
\hypertarget{p-936}{}%
Suppose you roll a standard pair of 6-sided dice X times until you get a sum of 7 or 11 a third time. Determine: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-283}{}the expected number of rolls needed on average.%
\item\hypertarget{li-284}{}P(X=5)%
\item\hypertarget{li-285}{}P( X > 5)%
\item\hypertarget{li-286}{}P( X < 5)%
\end{enumerate}
%
\end{inlineexercise}
%
\par
\hypertarget{p-937}{}%
\begin{inlineexercise}{- 2 standard deviations from the mean.}{exercise-64}%
\hypertarget{p-938}{}%
Given p = 0.3 determine the following: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-287}{}For Binomial with n = 50, \(P(\mu - 2\sigma \le X \le \mu + 2\sigma)\)%
\item\hypertarget{li-288}{}For Negative Binomial with r = 2, \(P(\mu - 2\sigma \le X \le \mu + 2\sigma)\)%
\end{enumerate}
%
\textbf{Solution}.\quad%
\hypertarget{p-939}{}%
For Binomial with p = 0.3 and n = 50, \(\mu = n \cdot p = 15\) and \(\sigma^2 = n \cdot p \cdot (1-p) = 10.5\).  So, \(\sigma = \sqrt{10.5}\).  Therefore,%
\begin{align*}
P(\mu - 2\sigma \le X \le \mu + 2\sigma) & = P(15 - 2 \sqrt{10.5} \le X \le 15 + 2 \sqrt{10.5})\\
& P( X \in \{9, 10, 11, ... , 19, 20, 21 \} )
\end{align*}
Then%
\begin{equation*}
F(21) - F(8) \approx 0.97491 - 0.01825 = 0.95666 
\end{equation*}
%
\par
\hypertarget{p-940}{}%
For Negative Binomial with p = 0.3 and r = 2, \(\mu = \frac{2}{0.3} = \frac{20}{3}\) and \(\sigma^2 = 2 \frac{0.7}{0.3^2} = \frac{140}{9}\) and so \(\sigma \approx 3.9\).  Therefore,%
\begin{align*}
P(\mu - 2\sigma \le X \le \mu + 2\sigma) & = P(6.7 - 7.8 \le X \le 6.7 + 7.8)\\
& P( X \in \{2, 3, ... , 14, 15, 16 \} )
\end{align*}
Then,%
\begin{equation*}
F(16) \approx 0.973888
\end{equation*}
%
\end{inlineexercise}
%
\par
\hypertarget{p-941}{}%
\begin{inlineexercise}{Chapter Experiment.}{exercise-65}%
\hypertarget{p-942}{}%
Take a die and roll it 4 times, keeping track on paper each time you get a 6 (say).  Repeat this 100 times. (Actually, you can use a standard 6-sided die or find a more exotic one with more sides.)  You should have gotten anywhere from 0 of the rolls to be a 6 up to all of the rolls to be a 6.  Collect the relative frequencies of each of the possible outcomes in R= \{0,1,2, ..., 100\} and plot.  Compare several of the experimental relative frequencies with the theoretical value you would expect using the proper distribution from this chapter. Comment on how well you did.%
\end{inlineexercise}
%
%
%
\typeout{************************************************}
\typeout{Chapter 8 Distributions based upon Poisson Processes}
\typeout{************************************************}
%
\chapter[{Distributions based upon Poisson Processes}]{Distributions based upon Poisson Processes}\label{PoissonExponentialGamma}
%
%
\typeout{************************************************}
\typeout{Section 8.1 Introduction}
\typeout{************************************************}
%
\section[{Introduction}]{Introduction}\label{section-51}
\hypertarget{p-943}{}%
In this chapter, you will investigate the relationship between number of successes over some interval.  For each, one of these quantities will be fixed and the other one variable. First, consider the following:%
\par
\hypertarget{p-944}{}%
\begin{definition}{Poisson Process.}{definition-34}%
\hypertarget{p-945}{}%
A Poisson process is a course of action in which: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-289}{}Successes in non-overlapping subintervals are independent of each other.%
\item\hypertarget{li-290}{}The probability of exactly one success in a sufficiently small interval of length h is proportional to h.  In notation, P(one success) = \(\lambda h\).%
\item\hypertarget{li-291}{}The probability of two or more successes in a sufficiently small interval is essentially 0.%
\end{enumerate}
%
\end{definition}
%
\par
\hypertarget{p-946}{}%
You should presume these assumptions implicitly for the distributions discussed in this chapter.%
\par
\hypertarget{p-947}{}%
In this chapter, you will investigate the following distributions: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-292}{}Poisson - the interval is fixed and X measures the variable number of successes.%
\item\hypertarget{li-293}{}Exponential - the number of successes is fixed--at 1--and X measures the variable interval length needed to get that success.%
\item\hypertarget{li-294}{}Gamma - the number of successes is fixed and X measures the variable interval needed to get the desired number of successes.%
\end{enumerate}
%
%
%
\typeout{************************************************}
\typeout{Section 8.2 Poisson Distribution}
\typeout{************************************************}
%
\section[{Poisson Distribution}]{Poisson Distribution}\label{section-52}
\hypertarget{p-948}{}%
Consider a Poisson Process where you start with an interval of fixed length T and where X measures the variable number of successes, or changes, within a that interval. The resulting distribution of X will be called a Poisson distribution.%
\par
\hypertarget{p-949}{}%
\begin{theorem}{Poisson Probability Function.}{}{theorem-50}%
\hypertarget{p-950}{}%
Assume X measures the number of successes in an interval [0,T] within some Poisson process. Then,%
\begin{equation*}
f(x) = \frac{\mu^x e^{-\mu}}{x!}
\end{equation*}
for \(R = \{ 0, 1, 2, ... \}\).%
\end{theorem}
\begin{proof}\hypertarget{proof-52}{}
\hypertarget{p-951}{}%
For a sufficiently large natural number n, break up the given interval [0,T] into n uniform parts each of width h = T/n.  Using the properties of Poisson processes, n very large implies h will be very small and eventually small enough so that%
\begin{equation*}
P(\text{exactly one success on a given interval}) = p = \lambda \frac{T}{n}.
\end{equation*}
However, since there are a finite number of independent intervals each with probability p of containing a success then you can use a Binomial distribution to evaluate the corresponding probabilities so long as n is finite. Doing so yields and taking the limit as n approaches infinity gives:%
\begin{align*}
f(x) & = P(\text{X changes in [0,T]}) \\
& = \lim_{n \rightarrow \infty} \binom{n}{x} p^x (1-p)^{n-x}\\
& = \lim_{n \rightarrow \infty} \binom{n}{x} (\frac{\lambda T}{n})^x (1-\lambda \frac{T}{n})^{n-x}\\
& = \lim_{n \rightarrow \infty} \frac{n(n-1)...(n-x+1)}{x!} ( \frac{\lambda T}{n})^x (1- \frac{\lambda T}{n})^{n-x}\\
& = \frac{(\lambda T)^x}{x!} \lim_{n \rightarrow \infty} \frac{n(n-1)...(n-x+1)}{n \cdot n \cdot ... \cdot n} (1-\lambda \frac{T}{n})^{n}(1-\lambda \frac{T}{n})^{-x}\\
& = \frac{(\lambda T)^x}{x!} \lim_{n \rightarrow \infty} (1-\frac{1}{n})...(1-\frac{x-1}{n})  (1- \frac{\lambda T}{n})^{n}(1- \frac{\lambda T}{n})^{-x}\\
& = \frac{(\lambda T)^x}{x!} 
\lim_{n \rightarrow \infty} (1- \frac{\lambda T}{n})^{n}
\lim_{n \rightarrow \infty} (1- \frac{\lambda T}{n})^{-x}\\
& = \frac{(\lambda T)^x}{x!} 
\lim_{n \rightarrow \infty} (1- \frac{\lambda T}{n})^{n} \cdot 1\\
& = \frac{(\lambda T)^x}{x!} 
e^{-\lambda T}\qedhere
\end{align*}
%
\end{proof}
%
\par
\hypertarget{p-952}{}%
\begin{theorem}{Verify Poisson Probability Function.}{}{theorem-51}%
\hypertarget{p-953}{}%
%
\begin{equation*}
\sum_{x=0}^{\infty} \frac{(\lambda T)^x}{x!} e^{-\lambda T} = 1
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-53}{}
\hypertarget{p-954}{}%
Using the Power Series expansion for the natural exponential,%
\begin{align*}
\sum_{x=0}^{\infty} f(x) & = \sum_{x=0}^{\infty} \frac{(\lambda T)^x}{x!} e^{-\lambda T} \\
& = e^{-\lambda T} \sum_{x=0}^{\infty} \frac{(\lambda T)^x}{x!} \\
& = e^{-\lambda T} e^{\lambda T}  \\
& = 1\qedhere
\end{align*}
%
\end{proof}
%
\par
\hypertarget{p-955}{}%
\begin{theorem}{Statistics for Poisson.}{}{theorem-52}%
\hypertarget{p-956}{}%
%
\begin{equation*}
\mu = \lambda T
\end{equation*}
%
\begin{equation*}
\sigma^2 = \mu
\end{equation*}
%
\begin{equation*}
\gamma_1 = \frac{1}{\sqrt{\mu}}
\end{equation*}
%
\begin{equation*}
\gamma_2 = \frac{1}{\mu}+3
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-54}{}
\hypertarget{p-957}{}%
Using the f(x) generated in the previous theorem%
\begin{align*}
\mu & = E[X] \\
& = \sum_{x=0}^{\infty} x \cdot \frac{(\lambda T)^x}{x!} e^{-\lambda T}\\
& = \lambda T e^{-\lambda T} \sum_{x=1}^{\infty} \frac{(\lambda T)^{x-1}}{(x-1)!} \\
& = \lambda T e^{-\lambda T} \sum_{k=0}^{\infty} \frac{(\lambda T)^k}{k!} \\
& = \lambda T e^{-\lambda T} e^{\lambda T} \\
& = \lambda T 
\end{align*}
which confirms the use of \(\mu\) in the original probability formula.%
\par
\hypertarget{p-958}{}%
Continuing with \(\mu = \lambda T\), the variance is given by%
\begin{align*}
\sigma^2 & = E[X(X-1)] + \mu - \mu^2 \\
& = \sum_{x=0}^{\infty} x(x-1) \cdot \frac{\mu^x}{x!} e^{-\mu} + \mu - \mu^2\\
& = e^{-\mu} \mu^2 \sum_{x=2}^{\infty} \frac{\mu^{x-2}}{(x-2)!} + \mu - \mu^2\\
& = e^{-\mu} \mu^2 \sum_{k=0}^{\infty} \frac{\mu^k}{k!} + \mu - \mu^2\\
& = \mu^2 + \mu - \mu^2 \\
& = \mu
\end{align*}
To derive the skewness and kurtosis, you can depend upon Sage...see the live cell below.%
\end{proof}
%
\par
\hypertarget{p-959}{}%
\leavevmode%
\begin{sageinput}
var('x,mu')
assume(x,'integer')

f(x) =e^(-mu)*mu^x/factorial(x)
mu = sum(x*f,x,0,oo).factor()
M2 = sum(x^2*f,x,0,oo).factor()
M3 = sum(x^3*f,x,0,oo).factor()
M4 = sum(x^4*f,x,0,oo).factor()

pretty_print('Mean = ',mu)

v = (M2-mu^2).factor()
pretty_print('Variance = ',v)
stand = sqrt(v)

sk = ((M3 - 3*M2*mu + 2*mu^3)).factor()/stand^3
pretty_print('Skewness = ',sk)

kurt = (M4 - 4*M3*mu + 6*M2*mu^2 -3*mu^4).factor()/stand^4
pretty_print('Kurtosis = ',(kurt-3).factor(),'+3')
\end{sageinput}
%
\par
\hypertarget{p-960}{}%
Approximation by binomial means you can also use Poisson to approximate Binomial for n sufficiently large.%
%
%
\typeout{************************************************}
\typeout{Section 8.3 Exponential Distribution}
\typeout{************************************************}
%
\section[{Exponential Distribution}]{Exponential Distribution}\label{section-53}
\hypertarget{p-961}{}%
Once again, consider a Poisson Process where you start with an interval of variable length X so that X measures the interval needed in order to obtain a first success with \(R = (0,\infty)\). The resulting distribution of X will be called an Exponential distribution.%
\par
\hypertarget{p-962}{}%
To derive the probability function for this distribution, consider finding f(x) by first considering F(x). This gives%
\begin{align*}
F(x)&  = P(X \le x)\\
& = 1 - P(X \gt x)\\
& = 1 - P(\text{first change occurs after an interval of length x})\\
& = 1 - P(\text{no changes in the interval [0,x]})\\
& = 1 - \frac{(\lambda x)^0 e^{-\lambda x}}{0!}\\
& = 1 - e^{-\lambda x}
\end{align*}
where the discrete Poisson Probability Function is used to answer the probability of exactly no changes in the "fixed" interval [0,x]. Using this distribution function and taking the derivative yields%
\begin{equation*}
f(x) = F'(x) = \lambda e^{-\lambda x}.
\end{equation*}
%
\par
\hypertarget{p-963}{}%
\begin{definition}{Exponential Distribution Probability Function.}{definition-35}%
\hypertarget{p-964}{}%
Given a Poisson process and a constant \(\mu\), suppose X measures the variable interval length needed until you get a first success.  Then X has an exponential distribution with probability function%
\begin{equation*}
f(x) = \frac{1}{\mu} e^{-\frac{x}{\mu}}.
\end{equation*}
%
\end{definition}
%
\par
\hypertarget{p-965}{}%
\begin{theorem}{Verification of Exponential Probability Function.}{}{theorem-53}%
\hypertarget{p-966}{}%
%
\begin{equation*}
\int_0^{\infty} \frac{1}{\mu} e^{-\frac{x}{\mu}} dx = 1
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-55}{}
\hypertarget{p-967}{}%
%
\begin{align*}
& \int_0^{\infty} \frac{1}{\mu} e^{-\frac{x}{\mu}} dx\\
& = \int_0^{\infty} e^{-u} dx\\
& = -e^{-u} \big |_0^{\infty} = 1\qedhere
\end{align*}
%
\end{proof}
%
\par
\hypertarget{p-968}{}%
\begin{theorem}{Distribution function for Exponential Distribution.}{}{theorem-54}%
\hypertarget{p-969}{}%
%
\begin{equation*}
F(x) = 1 - e^{-\frac{x}{\mu}}
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-56}{}
\hypertarget{p-970}{}%
Using \(f(x) = \frac{1}{\mu} e^{-\frac{x}{\mu}}\), note%
\begin{align*}
F(x) & = \int_0^x \frac{1}{\mu} e^{-\frac{u}{\mu}} du\\
& =  - e^{-\frac{u}{\mu}} \big |_0^x\\
& = 1 - e^{-\frac{x}{\mu}}\qedhere
\end{align*}
%
\end{proof}
%
\par
\hypertarget{p-971}{}%
\begin{theorem}{Derivation of Statistics for Exponential Distribution and Plotting.}{}{theorem-55}%
\hypertarget{p-972}{}%
%
\begin{equation*}
\sigma^2 = \mu^2
\end{equation*}
%
\begin{equation*}
\gamma_1 = 2
\end{equation*}
%
\begin{equation*}
\gamma_2 = 9
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-57}{}
\hypertarget{p-973}{}%
For the mean, notice that%
\begin{align*}
\text{Mean} & = \int_0^{\infty} x \cdot \frac{1}{\mu} e^{-\frac{x}{\mu}} \\
& = [ (1-x) e^{-\frac{x}{\mu}} ] \big |_0^{\infty} = \mu
\end{align*}
and so the use of \(\mu\) in f(x) is warranted.%
\par
\hypertarget{p-974}{}%
The remaining statistics are derived similarly using repeated integration by parts. The interactive Sage cell below calculates those for you automatically.%
\end{proof}
%
\par
\hypertarget{p-975}{}%
\leavevmode%
\begin{sageinput}
# Exponential Distribution
var('x,mu')
assume(mu>0)

f(x) =e^(-x/mu)/mu
mu = integral(x*f,x,0,oo).factor()
M2 = integral(x^2*f,x,0,oo).factor()
M3 = integral(x^3*f,x,0,oo).factor()
M4 = integral(x^4*f,x,0,oo).factor()

pretty_print('Mean = ',mu)

v = (M2-mu^2).factor()
pretty_print('Variance = ',v)
stand = sqrt(v)

sk = (((M3 - 3*M2*mu + 2*mu^3))/stand^3).simplify()
pretty_print('Skewness = ',sk)

kurt = (M4 - 4*M3*mu + 6*M2*mu^2 -3*mu^4).factor()/stand^4
pretty_print('Kurtosis = ',(kurt-3).factor(),'+3')
@interact
def _(m = slider(1,12,1/2,2,label='mu')):
    plot(f(mu=m),x,0,30).show(ymax=1)
\end{sageinput}
%
\par
\hypertarget{p-976}{}%
\begin{theorem}{The Exponential Distribution yields a continuous memoryless model..}{}{theorem-56}%
\hypertarget{p-977}{}%
If X has an exponential distribution and a and b are nonnegative integers, then%
\begin{equation*}
P( X > a + b | X > b ) = P( X > a)
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-58}{}
\hypertarget{p-978}{}%
Using the definition of conditional probability,%
\begin{align*}
P( X > a + b | X > b ) & = P( X > a + b \cap X > b ) \ P( X > b)\\
& = P( X > a + b ) / P( X > b)\\
& = e^{-(a+b)/ \mu} / e^{-b / \mu}\\
& = e^{-a/ \mu}\\
& = P(X > a)\qedhere
\end{align*}
%
\end{proof}
%
%
%
\typeout{************************************************}
\typeout{Section 8.4 Gamma Distribution}
\typeout{************************************************}
%
\section[{Gamma Distribution}]{Gamma Distribution}\label{section-54}
\hypertarget{p-979}{}%
Extending the exponential distribution model developed above, consider a Poisson Process where you start with an interval of variable length X so that X measures the interval needed in order to obtain the rth success for some natural number r. Then \(R = (0,\infty)\) and the resulting distribution of X will be called a Gamma distribution.%
\par
\hypertarget{p-980}{}%
\begin{definition}{Gamma Function.}{definition-36}%
\hypertarget{p-981}{}%
%
\begin{equation*}
\Gamma(t) = \int_0^{\infty} u^{t-1} e^{-u} du
\end{equation*}
%
\end{definition}
%
\par
\hypertarget{p-982}{}%
\begin{theorem}{Gamma Function on the natural numbers.}{}{theorem-57}%
\hypertarget{p-983}{}%
For \(n \in \mathbb{N}\),%
\begin{equation*}
\Gamma(n+1) = n!
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-59}{}
\hypertarget{p-984}{}%
Letting n be a natural number and applying integration by parts one time gives%
\begin{align*}
\Gamma(n+1) & = \int_0^{\infty} u^n e^{-u} du\\
& = -u^n \cdot e^{-u} \big |_0^{\infty} + n \int_0^{\infty} u^{n-1} e^{-u} du \\
& = 0 - 0 + n \Gamma(n)
\end{align*}
Continuing using an inductive argument to obtain the final result.%
\end{proof}
%
\par
\hypertarget{p-985}{}%
To find the probability function for the gamma distribution, once again focus on the development of F(x). Assuming r is a natural number greater than 1 and noting that X measures the interval length needed in order to achieve the rth success%
\begin{align*}
F(x) & = P(X \le x)\\
& = 1 - P(X \gt x)\\
& = 1 - P(\text{fewer than r successes in [0,x]})\\
& = 1 - \big [ \frac{(\lambda x)^0 e^{-\lambda x}}{0!} + \frac{(\lambda x)^1 e^{-\lambda x}}{1!} + ... + \frac{(\lambda x)^{r-1} e^{-\lambda x}}{(r-1)!} \big ]\\
& = 1 - \sum_{k=0}^{r-1} \frac{(\lambda x)^k e^{-\lambda x}}{k!} 
\end{align*}
where the discrete Poisson probability function is used on the interval [0,x]. The derivative of this function however is "telescoping" and terms cancel. Indeed,%
\begin{align*}
F'(x) & = \lambda e^{-\lambda x}/0!\\
& - \lambda e^{-\lambda x}/1! + \lambda x \cdot \lambda e^{-\lambda x}/1!\\
& - \lambda^2 2x e^{-\lambda x}/2! + \lambda^2 x^2 \cdot \lambda e^{-\lambda x}/2!\\
& - \lambda^3 3x^2 e^{-\lambda x}/3! + \lambda^3 x^3 \cdot \lambda e^{-\lambda x}/3!\\
& . . .\\
& - \lambda^{r-1} (r-1)x^{r-2} e^{-\lambda x}/(r-1)! + \lambda^{r-1} x^{r-1} \cdot \lambda e^{-\lambda x}/(r-1)!\\
& = \lambda^r x^{r-1} e^{-\lambda x}/(r-1)!
\end{align*}
where you can replace \((r-1)! = \Gamma(r)\).%
\par
\hypertarget{p-986}{}%
Notice that for this random variable, \(\mu = \lambda T\) can be obtained for the exponential distribution. For the Gamma distribution, the following takes \(\mu\) to be the average interval till the first success and then modifies the corresponding Gamma parameters according to increasing values of r.%
\par
\hypertarget{p-987}{}%
\begin{definition}{Gamma Distribution Probability Function.}{definition-37}%
\hypertarget{p-988}{}%
If X measures the interval until the rth success and \(\mu as the average interval until the 1st success\), then X with probability function%
\begin{equation*}
f(x) =  \frac{x^{r-1} \cdot e^{-x / \mu}}{\Gamma(r) \cdot \mu^r}
\end{equation*}
has a Gamma Distribution.%
\end{definition}
%
\par
\hypertarget{p-989}{}%
\begin{theorem}{Verify Gamma Probability function.}{}{theorem-58}%
\hypertarget{p-990}{}%
%
\begin{equation*}
\int_0^{\infty} \frac{x^{r-1} e^{-x/ \mu}}{\Gamma(r) \mu^r} dx = 1
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-60}{}
\hypertarget{p-991}{}%
Evaluate the sage code below.%
\end{proof}
%
\par
\hypertarget{p-992}{}%
\leavevmode%
\begin{sageinput}
# Gamma Distribution
var('x,mu,r')
assume(mu>0)
assume(r,'integer')
assume(r>1)
f(x) =x^(r-1)*e^(-x/mu)/(gamma(r)*mu^r)
S = integral(f,x,0,oo).full_simplify()
F = '$ \int_0^{\infty} \\frac{x^{r-1} e^{-x/ \mu}}{\Gamma(r) \mu^r} dx = %s$'%str(S)
html(F)
\end{sageinput}
%
\par
\hypertarget{p-993}{}%
\leavevmode%
\begin{sageinput}
# Gamma Distribution Graphing
var('x,mu,r')
assume(mu>0)
assume(r,'integer')
@interact
def _(r=[2,3,6,12,24],mu=slider(1,12,1,5,label='mu')):
    f(x) =x^(r-1)*e^(-x/mu)/(gamma(r)*mu^r)
    plot(f,x,0,200).show()
\end{sageinput}
%
\par
\hypertarget{p-994}{}%
Derivation of mean, variance, skewness, and kurtosis. Pick "alpha" for the general formulas. \leavevmode%
\begin{sageinput}
# Gamma Distribution
var('x,mu,r,alpha')
assume(mu>0)
assume(alpha,'integer')
assume(alpha>1)
@interact
def _(r=[2,3,6,9,alpha]):
    f(x) =x^(r-1)*e^(-x/mu)/(gamma(r)*mu^r)
    mean = integral(x*f,x,0,oo).full_simplify()
    M2 = integral(x^2*f,x,0,oo).full_simplify()
    M3 = integral(x^3*f,x,0,oo).full_simplify()
    M4 = integral(x^4*f,x,0,oo).full_simplify()
    
    pretty_print('Mean = ',mean)
    
    v = (M2-mean^2).factor()
    pretty_print('Variance = ',v)
    stand = sqrt(v)
    
    sk = (((M3 - 3*M2*mean + 2*mean^3))/stand^3).full_simplify()
    pretty_print('Skewness = ',sk)
    
    kurt = (M4 - 4*M3*mean + 6*M2*mean^2 -3*mean^4).factor()/stand^4
    pretty_print('Kurtosis = ',(kurt-3).factor(),'+3')
\end{sageinput}
%
\par
\hypertarget{p-995}{}%
Finally, the interactive cell below can be used to compute the distribution function for the gamma distribution for various input values. If you desire to let r get bigger than the slider allows, feel free to edit the cell above and evaluate again. \leavevmode%
\begin{sageinput}
# Gamma Distribution Calculator
var('x,mu,r')
pretty_print('Enter the number of successes desired, the given mean, and the value of X to get F(X)')
@interact
def _(r=slider(1,10,1,2),mu = input_box(2,label="$\mu = $",width=10),b=input_box(2,label="X = ",width=10)):
    f(x) =x^(r-1)*e^(-x/mu)/(gamma(r)*mu^r)
    p = integral(f,x,0,b)
    
    pretty_print('Probability = \t',p,' which is approximately \t',p.n(digits=5))
\end{sageinput}
%
%
%
\typeout{************************************************}
\typeout{Section 8.5 Summary}
\typeout{************************************************}
%
\section[{Summary}]{Summary}\label{section-55}
\hypertarget{p-996}{}%
Here is a summary of the major points in this chapter:%
\hypertarget{p-997}{}%
TBA%
%
%
\typeout{************************************************}
\typeout{Section 8.6 Exercises}
\typeout{************************************************}
%
\section[{Exercises}]{Exercises}\label{section-56}
\hypertarget{p-998}{}%
\begin{inlineexercise}{- Home Sales.}{exercise-66}%
\hypertarget{p-999}{}%
A local realty office sells on average 10 houses a week.  Let X measure the number of houses the sell in the next week.  Determine \leavevmode%
\begin{enumerate}
\item\hypertarget{li-295}{}the probability the realty office sells 12 houses next week.%
\item\hypertarget{li-296}{}the probability the realty office sells fewer than 10 houses next week.%
\item\hypertarget{li-297}{}the interval \(\mu - 2\sigma \le X \le \mu + 2\sigma\).%
\item\hypertarget{li-298}{}\(P(\mu - 2\sigma \le X \le \mu + 2\sigma)\).%
\end{enumerate}
%
\end{inlineexercise}
%
\par
\hypertarget{p-1000}{}%
\begin{inlineexercise}{- Customer arrivals - Total.}{exercise-67}%
\hypertarget{p-1001}{}%
Customers arrive at your store on the average of 10 per hour.  Assuming that the arrival of customers satisfies the properties of a Poisson process, determine: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-299}{}the expected number of customers to arrive in a given 3 hour period.%
\item\hypertarget{li-300}{}the probability that fewer than 10 customers arrive in a given hour.%
\end{enumerate}
 \textbf{Solution}.\quad%
\hypertarget{p-1002}{}%
Using the given information, apply the Poisson distribution.  For one hour \(\mu = 10\) so that for three hours the expected number of customers would be triple with 30 expected customers.%
\par
\hypertarget{p-1003}{}%
With \(\mu_1 = 10\),%
\begin{align*}
P(X \lt 10) & = F(9)\\
& = \frac{10^0 e^{-10}}{0!} + \frac{10^1 e^{-10}}{1!} + \frac{10^2 e^{-10}}{2!} + ... + \frac{10^8 e^{-10}}{8!} + \frac{10^9 e^{-10}}{9!}\\
& = e^{-10} \cdot ( 1 + 10 + \frac{100}{2} + ... + \frac{10^8}{8!} + \frac{10^9}{9!} )
\end{align*}
%
%
\end{inlineexercise}
%
\par
\hypertarget{p-1004}{}%
\begin{inlineexercise}{- Customer arrivals - First.}{exercise-68}%
\hypertarget{p-1005}{}%
Customers arrive at your store on the average of 10 per hour.  Assuming that the arrival of customers satisfies the properties of a Poisson process, determine: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-301}{}the number of minutes expected between the arrival of each customer%
\item\hypertarget{li-302}{}the probability it takes more than 9 minutes before the next customer arrives.%
\end{enumerate}
 \textbf{Solution}.\quad%
\hypertarget{p-1006}{}%
Using the given information, apply the exponential distribution.  Since 10 arrive on average in one hour then you would expect 1 to arrive in 6 minutes.%
\par
\hypertarget{p-1007}{}%
With \(\mu = 6\) minutes,%
\begin{equation*}
P(X \gt 9) = 1 - F(9) = e^{-9/6}.
\end{equation*}
%
%
\end{inlineexercise}
%
\par
\hypertarget{p-1008}{}%
\begin{inlineexercise}{- Customer arrivals - 10th.}{exercise-69}%
\hypertarget{p-1009}{}%
Customers arrive at your store on the average of 10 per hour.  Assuming that the arrival of customers satisfies the properties of a Poisson process, determine: \leavevmode%
\begin{enumerate}
\item\hypertarget{li-303}{}the number of minutes expected for the arrival of three customers.%
\item\hypertarget{li-304}{}the probability it takes less than 20 minutes before the third customer arrives.%
\end{enumerate}
 \textbf{Solution}.\quad%
\hypertarget{p-1010}{}%
Using the given information, apply the gamma distribution.  Since 10 arrive on average in one hour then you would expect 1 to arrive in 6 minutes and therefore 3 to arrive on average in 18 minutes.%
\par
\hypertarget{p-1011}{}%
With \(\mu = 18\) minutes,%
\begin{equation*}
P(X \lt 20) = F(20).
\end{equation*}
%
%
\end{inlineexercise}
%
\par
\hypertarget{p-1012}{}%
\begin{inlineexercise}{- Computer Network Data Traffic.}{exercise-70}%
\hypertarget{p-1013}{}%
Consider the arrival of requests on a server. Presume that the requests are considered as coming from an anonymous and large collection of users independently of each other on an average of 50 requests per second. If X measures the number of requests per second, determine \leavevmode%
\begin{enumerate}
\item\hypertarget{li-305}{}the probability that in any given second the server gets fewer than 50 requests%
\item\hypertarget{li-306}{}\(P(\mu - 2\sigma \le X \le \mu + 2\sigma)\)%
\item\hypertarget{li-307}{}the expected number of requests per hour.%
\end{enumerate}
 \textbf{Solution}.\quad%
\hypertarget{p-1014}{}%
Given the average of 50 requests per second and X measuring the number of "successes" in a given second long time interval given a Poission distribution%
\begin{equation*}
f(x) = \frac{50^x}{x!}e^{-50}.
\end{equation*}
Then,%
\begin{equation*}
P(X \lt 50) = F(49) = \sum_{x=0}^{49} \frac{50^x}{x!} e^{-50}
\end{equation*}
and using the graphing calculator function poissoncdf(50,49) = 0.48119.%
\par
\hypertarget{p-1015}{}%
For a time interval of one second, the mean is given to be 50 requests. Using the formulas developed above, the standard deviation therefore is \(\sqrt{50}\). Therefore%
\begin{align*}
P(\mu - 2\sigma \le X \le \mu + 2\sigma) & = P(50 - 2\sqrt{50} \le X \le 50 + 2\sqrt{50})\\
& = P(X \in \{ 36, 37, 38, ..., 62, 63, 64 \}).
\end{align*}
Using the distribution function,%
\begin{equation*}
F(64) - F(35) \approx 0.97640 - 0.01621 = 0.96019
\end{equation*}
%
\par
\hypertarget{p-1016}{}%
Finally, notice that the time interval has been adjusted. Since the mean formula is proportional to the interval over which X is measured, using \(\mu = \lambda T\) with \(\lambda = 1\) when the interval is 1 second, then when the interval is one hour, T = 3600 seconds. Hence, we would expect on average \(50 \cdot 3600 = 180,000\) requests in one hour.%
%
\end{inlineexercise}
%
%
%
\typeout{************************************************}
\typeout{Chapter 9 Normal Distributions}
\typeout{************************************************}
%
\chapter[{Normal Distributions}]{Normal Distributions}\label{Normal}
%
%
\typeout{************************************************}
\typeout{Section 9.1 Introduction}
\typeout{************************************************}
%
\section[{Introduction}]{Introduction}\label{section-57}
\hypertarget{p-1017}{}%
You should have noticed by now that many distributions tend to have a bell-shaped graph as parameters are allowed to increase. Indeed, the formulas for skewness \(\gamma_1\) and kurtosis \(\gamma_2\) approach 0 and 3 respectively for the Hypergeometric, Binomial, Negative Binomial, Poisson, and Gamma Distributions. One might wonder if this is just a happy coincidence or is something more insidious at play.%
\par
\hypertarget{p-1018}{}%
The answer by appealing to mathematics reveals that nothing sinister is going on but that it is indeed true that the eventual destiny for distributions is one that is bell-shaped. It is therefore of interest to figure out if that distribution has a nice form that can be accessed directly. The focus of this chapter is to consider this bell-shaped goal known as the "normal distribution."%
\par
\hypertarget{p-1019}{}%
We present the normal distribution by simply presenting it's probability function without derivation. In order to more carefully investigate the development of the normal distribution (and the Chi-Square Distribution) you will need to study "Moment Generating Functions" and some serious mathematics. Without supplying this rigor you can still utilize the results.%
%
%
\typeout{************************************************}
\typeout{Section 9.2 The Normal Distribution}
\typeout{************************************************}
%
\section[{The Normal Distribution}]{The Normal Distribution}\label{section-58}
\hypertarget{p-1020}{}%
\begin{definition}{The Normal Distribution.}{definition-38}%
\hypertarget{p-1021}{}%
Given two parameters \(\mu\) and \(\sigma\), a random variable X over \(R = (-\infty,\infty)\) has a normal distribution provided it has a probability function given by%
\begin{equation*}
f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{ -\left ( \frac{x-\mu}{\sigma} \right ) ^2 / 2}
\end{equation*}
%
\end{definition}
%
\par
\hypertarget{p-1022}{}%
The normal distribution is also sometimes referred to as the Gaussian Distribution (often by Physicists) or the Bell Curve (often by social scientists).%
\par
\hypertarget{p-1023}{}%
\leavevmode%
\begin{sageinput}
var('x,mu,sigma')
f(x) = e^(-((x-mu)/sigma)^2/2)/(sigma*sqrt(2*pi))
@interact
def _(m=slider(-10,10,1,0,label='$\mu$'),s=slider(1/5,5,1/10,1,label='$\sigma$')):
    titletext = "Normal Curve with mean "+str(m)+" and standard deviation "+str(s)
    G = plot(f(mu=m,sigma=s),(x,m-5*s,m+5*s))
    G += point((0,1),size=1)+point((12,0),size=1)+point((-12,0),size=1)
    G += point((m,f(x=m,mu=m,sigma=s)),color='red',size=20)
    G += point((m+s,f(x=m+s,mu=m,sigma=s)),color='green',size=20)
    G += point((m-s,f(x=m-s,mu=m,sigma=s)),color='green',size=20)    
    show(G,figsize=(5,3),title=titletext,ymin=0,ymax=1,xmin=-15,xmax=15)
\end{sageinput}
%
\par
\hypertarget{p-1024}{}%
\begin{theorem}{}{}{theorem-59}%
\hypertarget{p-1025}{}%
If \(\mu = 0\) and \(\sigma = 1\), then we say X has a standard normal distribution and often use Z as the variable name and will use \(\Phi(z)\) for the standard normal distribution function. In this case, the density function reduces to%
\begin{equation*}
f(z) = \frac{1}{\sqrt{2 \pi}} e^{ -z^2 / 2}
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-61}{}
\hypertarget{p-1026}{}%
Convert to "standard units" using the conversion%
\begin{equation*}
z = \frac{x-\mu}{\sigma} = \frac{x-0}{1} = x.\qedhere
\end{equation*}
%
\end{proof}
%
\par
\hypertarget{p-1027}{}%
\begin{theorem}{Verifying the normal probability function.}{}{theorem-60}%
\hypertarget{p-1028}{}%
%
\begin{equation*}
\int_{-\infty}^{\infty} \frac{1}{\sigma \sqrt{2 \pi}} e^{ -\left ( \frac{x-\mu}{\sigma} \right ) ^2 / 2} dx = 1
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-62}{}
\hypertarget{p-1029}{}%
Note that you can convert the integral above to {(((Unresolved xref, reference "StandardUnitConversion"; check spelling or use "provisional" attribute)))}\hyperlink{}{standard units~} so that it is sufficient to show%
\begin{equation*}
I = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{ -\frac{z^2}{2} } dz = 1
\end{equation*}
Toward this end, consider \(I^2\) and change the variables to get%
\begin{align*}
I^2 & = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{ -\frac{u^2}{2} } du \cdot \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{ -\frac{v^2}{2} } dv\\
& = \frac{1}{2 \pi} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{ -\frac{u^2+v^2}{2} } du dv
\end{align*}
Converting to polar coordinates using%
\begin{equation*}
du dv = r dr d\theta 
\end{equation*}
and%
\begin{equation*}
u^2 + v^2 = r^2
\end{equation*}
gives%
\begin{align*}
I^2 & = \frac{1}{2 \pi} \int_0^{2 \pi} \int_0^{\infty} e^{ -\frac{r^2}{2} } r dr d\theta\\
& = \frac{1}{2 \pi} \int_0^{2 \pi} -e^{ -\frac{r^2}{2} } \big |_0^{\infty} d\theta\\
& = \frac{1}{2 \pi} \int_0^{2 \pi} 1 \cdot d\theta\\
& = \frac{1}{2 \pi} \theta \big |_0^{2 \pi} = 1
\end{align*}
as desired.%
\end{proof}
%
\par
\hypertarget{p-1030}{}%
\begin{theorem}{Verifying the normal probability mean.}{}{theorem-61}%
\hypertarget{p-1031}{}%
%
\begin{equation*}
E[X] = \int_{-\infty}^{\infty} x \cdot \frac{1}{\sigma \sqrt{2 \pi}} e^{ - \left ( \frac{x-\mu}{\sigma} \right ) ^2 / 2} dx = \mu
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-63}{}
\hypertarget{p-1032}{}%
%
\begin{equation*}
z = \frac{x-\mu}{\sigma}
\end{equation*}
implies by solving that%
\begin{equation*}
x = \mu + z \sigma
\end{equation*}
and therefore%
\begin{align*}
E[X] &= \int_{-\infty}^{\infty} x \cdot \frac{1}{\sigma \sqrt{2 \pi}} e^{ - \left ( \frac{x-\mu}{\sigma} \right ) ^2 / 2} dx \\
&= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} (\mu + z\sigma) \cdot e^{ -z^2 / 2} dz\\
&= \mu \cdot \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{ -z^2 / 2} dz + \sigma \cdot \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} z \cdot e^{ -z^2 / 2} dz\\
&= \mu \cdot 1 + \sigma \cdot 0\\
& = \mu
\end{align*}
and therefore the use of \(\mu\) is warranted.%
\end{proof}
%
\par
\hypertarget{p-1033}{}%
\begin{theorem}{Verifying the normal probability variance.}{}{theorem-62}%
\hypertarget{p-1034}{}%
%
\begin{equation*}
E[(X-\mu)^2] = \int_{-\infty}^{\infty} (x-\mu)^2 \cdot \frac{1}{\sigma \sqrt{2 \pi}} e^{ - \left ( \frac{x-\mu}{\sigma} \right ) ^2 / 2} dx = \sigma^2
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-64}{}
\hypertarget{p-1035}{}%
%
\begin{align*}
E[(X-\mu)^2] & = \int_{-\infty}^{\infty} (x-\mu)^2 \cdot \frac{1}{\sigma \sqrt{2 \pi}} e^{ - \left ( \frac{x-\mu}{\sigma} \right ) ^2 / 2} dx\\
& = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} \sigma^2 z^2 \cdot  e^{ -z^2 / 2} dz\\
& = \frac{\sigma^2}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} z \cdot z e^{ -z^2 / 2} dz\\
& = \frac{\sigma^2}{\sqrt{2 \pi}} \cdot \big [ -z e^{-z^2 / 2} \big |_{-\infty}^{\infty} + \int_{-\infty}^{\infty}  e^{ -z^2 / 2}  dz \big ]\\
& = \frac{\sigma^2}{\sqrt{2 \pi}} \cdot \big [ 0 + \sqrt{2 \pi} \big ]\\
& = \sigma^2 
\end{align*}
using integration by parts and using the integration in the proof of the mean above.  So, the use of \(\sigma\) is warranted.%
\end{proof}
%
\par
\hypertarget{p-1036}{}%
\begin{theorem}{Properties of the Normal Distribution.}{}{theorem-63}%
\end{theorem}
%
\par
\hypertarget{p-1037}{}%
\begin{theorem}{Normal Distribution Maximum.}{}{theorem-64}%
\hypertarget{p-1038}{}%
The maximum of the normal distribution probability function occurs when \(x = \mu\)%
\end{theorem}
\begin{proof}\hypertarget{proof-65}{}
\hypertarget{p-1039}{}%
Take the derivative of the probability function to get%
\begin{equation*}
\frac{\sqrt{2} {\left(\mu - x\right)} e^{\left(-\frac{{\left(\mu - x\right)}^{2}}{2 \, \sigma^{2}}\right)}}{2 \, \sqrt{\pi} \sigma^{3}}
\end{equation*}
which is zero only when \(x = \mu\).  Easily by evaluating to the left and right of this value shows that this critical value yields a maximum.%
\end{proof}
%
\par
\hypertarget{p-1040}{}%
\begin{theorem}{Normal Distribution Points of Inflection.}{}{theorem-65}%
\hypertarget{p-1041}{}%
Points of Inflection for the normal distribution probability function occurs when \(x = \mu + \sigma\) and \(x = \mu - \sigma\).%
\end{theorem}
\begin{proof}\hypertarget{proof-66}{}
\hypertarget{p-1042}{}%
Take the second derivative of the probability function to get%
\begin{equation*}
\frac{\sqrt{2} {\left(\mu + \sigma - x\right)} {\left(\mu - \sigma - x\right)} e^{\left(-\frac{\mu^{2}}{2 \, \sigma^{2}} + \frac{\mu x}{\sigma^{2}} - \frac{x^{2}}{2 \, \sigma^{2}}\right)}}{2 \, \sqrt{\pi} \sigma^{5}}
\end{equation*}
which is zero only when \(x = \mu \pm \sigma\).  Easily by evaluating to the left and right of this value shows that these critical values yield points of inflection.%
\end{proof}
%
\par
\hypertarget{p-1043}{}%
Notice that the work needed to complete the integrals over the entire domain above was pretty serious. To determine probabilities for a given interval is however not possible in general and therefore approximations are needed. When using TI graphing calculators, you can use%
\begin{equation*}
P( a \lt x \lt b ) = \text{normalcdf}(a,b,\mu, \sigma).
\end{equation*}
Or you can use the calculator below.%
\par
\hypertarget{p-1044}{}%
\leavevmode%
\begin{sageinput}
@interact(layout=dict(top=[['a', 'b']],bottom=[['mu','sigma']]))
def _(a=input_box(-2,width=10,label='a = '),b=input_box(2,width=10,label='b = '),mu=input_box(0,width=8,label='$\mu = $'),sigma=input_box(1,width=8,label='$\sigma = $')):
    f = e^(-((x-mu)/sigma)^2/2)/(sigma*sqrt(2*pi))
    P = integral_numerical(f,a,b)[0]
    print "P("+str(a)+" < X < "+str(b)+") ~= "+str(P)
\end{sageinput}
%
%
%
\typeout{************************************************}
\typeout{Section 9.3 Chi-Square Distribution}
\typeout{************************************************}
%
\section[{Chi-Square Distribution}]{Chi-Square Distribution}\label{section-59}
\hypertarget{p-1045}{}%
The following distribution is related to both the Normal Distribution and to the Gamma Distribution. Initially, consider a gamma distribution with probability function%
\begin{equation*}
\frac{x^{r-1} \cdot e^{-x / \mu}}{\Gamma(r) \cdot \mu^r}.
\end{equation*}
Replacing \(\mu = 2\) and r with r/2 gives%
\begin{equation*}
\frac{x^{r/2-1} \cdot e^{-x/2}}{\Gamma(r/2) \cdot 2^{r/2}}
\end{equation*}
which is given a special name below.%
\par
\hypertarget{p-1046}{}%
\begin{definition}{Chi-Square Probability Function.}{definition-39}%
\(R = (0,\infty)\)%
\begin{equation*}
f(x) = \frac{x^{r/2-1} e^{-x/2} }{\Gamma(r/2) 2^{r/2}}.
\end{equation*}
\(\chi^2(r)\)\end{definition}
%
\par
\hypertarget{p-1047}{}%
\leavevmode%
\begin{sageinput}
# Chi-Square Grapher
@interact
def _(r=slider(1,20,1,3,label='r =')):
    f = x^(r/2-1)*e^(-x/2)/(gamma(r/2)*2^(r/2))
    plot(f,x,0,20).show()
\end{sageinput}
%
\par
\hypertarget{p-1048}{}%
\begin{theorem}{\(\chi^2\) statistics.}{}{theorem-66}%
%
\begin{equation*}
\mu = r
\end{equation*}
%
\begin{equation*}
\sigma^2 = 2r
\end{equation*}
%
\begin{equation*}
\gamma_1 = 2 \sqrt{2/r}
\end{equation*}
%
\begin{equation*}
\gamma_2 = \frac{12}{r} + 3
\end{equation*}
\end{theorem}
%
\par
\hypertarget{p-1049}{}%
\begin{theorem}{Relationship between Normal and \(\chi^2\).}{}{theorem-67}%
\(Z_1, Z_2, ..., Z_r\)%
\begin{equation*}
X = \sum_{k=1}^r Z_k^2
\end{equation*}
\(\chi^2(r)\)\end{theorem}
%
\par
\hypertarget{p-1050}{}%
It also can be difficult to compute Chi-Square probabilities manually so you will perhaps want to use a numerical approximation in this case as well. The TI graphing calculator can be used with \(\chi ^2\)cdf(a,b,r).  Or, you can use the calculator below.%
\par
\hypertarget{p-1051}{}%
\leavevmode%
\begin{sageinput}
# Chi-Square Calculator
@interact(layout=dict(top=[['a', 'b']],bottom=[['r']]))
def _(a=input_box(0,width=10,label='a = '),b=input_box(2,width=10,label='b = '),r=input_box(2,width=8,label='r =')):
    f = x^(r/2-1)*e^(-x/2)/(gamma(r/2)*2^(r/2))
    P = numerical_integral(f,a,b)[0]
    print "P("+str(a)+" < X < "+str(b)+") ~= "+str(P)
\end{sageinput}
%
%
%
\typeout{************************************************}
\typeout{Section 9.4 Other "Bell Shaped" distributions}
\typeout{************************************************}
%
\section[{Other "Bell Shaped" distributions}]{Other "Bell Shaped" distributions}\label{section-60}
\hypertarget{p-1052}{}%
The Normal distribution discussed above is very important when doing statistical analysis. It however is not the only distribution that is symmetrical about the mean and looks like a bell.  In this section, we consider two other options--one which is virtually useless and another which is very useful.%
\par
\hypertarget{p-1053}{}%
\begin{definition}{The Cauchy Distribution.}{definition-40}%
\hypertarget{p-1054}{}%
Consider a continuous random variable on the real numbers defined by%
\begin{equation*}
f(x) = \frac{1/\pi}{1+x^2}.
\end{equation*}
A random variable with this probability function is said to be a Cauchy Distribution.%
\end{definition}
%
\begin{theorem}{The Cauchy Distribution.}{}{theorem-68}%
%
\begin{equation*}
f(x) = \frac{1/\pi}{1+x^2}
\end{equation*}
\((-\infty, \infty)\)\end{theorem}
\begin{proof}\hypertarget{proof-67}{}
%
\begin{equation*}
\int_{-\infty}^{\infty} \frac{1}{1+x^2} dx = arctan(\infty) - arctan(-\infty) = \pi/2 - (-\pi/2) = \pi.
\end{equation*}
\(\pi\)\end{proof}
\begin{sageinput}
n &- 10
p &- 0.3

paste('Probability Function')
dcauchy(x, location = 0, scale = 1, log = FALSE)   # gives the probability function
paste('Distribution function')
pcauchy(q, location = 0, scale = 1, lower.tail = TRUE, log.p = FALSE)
   # gives the distribution function
paste('A random sample')
rcauchy(n, location = 0, scale = 1)    # gives a random sample of 15 items from b(n,p)

x=seq(-4,4,length=200)
y=dcauchy(x,dcauchy(x, location = 0, scale = 1, log = FALSE))
plot(x,y,type="l",lwd=2,col="red",ylab="p")
\end{sageinput}
\hypertarget{p-1055}{}%
Now that we have a probability function, it is important to determine its mean and variance. It should be obvious that when doing so using the Cauchy probability function, problems quickly arise.  Indeed,%
\begin{equation*}
\int_{-\infty}^{\infty} x \frac{1}{1+x^2} dx = (1/2) ( \ln( | \infty |) - \ln( | -\infty |)
\end{equation*}
which is problematic.  Further, for the variance%
\begin{equation*}
\int_{-\infty}^{\infty} x^2 \frac{1}{1+x^2} dx 
\end{equation*}
and note that the integrand does not converge to 0 at the endpoints and therefore the integral is automatically considered divergent.  Thus it is reasonable to note that the Cauchy distribution has no variance.%
\par
\hypertarget{p-1056}{}%
On the other hand, there is another bell-shaped distribution that is useful and its random variable can be created by using a mixture of a normal variable and a Cauchy variable. Indeed, suppose Z is a standard normal variable and Y is \(\Chi^2(r)\) with Y and Z independent.  Define a new random variable%
\begin{equation*}
T = \frac{Z}{\sqrt(Y/r)}.
\end{equation*}
Then, T is said to have a (Student) t distribution.  The good news is that this distribution is useful and its statistics are presented below without proof. \begin{theorem}{Student t-distribution.}{}{theorem-69}%
%
\begin{equation*}
\mu = 0
\end{equation*}
%
\begin{equation*}
\sigma^2 = \frac{r}{r-2}.
\end{equation*}
\end{theorem}
%
\begin{sageinput}
# Display the Student's t distributions with various
# degrees of freedom and compare to the normal distribution
# Copied from www.statmethods.net

x <- seq(-4, 4, length=100)
hx <- dnorm(x)

degf <- c(1, 3, 8, 30)
colors <- c("red", "blue", "darkgreen", "gold", "black")
labels <- c("df=1", "df=3", "df=8", "df=30", "normal")

plot(x, hx, type="l", lty=2, xlab="x value",
  ylab="Density", main="Comparison of t Distributions")

for (i in 1:4){
  lines(x, dt(x,degf[i]), lwd=2, col=colors[i])
}

legend("topright", inset=.05, title="Distributions",
  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)
\end{sageinput}
%
%
\typeout{************************************************}
\typeout{Section 9.5 Normal Distribution as a Limiting Distribution}
\typeout{************************************************}
%
\section[{Normal Distribution as a Limiting Distribution}]{Normal Distribution as a Limiting Distribution}\label{section-61}
\hypertarget{p-1057}{}%
Over the past several chapters you should have noticed that many distributions have skewness and kurtosis formulae which have limiting values of 0 and 3 respectively. This means that each of those distributions which can be approximated by the normal distribution for "large" parameter values.%
\par
\hypertarget{p-1058}{}%
To see how this works, consider a "random" distribution in the following two interactive experiments.  For the first graph below, a sequence of N random samples, each of size r, ranging from 0 to "Range" is generated and graphed as small data points.  As the number of samples N and the sample size r increase, notice that the data seems to cover the entire range of possible values relatively uniformly.  (For this scatter plot note that each row represents the data for one sample of size r.  The larger the N, the greater the number of rows.)  Each row is averaged and that mean value is plotted on the graph as a red circle.  If you check the "Show_Mean" box, the mean of these circles is indicated by the green line in the middle of the plot.%
\par
\hypertarget{p-1059}{}%
For the second graph below, the means are collected and the relative frequency of each is plotted.  As N increases, you should see that the results begin to show an interesting tendency.   As you increase the data range, you may notice this graph has a larger number of data values.  Smoothing groups this data into intervals of length two for perhaps a graph with less variability.%
\par
\hypertarget{p-1060}{}%
Consider each of the following: \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}As N increases with single digit values of r, what appears to happen to the mean and range of the means?  How does increasing the data range from 1-100 to 1-200 or 1-300 affect these results?%
\item{}As N increases (say, for a middle value of r), what appears to happen to the means?  How does increasing the data range from 1-100 to 1-200 or 1-300 affect these results?%
\item{}As r increases (say, for a middle value of N), what appears to happen to the range of the averages?  Does your conclusion actually depend upon the value of N?  (Look at the graph and don't worry about the actual numerical values.) How does increasing N for the second graph affect the skewness and kurtosis of that graph?  Do things change significantly as r is increased?%
\end{itemize}
%
\begin{sageinput}
var('n,k')
from sage.finance.time_series import TimeSeries

@interact(layout=dict(top=[['Range'],['Show_Mean', 'Smoothing']],  
bottom=[['N'],['r']]))

def _(Range=[100,200,300,500],N=slider(5,200,2,2,label="N = Number of Samples"),r=slider(3,200,1,2,label="r = Sample Size"),Show_Mean=False,Smoothing=False):
    R=[1..N]     #  R ranges over the number of samples...will point to the list of averages
    rangemax = Range

    data = random_matrix(ZZ,N,r,x=rangemax)
    datapoints = []
    avg_values = []
    avg_string = []
    averages = []
    for n in range(N):
        temp = 0
        for k in range(r):
            datapoints += [(data[n][k],n)]
            temp += data[n][k]
        avg_values.append(round(temp/r))
        if Smoothing:
            avg_string.append(str(2*round((temp/r)/2)))    
        else:
            avg_string.append(str(round(temp/r)))
            
        averages += [(round(temp/r),n)]   #  make these averages integers for use in grouping later
    SCAT = scatter_plot(datapoints,markersize=2,edgecolor='red',figsize=(10,4),axes_labels=['Sample Values','Sample Number'])
    AVGS = scatter_plot(averages,markersize=50,edgecolor='blue',marker='o',figsize=(7,4))
    
    freqslist = frequency_distribution(avg_string,1).function().items()
       
       
# compute sample statistics for the raw data as well as for the N averages
    Mean_data = (sum(sum(data))/(N*r)).n()
#    STD_data = sqrt(sum(sum( (data-Mean_data)^2 ))/(N*r)).n()
    Mean_averages = mean(avg_values).n()
#    STD_averages = sqrt(variance(avg_values).n())
#    print "Data mean =",Mean_data," vs Mean of the averages =",Mean_averages
#    print "Data STD = ",STD_data," vs Standard Dev of avgs =", STD_averages
    if Show_Mean:
        avg_line = line([(Mean_data,0),(Mean_data,N-1)],rgbcolor='green',thickness=10)
        avg_text = text('xbar',(Mean_data,N),horizontal_alignment='right',rgbcolor='green')
    else:
        avg_line = Graphics()
        avg_text = Graphics()
            
#  Plot a scatter plot exhibiting uniformly random data and the collection of averages 
    print(html("The random data plot on the left with each row representing a sample with size determined by\n"+
         "the slider above and each circle representing the average for that particular sample.\n"+
         "First, keep sample size relatively low and increase the number of samples.  Then, \n"+
         "watch what happens when you slowly increase the sample size."))

    
#  Plot the relative frequencies of the grouped sample averages
    print(html("Now, the averages (ie. the circles) from above are collected and counted\n"+
         "with the relative frequency of each average graphed below.  For a relatively large number of\n"+
         "samples, notice what seems to happen to these averages as the sample size increases."))
    if Smoothing:
        binRange = Range//2
    else:
        binRange = Range
    
    # normed=True  # if you want to have relative frequencies below
    
    his_low = 2*rangemax/7
    his_high = 5*rangemax/7
    
    T = histogram(avg_values,normed=False,bins=binRange,range=(his_low,his_high),axes_labels=['Sample Averages','Frequency']) 
    #T = TimeSeries(avg_values).plot_histogram(axes_labels=['Sample Averages','Frequency'])   
    
    pretty_print('Scatter Plot of random data.  Horizontal is number of samples.')
    (SCAT+AVGS+avg_line+avg_text).show()
    pretty_print('Histogram of Sample Averages')
    T.show(figsize=(5,2))
\end{sageinput}
\begin{sageinput}
var('n,k')
from sage.finance.time_series import TimeSeries

@interact(layout=dict(top=[['Range'],['Show_Mean', 'Smoothing']],  
bottom=[['N'],['r']]))

def _(Range=[100,200,300,500],N=slider(5,200,2,2,label="N = Number of Samples"),r=slider(3,200,1,2,label="r = Sample Size"),Show_Mean=False,Smoothing=False):
    R=[1..N]     #  R ranges over the number of samples...will point to the list of averages
    rangemax = Range

    data = random_matrix(ZZ,N,r,x=rangemax)
    datapoints = []
    avg_values = []
    avg_string = []
    averages = []
    for n in range(N):
        temp = 0
        for k in range(r):
            datapoints += [(data[n][k],n)]
            temp += data[n][k]
        avg_values.append(round(temp/r))
        if Smoothing:
            avg_string.append(str(2*round((temp/r)/2)))    
        else:
            avg_string.append(str(round(temp/r)))
            
        averages += [(round(temp/r),n)]   #  make these averages integers for use in grouping later
    SCAT = scatter_plot(datapoints,markersize=2,edgecolor='red',figsize=(10,4),axes_labels=['Sample Values','Sample Number'])
    AVGS = scatter_plot(averages,markersize=50,edgecolor='blue',marker='o',figsize=(7,4))
    
    freqslist = frequency_distribution(avg_string,1).function().items()
       
       
# compute sample statistics for the raw data as well as for the N averages
    Mean_data = (sum(sum(data))/(N*r)).n()
#    STD_data = sqrt(sum(sum( (data-Mean_data)^2 ))/(N*r)).n()
    Mean_averages = mean(avg_values).n()
#    STD_averages = sqrt(variance(avg_values).n())
#    print "Data mean =",Mean_data," vs Mean of the averages =",Mean_averages
#    print "Data STD = ",STD_data," vs Standard Dev of avgs =", STD_averages
    if Show_Mean:
        avg_line = line([(Mean_data,0),(Mean_data,N-1)],rgbcolor='green',thickness=10)
        avg_text = text('xbar',(Mean_data,N),horizontal_alignment='right',rgbcolor='green')
    else:
        avg_line = Graphics()
        avg_text = Graphics()
            
#  Plot a scatter plot exhibiting uniformly random data and the collection of averages 
    print(html("The random data plot on the left with each row representing a sample with size determined by\n"+
         "the slider above and each circle representing the average for that particular sample.\n"+
         "First, keep sample size relatively low and increase the number of samples.  Then, \n"+
         "watch what happens when you slowly increase the sample size."))

    
#  Plot the relative frequencies of the grouped sample averages
    print(html("Now, the averages (ie. the circles) from above are collected and counted\n"+
         "with the relative frequency of each average graphed below.  For a relatively large number of\n"+
         "samples, notice what seems to happen to these averages as the sample size increases."))
    if Smoothing:
        binRange = Range//2
    else:
        binRange = Range
    
    # normed=True  # if you want to have relative frequencies below
    
    his_low = 2*rangemax/7
    his_high = 5*rangemax/7
    
    T = histogram(avg_values,normed=False,bins=binRange,range=(his_low,his_high),axes_labels=['Sample Averages','Frequency']) 
    #T = TimeSeries(avg_values).plot_histogram(axes_labels=['Sample Averages','Frequency'])   
    
    pretty_print('Scatter Plot of random data.  Horizontal is number of samples.')
    (SCAT+AVGS+avg_line+avg_text).show()
    pretty_print('Histogram of Sample Averages')
    T.show(figsize=(5,2))
\end{sageinput}
\hypertarget{p-1061}{}%
So, even with random data, if you are to consider the arrangement of the collected means rather than the arrangement of the actual data then the means appear to have a bell-shaped distribution as well.%
%
%
\typeout{************************************************}
\typeout{Section 9.6 Central Limit Theorem}
\typeout{************************************************}
%
\section[{Central Limit Theorem}]{Central Limit Theorem}\label{section-62}
\hypertarget{p-1062}{}%
Often, when one wants to solve various scientific problems, several assumptions will be made regarding the nature of the underlying setting and base their conclusions on those assumptions.  Indeed, if one is going to use a Binomial Distribution or a Negative Binomial Distribution, an assumption on the value of p is necessary.  For Poisson and Exponential Distributions, one must know the mean.  For Normal Distributions, one must assume values for both the mean and the standard deviation.   Where do these values come from?  Often, one may perform a preliminary study and obtain a sample statistic...such as a sample mean or a relative frequency and use these values for μ or p.%
\par
\hypertarget{p-1063}{}%
But what is the underlying distribution of these sample statistics?  The Central Limit Theorem gives the answer...%
\par
\hypertarget{p-1064}{}%
The results from the previous section illustrate the tendency for bell-shaped distributions. This tendency can be described more mathematically through the following theorem. It is presented here without proof.%
\begin{theorem}{Central Limit Theorem.}{}{theorem-70}%
\hypertarget{p-1065}{}%
Presume X is a random variable from a distribution with known mean \(\mu\) and known variance \(\sigma_x^2\). For some natural number n, sample the distribution repeatedly creating a string of random variables denoted \(X_1, X_2, ... , X_n\) and set \(\overline{X} = \frac{\sum X_k}{n}\).%
\par
\hypertarget{p-1066}{}%
Then, \(\overline{X}\) is approximately normally distributed with mean \(\mu\) and variance \(\sigma^2 = \frac{\sigma_x^2}{n}\).%
\end{theorem}
\hypertarget{p-1067}{}%
Often the Central Limit Theorem is stated more formally using a conversion to standard units. Indeed, the theorem indicates that the random variable \(\overline{X}\) has variance \(\frac{\sigma^2}{n}\) which means as n grows this variance approaches 0. So, the limiting random variable has a zero variance and therefore is no longer a random variable. To avoid this issue, the Central Limit Theorem is often stated as:%
\par
\hypertarget{p-1068}{}%
For random variables%
\begin{equation*}
W_n = \frac{\overline{X} - \mu}{\sigma/ \sqrt{n}}
\end{equation*}
with corresponding distribution function \(F_n(W_n)\),%
\begin{equation*}
\lim_{n \rightarrow \infty} F_n(c) = \int_{-\infty}^c \frac{1}{\sqrt{2 \pi}} e^{-z^2/2} dz = \Phi(c)
\end{equation*}
that is, the standard normal distribution function.%
\begin{example}{Exponential X vs Normal \(\overline{X}\).}{example-36}%
\hypertarget{p-1069}{}%
Consider an exponential variable X with mean time till first success of \(\mu = 4\).  Then, \(\sigma = 2\) using the exponential formulas.%
\par
\hypertarget{p-1070}{}%
You can use the exponential probability function to compute probabilities dealing with X. Indeed,%
\begin{equation*}
P(X \lt 3.9) = F(3.9) = 1 - e^{-3.9/4} \approx 0.6228 .
\end{equation*}
%
\par
\hypertarget{p-1071}{}%
If instead you plan to sample from this distribution n=32 times, the Central Limit Theorem implies that you will get a random variable \(\overline{X}\) which has an approximate normal distribution with the same mean but with new variance \(\sigma_{\overline{X}}^2 = \frac{4}{32} = \frac{1}{8}\).  Therefore%
\begin{equation*}
P( \overline{X} \lt 3.9 ) \approx normalcdf(0,3.9,4,sqrt(1/8)) = 0.2119 .
\end{equation*}
%
\end{example}
\hypertarget{p-1072}{}%
When converting probability problems from continuous (such as exponential or uniform) then no adjustment to the question is needed since you are approximating one area with another area. However, when converting probability problems from discrete (such as binomial or geometric) then you need to consider how the interval would need to be adjusted so that histogram areas for the discrete problem would relate to areas under the normal curve. Generally, you will need to expand the stated interval each way by 1/2.%
\par
\hypertarget{p-1073}{}%
The Central Limit Theorem provides that regardless of the distribution of X, the distribution of an average of X's is approximately normally distributed. However, it also shows why X may also be approximated for some distributions using the normal distribution as certain parameters are allowed to increase. Below, you can see how Binomial and Poisson distributions can be approximated directly using the Normal distribution.%
\par
\hypertarget{p-1074}{}%
Toward that end, for \(0 \lt p \lt 1\) consider a sequence of Bernoulli trials \(Y_1, Y_2, ..., Y_n\) with each over the space {0,1}. Then,%
\begin{equation*}
X = \sum_{k=1}^n Y_k
\end{equation*}
is a Binomial variable.%
\begin{theorem}{Binomial as approximate Normal.}{}{theorem-71}%
\(\mu = np\)\(\sigma^2 = np(1-p)\)\(np \gt 5\)\(n(1-p) \gt 5\)\end{theorem}
\begin{proof}\hypertarget{proof-68}{}
\hypertarget{p-1075}{}%
Using the Bernoulli variables \(Y_k\) each with mean p and variance p(1-p), note that the Central Limit Theorem applied to \(\overline{X} = \frac{\sum Y_k}{n}\) gives that%
\begin{equation*}
\frac{\overline{X}-p}{\sqrt{p(1-p)/n}}
\end{equation*}
is approximately standard normal. By multiplying top and bottom by n yields%
\begin{equation*}
\frac{\sum Y_k - np}{\sqrt{np(1-p)}}
\end{equation*}
is approximately standard normal. But \(\sum Y_k\) actually is the sum of the number of successes in n trials and is therefore a Binomial variable.%
\end{proof}
\begin{example}{Binomial as Normal.}{example-37}%
\hypertarget{p-1076}{}%
Binomial becomes normal as \(n \rightarrow \infty\).  Consider n = 50 and p = 0.3.  Then, \(\mu = 15\) and \(\sigma^2 = 10.5\).%
\par
\hypertarget{p-1077}{}%
Using the binomial formulas, for example,%
\begin{equation*}
P( X = 16 ) = \binom{50}{16} 0.3^{16} \cdot 0.7^{34} \approx 0.11470
\end{equation*}
Using the normal distribution,%
\begin{align*}
P( X = 16 ) & = P( 15.5 \lt X \lt 16.5) \\
& \approx normalcdf(15.5,16.5,15,sqrt(10.5)) \\
& = 0.11697
\end{align*}
Notice that these are very close.%
\end{example}
\begin{corollary}{Poisson as approximate Normal.}{}{corollary-7}%
\(\mu\)\(\sigma^2 = \mu\)\(\mu \gt 5\)\end{corollary}
\begin{proof}\hypertarget{proof-69}{}
\hypertarget{p-1078}{}%
Note from before that the Poisson distribution function was derived by approximating with Binomial and letting n approach infinity. Therefore, by the previous theorem, the Poisson variable is also approximately Normal using the Poisson mean and variance rather than the binomial's. Indeed, in standard units%
\begin{equation*}
\frac{Y - \mu}{\sqrt{\mu}}
\end{equation*}
is approximately normal for large \(\mu\).%
\end{proof}
\begin{example}{Poisson as Normal.}{example-38}%
\hypertarget{p-1079}{}%
Poisson becomes normal as \(\mu \rightarrow \infty\).  Consider \(\mu = 20\).  Then, \(\sigma^2 = \mu = 20\).%
\par
\hypertarget{p-1080}{}%
Using the Poisson formulas, for example,%
\begin{equation*}
P( X = 19 ) = \frac{20^{19} e^{-20}}{19!} \approx 0.08883
\end{equation*}
Using the normal distribution,%
\begin{align*}
P( X = 19 ) & = P( 18.5 \lt X \lt 19.5) \\
& \approx normalcdf(18.5,19.5,20,sqrt(20)) \\
& = 0.08683
\end{align*}
Again, these are very close.%
\end{example}
\begin{theorem}{Gamma as approximate Normal.}{}{theorem-72}%
\(r \mu\)\(r BLOB\)\end{theorem}
\begin{example}{Gamma as Normal.}{example-39}%
\hypertarget{p-1081}{}%
Gamma becomes normal as \(r \rightarrow \infty\).  Assume that the average time till a first success is 12 minutes and that \(r = 8\).  Then, the mean for the Gamma distribution is \(\mu = 12 \cdot 8 = 96\) and \(\sigma^2 = 8 \cdot 12^2 = 1152\) and so \(\sigma \approx 33.9411\).%
\par
\hypertarget{p-1082}{}%
Using the Gamma formulas,%
\begin{align*}
P( 90 \le X \le 100 ) & = \int_{90}^{100} f(x) dx \\
& = 0.59252 - 0.47536 = 0.11716.
\end{align*}
Using the normal distribution,%
\begin{equation*}
P( 90 \le X \le 100) \approx normalcdf(90,100,96,33.9411) = 0.11707.
\end{equation*}
Amazingly, these are also very close.%
\end{example}
\begin{example}{Uniform X vs Normal \(\overline{X}\).}{example-40}%
\hypertarget{p-1083}{}%
Consider a discrete uniform variable X over R = {1,2,...,20}.  Then, \(\mu = 10.5\) and \(\sigma = \frac{20^2-1^2}{20}\) using the uniform formulas.%
\par
\hypertarget{p-1084}{}%
You can use the uniform probability function to compute probabilities dealing with X. Indeed,%
\begin{equation*}
P(8 \le X \lt 12) = P(X \in \{8,9,10,11 \} = \frac{4}{20} = 1/5.
\end{equation*}
%
\par
\hypertarget{p-1085}{}%
If instead you plan to sample from this distribution n=49 times, the Central Limit Theorem implies that you will get a random variable \(\overline{X}\) which has an approximate normal distribution with the same mean but with new variance \(\sigma_{\overline{X}}^2 = \frac{199/20}{49} = \frac{199}{580}\).  Therefore, expanding the interval to include the boundaries of the corresponding histogram areas,%
\begin{equation*}
P( 8 \le \overline{X} \lt 12 ) = P(7.5 \le \overline{X} \le 11.5) \approx normalcdf(7.5,11.5,10.5,0.585750) \approx 0.9561 .
\end{equation*}
%
\end{example}
\hypertarget{p-1086}{}%
As these examples illustrate, you will have increasing success in approximating the desired probabilities so long as the distribution's corresponding parameter is allowed to be "sufficiently large". The mathematical reasoning this is true is not provided but depends upon the "Central Limit Theorem" discussed in the next section.%
\par
\hypertarget{p-1087}{}%
The above theorems allow you to utilize the normal distribution to compute approximate probabilities for the variable X in the stated distributions. This is not always true for all distributions since some do not have parameters which allow for approaching normality. However, regardless of the distribution the Central Limit Theorem always allows you to approximate probabilities if they involve an average of repeated attempts...that is, for variable \(\overline{X}\). This usefulness is illustrated in the examples below.%
%
%
\typeout{************************************************}
\typeout{Section 9.7 Summary}
\typeout{************************************************}
%
\section[{Summary}]{Summary}\label{section-63}
\hypertarget{p-1088}{}%
Here is a summary of the major points in this chapter:%
\hypertarget{p-1089}{}%
TBA%
%
%
\typeout{************************************************}
\typeout{Section 9.8 Exercises}
\typeout{************************************************}
%
\section[{Exercises}]{Exercises}\label{section-64}
\begin{inlineexercise}{- Computing basic standard normal probabilities.}{exercise-71}%
\hypertarget{p-1090}{}%
Compute \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}%
\begin{equation*}
P( Z \gt 0)
\end{equation*}
%
\item{}%
\begin{equation*}
P( Z \lt 0.892)
\end{equation*}
%
\item{}%
\begin{equation*}
P( Z \lt -0.892)
\end{equation*}
%
\item{}%
\begin{equation*}
P( -1.45 \lt Z \lt 2.37)
\end{equation*}
%
\item{}%
\begin{equation*}
P( -1 \lt Z \lt 1)
\end{equation*}
which is the probability of lying within 1 standard deviation of the mean.%
\item{}%
\begin{equation*}
P( -2 \lt Z \lt 2)
\end{equation*}
which is the probability of lying within 2 standard deviations of the mean.%
\item{}%
\begin{equation*}
P( -3 \lt Z \lt 3)
\end{equation*}
which is the probability of lying within 3 standard deviations of the mean.%
\item{}A value for a so that%
\begin{equation*}
P( Z \lt a) = 0.8
\end{equation*}
which would be the location of the 80th percentile.%
\end{itemize}
%
\end{inlineexercise}
\begin{inlineexercise}{- Computing basic normal probabilities.}{exercise-72}%
\hypertarget{p-1091}{}%
Given \(\mu = 25\) and \(\sigma = 4\) compute%
\begin{equation*}
P(X \lt \mu)
\end{equation*}
%
\begin{equation*}
P( X \gt 26)
\end{equation*}
%
\begin{equation*}
P( X \gt 22)
\end{equation*}
%
\begin{equation*}
P( 20 \le X \le 26)
\end{equation*}
%
\end{inlineexercise}
\begin{inlineexercise}{- IQ values.}{exercise-73}%
\hypertarget{p-1092}{}%
The Intelligence Quotient (IQ) is a measure of your ability to think and reason. Presuming that IQ scores are normally distributed with mean 100 and standard deviation 15, determine the location of the 90th percentile.  That is, the IQ score below which you will find approximately 90% of other IQ scores.%
\end{inlineexercise}
%
%
\typeout{************************************************}
\typeout{Chapter 10 Estimation}
\typeout{************************************************}
%
\chapter[{Estimation}]{Estimation}\label{IntervalEstimation}
%
%
\typeout{************************************************}
\typeout{Section 10.1 Introduction}
\typeout{************************************************}
%
\section[{Introduction}]{Introduction}\label{section-65}
\hypertarget{p-1093}{}%
You should have noticed by now that repeatedly sampling from a given distribution will yield a variety of sample statistics such as \(\overline{x}\) as an estimate perhaps for the population mean \(\mu\) or \(\frac{Y}{n}\) as an estimate for the population likelihood of success p. In this section, you will see how these sample "point estimators" are actually the best possible choices.%
\par
\hypertarget{p-1094}{}%
In creating these point estimates repeatedly, you have noticed that the results will change somewhat over time. Indeed, flip a coin 20 times and you might expect 10 heads. However, in practice it is likely to 9 or 12 out of 20 and possible to get any of the other possible outcomes. This natural variation makes the point estimates noted above to almost certainly be in error. However one would expect that they should be close and the Central Limit Theorem does indicate that the distribution of sample means should be approximately normally distributed. Thus, instead of relying just on the value of the point estimate, you might want to investigate a way to determine a reasonable interval centered on the sample statistic in which you have some confidence the actual population statistic should belong. This leads to a discussion of interval estimates known as confidence intervals (using calculational tools) and statistical tolerance intervals (using order statistics).%
\par
\hypertarget{p-1095}{}%
In this chapter we first discuss how to determine appropriate methods for estimating the needed population statistics (point estimates) and then quantify how good they are (confidence intervals).%
%
%
\typeout{************************************************}
\typeout{Section 10.2 Interval Estimates - Chebyshev}
\typeout{************************************************}
%
\section[{Interval Estimates - Chebyshev}]{Interval Estimates - Chebyshev}\label{section-66}
\hypertarget{p-1096}{}%
An interval centered on the mean in which at least a certain proportion of the actual data must lie.%
\begin{theorem}{Chebyshev's Theorem.}{}{theorem-73}%
\(\mu\)\(\sigma\)\(a \in \mathbb{R}^+\)%
\begin{equation*}
P( \big | X - \mu \big | \lt a ) \gt 1 - \frac{\sigma^2}{a^2}
\end{equation*}
\end{theorem}
\begin{proof}\hypertarget{proof-70}{}
\hypertarget{p-1097}{}%
Notice that the variance of a continuous variable X is given by%
\begin{align*}
\sigma^2 & = \int_{-\infty}^{\infty} (x - \mu)^2 f(x) dx\\
& \ge \int_{-\infty}^{\mu-a} (x - \mu)^2 f(x) dx + \int_{\mu + a}^{\infty} (x - \mu)^2 f(x) dx\\
& \ge \int_{-\infty}^{\mu-a} a^2 f(x) dx + \int_{\mu + a}^{\infty} a^2 f(x) dx\\
& = a^2 \big ( \int_{-\infty}^{\mu-a} f(x) dx + \int_{\mu + a}^{\infty} f(x) dx \big )\\
& = a^2 P( X \le \mu - a \text{or} X \ge \mu + a )\\
& = a^2 P( \big | \mu - a \big | \ge a)
\end{align*}
Dividing by \(a^2\) and taking the complement gives the result.%
\end{proof}
\begin{corollary}{Alternate Form for Chebyshev's Theorem.}{}{corollary-8}%
\hypertarget{p-1098}{}%
For positive k,%
\begin{equation*}
P( \big | X - \mu \big | \lt k \sigma ) \gt 1 - \frac{1}{k^2}
\end{equation*}
%
\end{corollary}
\begin{corollary}{Special Cases for Chebyshev's Theorem.}{}{corollary-9}%
\end{corollary}
\begin{proof}\hypertarget{proof-71}{}
\hypertarget{p-1099}{}%
Apply the Chebyshev Theorem with \(a = \sigma\) to get%
\begin{equation*}
P(\mu - \sigma \lt X \lt \mu + \sigma) \gt 1 - \frac{\sigma^2}{\sigma^2} = 0
\end{equation*}
%
\par
\hypertarget{p-1100}{}%
Apply the Chebyshev Theorem with \(a = 2 \sigma\) to get \(1 - \frac{1}{2^2} = 0.75\) and with \(k = 3 \sigma\) to get \(1 - \frac{1}{3^2} = \frac{8}{9} > 0.8888\).%
\end{proof}
\begin{example}{- Comparing known distribution to Chebyshev.}{example-41}%
\hypertarget{p-1101}{}%
%
\end{example}
%
%
\typeout{************************************************}
\typeout{Section 10.3 Point Estimates}
\typeout{************************************************}
%
\section[{Point Estimates}]{Point Estimates}\label{section-67}
\hypertarget{p-1102}{}%
For Binomial, Geometric, what is p? For exponential, what is the mean?  For normal, what are the mean and standard deviation? Each of these parameters are necessary before you can compute any probability values from their respective formulas. Since they might not be given in a particular instance, they will need to be estimated in some manner.%
\par
\hypertarget{p-1103}{}%
This estimate will have to be determined likely by utilizing sampling in some form. Since such an estimate will come from partial information (i.e. a sample) then it is very likely going to only be an approximation to the exact (but unknown) value. In general, an estimator is a numerical value which is used in the place of an unknown population statistic. To determine precisely what is a "best" estimator requires a multivariate approach and is beyond the scope of this text. Indeed, to justify why each of the following are good estimators look up the topic "Maximum Likelihood Estimators".%
\par
\hypertarget{p-1104}{}%
From your previous experience with the Poisson, Exponential, and Gamma distributions, you should also remember that each required a known value for \(\mu\) before proceeding with calculations.  It is sensible to consider estimating the unknown population mean \(\mu\) using the sample mean%
\begin{equation*}
\mu \approx \overline{x} = \frac{\sum x_k}{n}
\end{equation*}
where the values \(x_k\) are the n individual sample values.%
\par
\hypertarget{p-1105}{}%
For any continous variable and indeed for \(\overline{X}\)), \(P(\overline{X} = \mu) = 0\). In general, you should expect a sample statistic to be close but not precisely equal to the population statistic. Indeed, if you were so lucky as to have the sample statistic to land on the population statistic, doing one more trial would mess things up anyway and the sample statistic would certainly change some.%
\par
\hypertarget{p-1106}{}%
In a similar manner with the Binomial, Geometric, and Negative Binomial distributions, you will remember that each required a known value for p before  proceeding with any calculations. From our experiments we saw that relative frequency appeared to stabilize around what you might expect for the true proportion of success and therefore estimating the unknown proportion of success p using relative frequency%
\begin{equation*}
p \approx \tilde{p} = \frac{y}{n}
\end{equation*}
where y is the number of successes in a collection of n bernoulli trials. Again, notice that the relative frequency \(\tilde{p}\) is technically an average as well so the probability that a given relative frequency will like exactly on the actual value of p is again zero.%
\par
\hypertarget{p-1107}{}%
Finally, the Normal distribution requires a numerical value for \(\sigma\), the population's standard deviation. It can be shown that the maximum likelihood estimator for \(\sigma^2\) is the variance v found in chapter one. However, you may remember that at that time we always adjusted this value somewhat using the formula \(s^2 = \frac{n}{n-1} v\) which increased the variance slightly. To uncover why you would not use the maximum likelihood estimator v requires you to look up the idea of "bias". As it turns out, v is maximum likelihood but exhibits mathematical bias whereas \(s^2\) is slightly suboptimal with respect to likelihood but exhibits no bias. Therefore, for estimating the unknown population variance \(\sigma^2\) you can use sample variance%
\begin{equation*}
\sigma^2 \approx s^2
\end{equation*}
and similarly sample standard deviation%
\begin{equation*}
\sigma \approx s
\end{equation*}
to approximate the theoretical standard deviation.%
%
%
\typeout{************************************************}
\typeout{Section 10.4 Interval Estimates - Confidence Interval for p}
\typeout{************************************************}
%
\section[{Interval Estimates - Confidence Interval for p}]{Interval Estimates - Confidence Interval for p}\label{section-68}
\hypertarget{p-1108}{}%
Sometimes selecting a value for p for a Binomial, Geometric, or Negative Binomial distribution problem can be done by using a theoretical value. Indeed, when flipping a coin it is reasonable to assume p = 1/2 is the probability of getting a head on one flip. Similarly, it is reasonable to assume p = 1/6 when you are looking for a particular side of a 6-sided die. However, many times you will want to deal with a problem in which it is not possible to determine exactly the precise value for the likelihood of success such as your true probability of making a free throw in basketball or knowing the true percentage of the electorate that will vote for your favorite candidate.%
\par
\hypertarget{p-1109}{}%
In these later situations, we found in the previous section that relative frequency \(\frac{Y}{n}\) is generally a good way to estimate p. In this section, you will investigate how to measure the closeness--and thereby assure some confidence in that estimate--regarding how well the point estimate approximates the actual value of p.%
\begin{definition}{Confidence Intervals for p.}{definition-41}%
\hypertarget{p-1110}{}%
Given a point estimate \(\tilde{p}\) for p, a confidence interval for p is a range of values which contains the actual value of p with high probability. In notation, a two-sided confidence interval for p is of the form%
\begin{equation*}
\tilde{p} - E_1 \lt p \lt \tilde{p} + E_2
\end{equation*}
with%
\begin{equation*}
P(\tilde{p} - E_1 \lt p \lt \tilde{p} + E_2) = 1 - \alpha
\end{equation*}
where \(\alpha\) is near 0 and \(E_k \gt 0\).  One-sided confidence intervals for p can be similarly described%
\begin{equation*}
P( p \lt \tilde{p} + E_2) = 1 - \alpha
\end{equation*}
or%
\begin{equation*}
P(\tilde{p} - E_1 \lt p) = 1 - \alpha.
\end{equation*}
%
\end{definition}
\hypertarget{p-1111}{}%
Generally, symmmetry is presumed when using a two-sided confidence interval so that \(E_1 = E_2 = E\) and therefore the interval looks like%
\begin{equation*}
P(\tilde{p} - E \lt p \lt \tilde{p} + E) = 1 - \alpha.
\end{equation*}
In this case, E is known as the margin of error.%
\par
\hypertarget{p-1112}{}%
To determine E carefully, note that from the central limit theorem%
\begin{equation*}
\frac{Y-np}{\sqrt{np(1-p}} = \frac{\tilde{p} - p}{\sqrt{p(1-p)/n}}
\end{equation*}
is approximately standard normal for large n.  Presuming that \(\tilde{p} \approx p\) and replacing the unknown p terms on the bottom with \(\tilde{p}\) gives%
\begin{equation*}
z = \frac{\tilde{p} - p}{\sqrt{\tilde{p}(1-\tilde{p})/n}}
\end{equation*}
where z is a standard normal distribution variable. So, using the central limit theorem and the standard normal distribution, you can find the value \(z_{ \alpha/2}\) where%
\begin{equation*}
P( -z_{ \alpha/2} \lt z \lt z_{ \alpha/2}) = 1 - \alpha
\end{equation*}
%
\begin{equation*}
P( -z_{ \alpha/2} \lt \frac{\tilde{p} - p}{\sqrt{\tilde{p}(1-\tilde{p})/n}} \lt z_{ \alpha/2}) = 1 - \alpha
\end{equation*}
or by rearranging the inside inequality%
\begin{equation*}
P( \tilde{p} - z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n} \lt  p \lt \tilde{p} + z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n}) = 1 - \alpha.
\end{equation*}
Setting \(E = z_{ \alpha/2}\sqrt{\tilde{p}(1-\tilde{p})/n}\) gives a way to determine a confidence interval centered on \(\tilde{p} = \frac{Y}{n}\) for p with "confidence level" \(1-\alpha\).%
\par
\hypertarget{p-1113}{}%
To complete the interval, one needs a specific value for \(z_{ \alpha/2}\). Generally, one chooses confidence levels on the order of 90%, 95%, or 99% with 95% being the usual choice.  Fortunately this value is easily computed using graphing calculators or other automatic methods although your ancient teacher might have been required to use tables. On a TI calculator, use%
\begin{equation*}
z_{\alpha /2} = \text{InvNorm}( 1 - \frac{\alpha}{2} )
\end{equation*}
%
\par
\hypertarget{p-1114}{}%
For 90% confidence level, you need to find a z-value so that%
\begin{equation*}
P( -z_{ \alpha/2} \lt z \lt z_{ \alpha/2}) = 0.9 = 1 - 0.1 .
\end{equation*}
Using the symmetry of the normal distribution, this can be rewritten%
\begin{equation*}
F(z_{ \frac{0.1}{2}}) = P( z \lt z_{ \frac{0.1}{2}}) = 0.95 = 1 - \frac{0.1}{2} .
\end{equation*}
Using the inverse of the standard normal distribution (on the TI calculator this is InvNorm(0.95)) gives \(z_{ 0.05} \approx 1.645\).%
\par
\hypertarget{p-1115}{}%
Similarly, for a 95 % confidence level, find where%
\begin{equation*}
F(z_{ \frac{0.05}{2}}) = P( z \lt z_{ \frac{0.05}{2}}) = 0.975 = 1 - \frac{0.05}{2} .
\end{equation*}
The calculators InvNorm(0.975) gives \(z_{ 0.025} \approx 1.960\).%
\par
\hypertarget{p-1116}{}%
For a 99 % confidence level, find where%
\begin{equation*}
F(z_{ \frac{0.01}{2}}) = P( z \lt z_{ \frac{0.01}{2}}) = 0.995 = 1 - \frac{0.01}{2} .
\end{equation*}
The calculators InvNorm(0.995) gives \(z_{ 0.005} \approx 2.576\).%
\par
\hypertarget{p-1117}{}%
Notice that when computing the confidence intervals above that we choose to just replace some of the p terms with \(\tilde{p}\) so that only one p term was left and could be isolated in the middle. There are other ways to deal with this. The easiest is to take the worst case scenario for the p terms in the denominator above. Indeed, the confidence interval is made wider (and therefore more likely to contain the actual p) if the square root term is as large as possible, using basic calculus it is easy to see that p(1-p) is maximized when p = 1/2. Therefore, a second alternative is to create your confidence interval using%
\begin{equation*}
z = \frac{\tilde{p} - p}{\frac{1}{2\sqrt{n}}}
\end{equation*}
and therefore \(E = \frac{z_{ \alpha/2}}{2\sqrt{n}}\). This method should be used only when trying to create the roughest and "safest" interval.%
\par
\hypertarget{p-1118}{}%
The methods for determining a confidence interval for p above depend upon a good approximation with the Central Limit Theorem. This approximation will be fine if n is relatively large. To consider a confidence interval for p when n is small, note that the binomial random variable is discrete and so expanding the interval by a factor of \(\frac{1}{2n}\) might be in order.%
\par
\hypertarget{p-1119}{}%
Another more elaborate mechanism when n is relatively large is given by the Wilson Score. This confidence interval is more complicated than just taking \(\tilde{p}\) and adding and subtracting E. This approach notes that the possible extreme values for p must satisfy (before replacing some of the p terms with \(\tilde{p}\))%
\par
\hypertarget{p-1120}{}%
\begin{theorem}{Wilson Score Confidence Interval for p.}{}{theorem-74}%
\hypertarget{p-1121}{}%
%
\begin{equation*}
\frac{\tilde{p} + \frac{z_{\alpha/2}^2}{2n} - z_{\alpha/2} \sqrt{\frac{\tilde{p}(1-\tilde{p}) + \frac{z_{\alpha/2}^2}{4n}}{n}}}{1 + \frac{z_{\alpha/2}^2}{n}} \lt p \lt \frac{\tilde{p} + \frac{z_{\alpha/2}^2}{2n} + z_{\alpha/2} \sqrt{\frac{\tilde{p}(1-\tilde{p}) + \frac{z_{\alpha/2}^2}{4n}}{n}}}{1 + \frac{z_{\alpha/2}^2}{n}}
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-72}{}
\hypertarget{p-1122}{}%
Again, noting that \(\tilde{p} = \frac{Y}{n}\), the expression above%
\begin{equation*}
\big | p - \tilde{p} \big | = z_{\alpha /2} \sqrt{\frac{p(1-p)}{n}}
\end{equation*}
can be simplified by squaring both sides to get%
\begin{equation*}
\big ( p - \tilde{p} \big )^2 = z_{\alpha /2}^2 \frac{p(1-p)}{n}.
\end{equation*}
Replacing \(\tilde{p}\) with the relative frequency gives%
\begin{equation*}
\big ( p - \frac{Y}{n} \big )^2 = z_{\alpha /2}^2 \frac{p(1-p)}{n}
\end{equation*}
or by simplifying%
\begin{equation*}
(n+z_{\alpha /2}^2 )p^2 - (2Y+z_{/alpha /2}^2) p + \frac{Y^2}{2} = 0.
\end{equation*}
Solving for p using the quadratic formula and simplifying ultimately results in the described interval.%
\end{proof}
 \begin{example}{Comparison of the three Confidence Interval methods for p.}{example-42}%
\hypertarget{p-1123}{}%
Presume that from a sample of size n = 400 you get Y = 144 successes.  Determine 95% two-sided confidence intervals for the actual p using all three of the methods above. Note that for each you will utilize \(z_{\alpha/2} = z_{0.025} = 1.960\) and \(\tilde{p} = \frac{144}{400} = 0.36\).%
\par
\hypertarget{p-1124}{}%
Normal Interval:%
\begin{equation*}
P( 0.36 - 1.960 \sqrt{0.36 \cdot 0.64) / 400} \lt  p \lt 0.36 + 1.960 \sqrt{0.36 \cdot 0.64) / 400}) = 1 - \alpha.
\end{equation*}
or%
\begin{equation*}
P( 0.36 - 1.960 \cdot 0.6 \cdot 0.8) / 20 \lt  p \lt 0.36 + 1.960 \cdot 0.6 \cdot 0.8) / 20) = 0.95 
\end{equation*}
or%
\begin{equation*}
P( 0.36 - 0.04704 \lt  p \lt 0.36 + 0.04704) = 0.95 .
\end{equation*}
or%
\begin{equation*}
P( 0.31296 \lt  p \lt 0.40704) = 0.95 .
\end{equation*}
So, there is a 95% chance that the actual value for p lies inside the interval \((0.31296 , 0.40704).\)%
\par
\hypertarget{p-1125}{}%
Maximal Interval:%
\begin{equation*}
P( 0.36 - 1.960 \frac{1}{2\sqrt{400}} \lt  p \lt 0.36 + 1.960 \frac{1}{2\sqrt{400}} ) = 1 - \alpha.
\end{equation*}
or%
\begin{equation*}
P( 0.36 - 1.960 \frac{1}{40} \lt  p \lt 0.36 + 1.960 \frac{1}{40} ) = 1 - \alpha.
\end{equation*}
or%
\begin{equation*}
P( 0.311 \lt  p \lt 0.409 ) = 1 - \alpha.
\end{equation*}
Notice the interval is only slightly wider than when using \(\tilde{p}\) to estimate p in the first case.%
\par
\hypertarget{p-1126}{}%
Wilson Score Interval:  Let's do this on in parts...%
\begin{equation*}
z_{\alpha/2} \sqrt{\frac{\tilde{p}(1-\tilde{p}) + \frac{z_{\alpha/2}^2}{4n}}{n}} = 1.96 \sqrt{ \frac{0.36 \cdot 0.64 + \frac{1.96^2}{1600}}{400}} \approx 0.04728
\end{equation*}
Therefore,%
\begin{equation*}
\frac{0.36 + \frac{1.96^2}{800} - 0.04728}{1 + \frac{1.96^2}{400}} \lt p \lt \frac{0.36 + \frac{1.96^2}{800} + 0.04728}{1 + \frac{1.96^2}{400}}
\end{equation*}
or%
\begin{equation*}
0.3145 \lt p \lt 0.4082
\end{equation*}
which is slightly different than the first and slightly smaller than the second.%
\end{example}
%
\begin{theorem}{Determining Sample Size for proportions.}{}{theorem-75}%
\(\tilde{p_0}\)%
\begin{equation*}
n \gt \left ( \frac{z_{\alpha /2}}{E} \right )^2 \tilde{p_0}(1-\tilde{p_0}).
\end{equation*}
\end{theorem}
\begin{proof}\hypertarget{proof-73}{}
\end{proof}
\begin{example}{Determining Sample Size for one proportion.}{example-43}%
\hypertarget{p-1127}{}%
Given a 99% confidence level, margin of error E=0.03, and preliminary estimate \(\tilde{p_0} = 0.35\), notice that \(z_{\alpha / 2} = 2.58\) gives%
\begin{equation*}
n \gt \left ( \frac{2.58}{0.03} \right )^2 0.35 \cdot 0.65 \approx 1682.59
\end{equation*}
or a sample size of at least 1683.%
\end{example}
%
%
\typeout{************************************************}
\typeout{Section 10.5 Interval Estimates - Confidence Interval for \(\mu\)}
\typeout{************************************************}
%
\section[{Interval Estimates - Confidence Interval for \(\mu\)}]{Interval Estimates - Confidence Interval for \(\mu\)}\label{section-69}
\hypertarget{p-1128}{}%
As with the confidence intervals above for proportions, the Central Limit Theorem also allows you to create an interval centered on a sample mean for estimating the population mean \(\mu\).%
\begin{definition}{Confidence Interval for One Mean.}{definition-42}%
\hypertarget{p-1129}{}%
Given a sample mean \(\overline{x}\), a two-sided confidence interval for the mean with confidence level \(1-\alpha\) is an interval%
\begin{equation*}
\overline{x} - E_1 \lt \mu \lt \overline{x} + E_2
\end{equation*}
such that%
\begin{equation*}
P(\overline{x} - E_1 \lt \mu \lt \overline{x} + E_2) = 1-\alpha.
\end{equation*}
Generally, the interval is symmetrical of the form \(\overline{x} \pm E\) with E again known as the margin of error.  One-sided confidence intervals can be determined in the same manner as in the previous section.%
\end{definition}
\hypertarget{p-1130}{}%
Once again, utilize the Central Limit Theorem.  Notice that the symmetrical confidence interval%
\begin{equation*}
P(\overline{x} - E \lt \mu \lt \overline{x} + E) = 1-\alpha.
\end{equation*}
is equivalent to%
\begin{equation*}
P \left ( \frac{-E}{\sigma / \sqrt{n}} \lt \frac{\overline{x} - \mu}{\sigma / \sqrt{n}} \lt \frac{E}{\sigma / \sqrt{n}} \right ) = 1 - \alpha
\end{equation*}
in which the middle term can be approximated using a standard normal variable and therefore this statement is approximately%
\begin{equation*}
P \left ( \frac{-E}{\sigma / \sqrt{n}} \lt Z \lt \frac{E}{\sigma / \sqrt{n}} \right ) = 1 - \alpha.
\end{equation*}
Using the symmetry of the standard normal distribution about Z=0 gives%
\begin{equation*}
\Phi (z_{\alpha/2} ) = \Phi \left ( \frac{E}{\sigma / \sqrt{n}} \right ) = P \left ( Z \lt \frac{E}{\sigma / \sqrt{n}} \right ) = 1 - \frac{\alpha}{2}
\end{equation*}
and so to determine E again requires the inverse of the standard normal distribution function.  Using an appropriate \(z_{\alpha /2}\) (as determine in a manner described in the previous section) gives a confidence interval for the mean%
\begin{equation*}
\overline{x} - z_{\alpha / 2} \frac{\sigma}{\sqrt{n}} \lt \mu \lt \overline{x} + z_{\alpha / 2} \frac{\sigma}{\sqrt{n}}
\end{equation*}
with confidence level \(1-\alpha\) and margin of error%
\begin{equation*}
E = z_{\alpha /2} \frac{\sigma}{\sqrt{n}}.
\end{equation*}
%
\par
\hypertarget{p-1131}{}%
It should be noted that the use of the Central Limit Theorem makes the use of  InvNorm an approximation. It can be shown that so long as n is larger than 30 then generally this approximation is reasonable.%
\par
\hypertarget{p-1132}{}%
Additionally, this derivation assumes that \(\mu\) is not known...indeed the goal is to approximate that mean using \(\overline{x}\)...but that \(\sigma\) is known. This is often not the case. It can however be shown that if n is larger than 30, replacing \(\sigma\) with the sample standard deviation s gives an acceptable confidence interval.%
\begin{theorem}{Sample Size needed for \(\mu\) given Margin of Error.}{}{theorem-76}%
\hypertarget{p-1133}{}%
Given confidence level \(1-\alpha\) and margin of error E, the sample size needed to determine an appropriate confidence interval satisfies%
\begin{equation*}
n \gt \left ( z_{\alpha /2} \frac{\sigma}{E} \right )^2
\end{equation*}
%
\end{theorem}
\begin{proof}\hypertarget{proof-74}{}
\hypertarget{p-1134}{}%
Solve for n in the formula for E above. Notice that n must be an integer so you will need to round up. You will also need an estimate for the sample standard deviation s by using a preliminary sample.%
\end{proof}
\hypertarget{p-1135}{}%
Notice, in practice you might want to take n to be a little larger than the absolute minimum value prescribed above since you are dealing with approximations (Central Limit Theorem and the use of an estimate for s rather than the actual \(\sigma\).)%
\begin{example}{Determining Sample Size for one Mean.}{example-44}%
\hypertarget{p-1136}{}%
Given a 95% confidence level, margin of error E=0.1, and preliminary sample with standard deviation s = 2, \(z_{\alpha / 2} = 1.96\) gives%
\begin{equation*}
n \gt \left ( 1.96 \cdot \frac{2}{0.1} \right )^2 \approx 1536.64
\end{equation*}
or a sample size of at least 1537.%
\end{example}
%
%
\typeout{************************************************}
\typeout{Section 10.6 Interval Estimates - Confidence Interval for \(\sigma^2\)}
\typeout{************************************************}
%
\section[{Interval Estimates - Confidence Interval for \(\sigma^2\)}]{Interval Estimates - Confidence Interval for \(\sigma^2\)}\label{section-70}
\hypertarget{p-1137}{}%
Once again, you may need to approximate the population variance or standard deviation but only have the sample values available. One difference from the previous sections is that you are not dealing with an average of values (such as \(\overline{x}\) or \(\tilde{p}\)) but with the average of the squares of values. The Central Limit Theorem does not directly help you in this case but the following result (presented without proof) provides a solution.%
\begin{theorem}{Relationship between Variance and \(\chi ^2\).}{}{theorem-77}%
\hypertarget{p-1138}{}%
If \(S^2\) is a random variable of possible sample variance values from a sample of size n, then%
\begin{equation*}
W = \frac{(n-1)S^2}{\sigma^2}
\end{equation*}
is approximately \(\chi ^2(n-1).\)%
\end{theorem}
\hypertarget{p-1139}{}%
To create a confidence interval for \(\sigma^2\) first consider an interval of the form%
\begin{equation*}
E_1 \lt \sigma^2 \lt E_2
\end{equation*}
and determine values for the boundaries so that the likelihood of this being true is high. For this case, since the chi-square distribution only has a positive domain and is not symmetrical, you will not expect to determine a symmetrical confidence interval.  Therefore, consider%
\begin{equation*}
P (E_1 \lt \sigma^2 \lt E_2 ) = 1 - \alpha 
\end{equation*}
and by playing around with algebra you get%
\begin{equation*}
P \left ( \frac{E_1}{(n-1)S^2} \lt \frac{\sigma^2}{(n-1)S^2} \lt \frac{E_2}{(n-1)S^2} \right ) = 1 - \alpha 
\end{equation*}
or by inverting the inequality yields%
\begin{equation*}
P \left ( \frac{(n-1)S^2}{E_2} \lt \frac{(n-1)S^2}{\sigma^2} \lt \frac{(n-1)S^2}{E_1} \right ) = 1 - \alpha .
\end{equation*}
Using the previous theorem, note that the inside variable can be replaced with a chi-square variable. If F is the distribution function for chi-square, then you get%
\begin{equation*}
F \left ( \frac{(n-1)S^2}{E_1} \right ) - F \left ( \frac{(n-1)S^2}{E_2} \right ) = 1 - \alpha .
\end{equation*}
For a given value of \(\alpha\) there are many possible choices but often one often utilized is one in which%
\begin{equation*}
F(\chi^2_{1-\alpha/2} ) = F \left ( \frac{(n-1)S^2}{E_1} \right ) = 1 - \alpha / 2
\end{equation*}
and%
\begin{equation*}
F(\chi^2_{\alpha/2} ) = F \left ( \frac{(n-1)S^2}{E_2} \right ) = \alpha / 2.
\end{equation*}
Using the inverse chi-square gives values for the expression on the inside and algebra can be used to solve for each of \(E_1, E_2\). Indeed,%
\begin{equation*}
E_1 = \frac{(n-1)S^2}{\chi^2_{1-\alpha/2}} 
\end{equation*}
and%
\begin{equation*}
E_2 = \frac{(n-1)S^2}{\chi^2_{\alpha/2}} 
\end{equation*}
%
\par
\hypertarget{p-1140}{}%
To determine appropriate values for \(\chi^2_{\alpha/2} \) and \(\chi^2_{1-\alpha/2} \) with equal probabilities in each tail, consider using the interactive cell below:%
\begin{sageinput}
# Chi-Square Calculator for confidence intervals with equal alpha/2 tails
var('t')
@interact(layout=dict(top=[['c'],['n']]))
def _(c=input_box(0.95,width=10,label='Confidence Level = '),n=input_box(20,width=8,label='n =')):
    alpha = 1-c
    T = RealDistribution('chisquared', n)
    a = T.cum_distribution_function_inv(alpha/2)
    a1 = T.cum_distribution_function(a)
    b = T.cum_distribution_function_inv(1-alpha/2)
    b1 = T.cum_distribution_function(b)
    
    print 'From the Chi-Square distribution for X:'
    print 'P(',a,'< X < ',(b),') = ',c
    print 'with'
    print 'P( X < ',a,') = ',a1
    print 'P( X < ',b,') = ',b1
    
    f = x^(n/2-1)*e^(-x/2)/(gamma(n/2)*2^(n/2))
    G = plot(f,x,0,b+(b-a)/2)+plot(f,x,a,b,thickness=5,color='green')
    G += line([(a,0),(a,f(x=a))],color='green',thickness=3)
    G += line([(b,0),(b,f(x=b))],color='green',thickness=3)
    G += text(str(c.n(digits=5)),((a+b)/2,f(x=(a+b)/2)/3),color='green')
    G.show()
\end{sageinput}
\hypertarget{p-1141}{}%
The example below uses the specific chi-square values given in the interactive cell below: \leavevmode%
\begin{sageinput}
# Chi-Square Calculator specifics
var('t')
c=0.95
n=8
alpha = 1-c
T = RealDistribution('chisquared', n)
a = T.cum_distribution_function_inv(alpha/2)
a1 = T.cum_distribution_function(a)
b = T.cum_distribution_function_inv(1-alpha/2)
b1 = T.cum_distribution_function(b)

print 'From the Chi-Square distribution for X:'
print 'P(',a,'< X < ',(b),') = ',c
print 'with'
print 'P( X < ',a,') = ',a1
print 'P( X < ',b,') = ',b1

f = x^(n/2-1)*e^(-x/2)/(gamma(n/2)*2^(n/2))
G = plot(f,x,0,b+(b-a)/2)+plot(f,x,a,b,thickness=5,color='green')
G += line([(a,0),(a,f(x=a))],color='green',thickness=3)
G += line([(b,0),(b,f(x=b))],color='green',thickness=3)
G += text(str(c.n(digits=5)),((a+b)/2,f(x=(a+b)/2)/3),color='green')
G.show()
\end{sageinput}
 \begin{example}{- Two-sided Confidence interval for \(\sigma^2\) and \(\sigma\).}{example-45}%
\hypertarget{p-1142}{}%
Given the data 570, 561, 546, 540, 609, 580, 550, 577, 585, determine a 95% confidence interval for \(\sigma^2\).%
\par
\hypertarget{p-1143}{}%
Using the computational forumaula (or your calculator) gives \(s^2 \approx 479.5\). Also, notice for n=9, the resulting interval will use a Chi-square variable with 8 degrees of freedom. Using the symmetric option, gives \(\chi_{0.025}^2 = 2.18\) and \(\chi_{0.975}^2 = 17.53\).  Therefore%
\begin{equation*}
E_1 = \frac{8 \cdot 479.5}{17.53} \approx 221.095
\end{equation*}
and%
\begin{equation*}
E_2 = \frac{8 \cdot 479.5}{2.18} \approx 1759.63.
\end{equation*}
Hence, you are 95% certain that%
\begin{equation*}
221.095 \lt \sigma^2 \lt 1759.63.
\end{equation*}
By taking square roots you get%
\begin{equation*}
14.87 \lt \sigma \lt 41.95.
\end{equation*}
Notice, this interval is relatively wide which is a result both of the number of data values being relatively small (n=9) and the actual data values being relatively large and spread out.%
\end{example}
 The example below uses the specific chi-square values given in the interactive cell below: \begin{sageinput}
# Chi-Square Calculator specifics
var('t')
c=0.95
n=399
alpha = 1-c
T = RealDistribution('chisquared', n)
a = T.cum_distribution_function_inv(alpha/2)
a1 = T.cum_distribution_function(a)
b = T.cum_distribution_function_inv(1-alpha/2)
b1 = T.cum_distribution_function(b)

print 'From the Chi-Square distribution for X:'
print 'P(',a,'< X < ',(b),') = ',c
print 'with'
print 'P( X < ',a,') = ',a1
print 'P( X < ',b,') = ',b1
\end{sageinput}
 \begin{example}{- Two-sided Confidence interval for the variance and standard deviation with large n..}{example-46}%
\hypertarget{p-1144}{}%
Continuing the previous example, suppose now that you have n=400 data values and suppose you have computed from those a sample variance of \(s^2 = 479.5\).  Then, the only change in the calculation is the two chi-square statistic values. For 95% but now with 399 degrees of freedom \(\chi_{0.025}^2 = 345.55\) and \(\chi_{0.975}^2 = 456.24\). Therefore%
\begin{equation*}
E_1 = \frac{8 \cdot 479.5}{456.24} \approx 419.3
\end{equation*}
and%
\begin{equation*}
E_2 = \frac{8 \cdot 479.5}{345.55} \approx 553.7.
\end{equation*}
Hence, you are 95% certain that%
\begin{equation*}
419.24 \lt \sigma^2 \lt 553.7.
\end{equation*}
By taking square roots you get%
\begin{equation*}
20.48 \lt \sigma \lt 23.53
\end{equation*}
which is a relatively tight confidence interval.  Notice, these are also completely contained in the confidence intervals from the previous small n example.%
\end{example}
%
\par
\hypertarget{p-1145}{}%
Similar to above, another choice to estimate \(\sigma ^2\) is to use a one sided confidence interval. If you want to find one of these, continue as described above but just leave one endpoint off.  Indeed,%
\begin{equation*}
\sigma^2 \lt E_2
\end{equation*}
can be determined using%
\begin{equation*}
F(\chi^2_{\alpha} ) = F \left ( \frac{(n-1)S^2}{E_2} \right ) = \alpha
\end{equation*}
and%
\begin{equation*}
E_1 \lt \sigma^2 
\end{equation*}
can be determined using%
\begin{equation*}
F(\chi^2_{1-\alpha} ) = F \left ( \frac{(n-1)S^2}{E_1} \right ) = 1 - \alpha.
\end{equation*}
\begin{example}{- One-sided Confidence intervals for \(\sigma^2\).}{example-47}%
\hypertarget{p-1146}{}%
%
\end{example}
%
\par
\hypertarget{p-1147}{}%
Finally, to determine a confidence interval for \(\sigma\), proceed using the protocols described above and simply take the square root on the resulting interval. \begin{example}{- Confidence intervals for \(\sigma\).}{example-48}%
\hypertarget{p-1148}{}%
%
\end{example}
%
%
%
\typeout{************************************************}
\typeout{Section 10.7 Exercises}
\typeout{************************************************}
%
\section[{Exercises}]{Exercises}\label{section-71}
\hypertarget{p-1149}{}%
\begin{inlineexercise}{- Basic Confidence interval for p.}{exercise-74}%
\hypertarget{p-1150}{}%
Given Y = 30 successes in n = 100 trials, determine a 90% confidence interval for the unknown value for p. \textbf{Solution}.\quad%
\hypertarget{p-1151}{}%
\(\tilde{p} = 0.3\) and \(z_{0.05} = 1.645\) gives%
\begin{equation*}
0.3 - 1.645 \sqrt{\frac{0.3 \cdot 0.7}{100}} \lt p \lt 0.3 + 1.645 \sqrt{\frac{0.3 \cdot 0.7}{100}}
\end{equation*}
or%
\begin{equation*}
0.225 \lt p \lt 0.375.
\end{equation*}
%
%
\end{inlineexercise}
 \begin{inlineexercise}{- Sample Size for confidence interval for p.}{exercise-75}%
\hypertarget{p-1152}{}%
Given a preliminary estimate \(\tilde{p_0} = 0.23\), determine the same size needed for determine a 95% confidence interval for p with margin of error 0.02. \textbf{Solution}.\quad%
\hypertarget{p-1153}{}%
Using \(z_{0.025} = 1.96\),%
\begin{equation*}
n \gt \big ( \frac{1.96}{0.02} \big )^2 \cdot 0.23 \cdot 0.77 \approx 1700.87
\end{equation*}
and so pick at least 1701 as the sample size.%
%
\end{inlineexercise}
 \begin{inlineexercise}{- Voting projection.}{exercise-76}%
\hypertarget{p-1154}{}%
Randomly polling 3200 eligible voters for governor in a particular state resulted in finding that 1590 favored your candidate. Determine an appropriate 95% confidence interval for the true proportion p of voters who favor your candidate. Noting that \(\tilde{p}\) in this instance is smaller than 50%, write a short paragraph regarding what you might conclude from this confidence interval regarding your candidate's chances in winning the election. \textbf{Solution}.\quad%
\hypertarget{p-1155}{}%
Note that although the point estimate is below 50%, the confidence interval includes the possibility that the actual value for p is greater than 50%. So, you cannot conclude that your candidate will either win or lose.%
%
\end{inlineexercise}
 \begin{inlineexercise}{- Basic Confidence interval for the mean.}{exercise-77}%
\hypertarget{p-1156}{}%
Given a sample mean of \(\overline{x} = 25.3\) with n = 121 and sample variance \(s^2 = 12.1\), determine a 99% confidence interval for the true mean \(\mu\). \textbf{Solution}.\quad%
\hypertarget{p-1157}{}%
Using \(z_{0.005} = 2.576\) and \(s = \sqrt{12.1} \approx 3.4786\) gives a confidence interval%
\begin{equation*}
25.3 - 2.576 \cdot \frac{3.4786}{11} \lt \mu \lt 25.3 + 2.576 \cdot \frac{3.4786}{11}
\end{equation*}
or%
\begin{equation*}
24.4854 \lt \mu \lt 26.1148.
\end{equation*}
%
%
\end{inlineexercise}
 \begin{inlineexercise}{- Confidence Interval Experiment.}{exercise-78}%
\hypertarget{p-1158}{}%
%
\end{inlineexercise}
 Roll two regular pair of dice 35 times, recording the sum of the dots for each roll.  Using the data from your sample, determine the corresponding sample mean and sample variance.  Using this data, create a 95% confidence interval for the population mean and a 95% "centered" confidence interval for the standard deviation. Once complete, compare your results with what you know should be the correct population statistics an appropriate "hat" distribution. (You might want to use the chi-square calculator provided earlier in this text.)%
\par
\hypertarget{p-1159}{}%
Go back over your 35 rolls and count the number of 7's or 11's rolled. Determine a corresponding relative frequency for this outcome. Using this data, create a 95% confidence interval for the theoretical proportion of success p.  Compare your result with what you know should be the correct theoretical p.%
\par
\hypertarget{p-1160}{}%
Repeat this exercise but this time roll 105 times.  Notice how these differ from the confidence intervals created with the smaller set.  Write a paragraph describing how these compare and whether one is better or not than the other.%
%
%
\typeout{************************************************}
\typeout{Chapter 11 Hypothesis Testing}
\typeout{************************************************}
%
\chapter[{Hypothesis Testing}]{Hypothesis Testing}\label{HypothesisTesting}
%
%
\typeout{************************************************}
\typeout{Section 11.1 Introduction}
\typeout{************************************************}
%
\section[{Introduction}]{Introduction}\label{section-72}
\hypertarget{p-1161}{}%
When creating confidence intervals, you started with a sample and used the sample statistic (sample mean, relative frequency, sample variance, etc.) to anchor an interval which (with high possibility) contains the corresponding population statistic \(\mu, p, \sigma^2\), etc. In this section, we instead start with an educated guess for one of the population statistics \(\mu, p, \sigma^2\) and then statistically compare that value with the subsequently collected sample statistic.%
\par
\hypertarget{p-1162}{}%
The educated guess noted above is often called the "null hypothesis" and should be considered as a guess that one tries to disprove if possible by using a subsequent statistical sample.%
%
%
\typeout{************************************************}
\typeout{Section 11.2 Hypotheses and Errors}
\typeout{************************************************}
%
\section[{Hypotheses and Errors}]{Hypotheses and Errors}\label{section-73}
\hypertarget{p-1163}{}%
In formulating a hypothesis, you will be making a declarative statement (a proposition) that has an actual truth value. That is, it is either true or it is not true in real life. However, since we will assess that truth using a test sample then measuring that truth value will never be 100% certain in the same manner that confidence intervals can never include 100% of the possible choices for a particular statistic. Hence, it is possible for you to make an incorrect conclusion.%
\par
\hypertarget{p-1164}{}%
In general, there are four different outcomes that are possible when testing a hypothesis: \leavevmode%
\begin{itemize}[label=\textbullet]
\item{}Your hypothesis is true and you determine that it is true.%
\item{}Your hypothesis is false but you determine that it is true.%
\item{}Your hypothesis is true but you determine that it is false.%
\item{}Your hypothesis is false and you determine that it is false.%
\end{itemize}
 The first and last cases are "good" since you have accurately determined the truth of the hypothesis. The second and third are however bad since you either believe something that is not true or you don't believe something that is true. We would like to minimize the likelihood of allowing these last two possibilities.%
\par
\hypertarget{p-1165}{}%
Toward that end, let's consider the case where the hypothesis is true but you determine (in error) that it is false. This is called a Type I error and we will designate the probability of this error by \(\alpha\). To lower the risk of a Type I error, you will want to make \(\alpha\) smaller.  In general, \(\alpha\) is also called "the significance level" with \(\alpha = 0.05\) a common choice with 0.01 and 0.10 also sometimes used. In general, any value between 0 and 1 is ok but large values mean large likelihood for error so choosing a value closer to 0 is preferred.%
\par
\hypertarget{p-1166}{}%
In a similar manner, consider the case where the hypothesis is false but you determine (in error) that it is true. This is called a Type II error and will be denote the probability of a Type II error by \(\beta\). Again, your goal is to make the risk of a Type II error smaller and therefore want \(\beta\) to be as small as possible.%
\par
\hypertarget{p-1167}{}%
In the following sections you will play around with minimizing Type I and Type II errors. Type I errors will be minimized by simply choosing a smaller value for \(\alpha\) when working through the formulas. Type II errors will be minimized by talking (if possible) a larger sample size when computing the needed sample statistics.%
\par
\hypertarget{p-1168}{}%
Toward that end, you will compose in each case a \emph{Null Hypothesis} (denoted \(N_0\)). This statement is what you will test for truth. You will also compose an \emph{Alternate Hypothesis} (denoted \(N_a\)) that is often the logical complement (but not always) of \(N_0\). The null hypothesis is often a statement corresponding to the likelihood that observations occur purely by chance while the alternate hypothesis will often indicate that outcomes are not actually random but are influenced by some (possibly unknown) causes.  If our sample shows that the null hypothesis \(N_0\) is false, then we will accept the alternate \(N_a\). If this is a bad decision then it will be true that the hypothesis is true but you will have determined that it is false...that is, made a Type I error.%
\par
\hypertarget{p-1169}{}%
To avoid ever making an Type II error, often often never "accepts" the null hypothesis \(N_0\) even if the sample does not conclude that it is false. In other words, if you do this then you will never allow yourself to determine that \(N_0\) is true.  Seems odd but this is one way to avoid ever worrying much about type II errors. The best way to avoid this blind spot is to use relatively large test samples and in doing so you will minimize the likelihood of type II.%
\par
\hypertarget{p-1170}{}%
So, our plan of attack is to find some way to determine, from a sample, the amount of Type I error we might make. For a given problem, from the sample, you will create a specific estimate for Type I error...called a p-statistic...and compare to the chosen significance level \(\alpha\). Sounds easy enough?%
\par
\hypertarget{p-1171}{}%
Finally, you will notice in each of the instances presented that the form of the solution method will be very similar to the forms we used in creating confidence intervals. The difference is that for hypothesis testing we \emph{assume} the value in the middle and test to see whether the sample statistic is in one of the tails so that we can reject rather than with confidence intervals we assume as sample statistic is in the middle and then use that sample to create boundaries within which the theoretical population statistic \emph{must} like with high confidence.%
%
%
\typeout{************************************************}
\typeout{Section 11.3 Hypothesis Test for one proportion}
\typeout{************************************************}
%
\section[{Hypothesis Test for one proportion}]{Hypothesis Test for one proportion}\label{section-74}
\hypertarget{p-1172}{}%
In this section, you will consider the following options for null hypothesis and corresponding alternate hypothesis with respect to the unknown value p:%
\begin{equation*}
H_0 : p = p_0 \\ H_a : p \neq p_0
\end{equation*}
or%
\begin{equation*}
H_0 : p \le p_0 \\ H_a : p \gt p_0
\end{equation*}
or%
\begin{equation*}
H_0 : p \ge p_0 \\ H_a : p \lt p_0.
\end{equation*}
For any given problem, we choose only one of these three options. The first is called a "two-tailed" test since the alternate hypothesis can not be equal if it is actually larger or smaller. The last two are called "one-tailed" tests since the alternate hypothesis only allows for being on one side.  Note that some people will write all of these null hypothesis options using equality but the alternate hypothesis determines the number of tails.%
\par
\hypertarget{p-1173}{}%
From the Central Limit Theorem, you found that every interesting distribution eventually becomes approximately normal. This includes the binomial distribution with mean \(\mu = n p_0\) and variance \(\sigma^2 = n p_0 (1-p_0)\). Hence, the \emph{z-statistic}%
\begin{equation*}
Z = \frac{X - n p_0}{\sqrt{n p_0(1-p_0)}} = \frac{p - p_0}{\sqrt{p_0(1-p_0)/n}}
\end{equation*}
is approximately standard normal and so probabilities on this statistic can be computed as needed using the normal distribution.%
\par
\hypertarget{p-1174}{}%
Let's look at an example for this by considering:%
\begin{equation*}
H_0: p = 0.20
\end{equation*}
vs%
\begin{equation*}
H_a: p \ne 0.20.
\end{equation*}
Tests like this are called \emph{two-tailed} since there are two ways to reject the null hypothesis: we find that p should be less than 0.2 or we find that p should be greater than 0.2.%
\par
\hypertarget{p-1175}{}%
To test our hypothesis, let's now chose a significance level of \(\alpha = 0.05\) and take a sample. Presuming we actually do this, let's assume that we find that out of n=100 sample values we get X = 27 successes. Hence, our actual test statistic is \(p = \frac{27}{100} = 0.27\).%
\par
\hypertarget{p-1176}{}%
So, in this case we have%
\begin{equation*}
\sigma = \sqrt{\frac{p_0(1-p_0)}{n}} = \sqrt{\frac{0.2 \cdot 0.8}{100}} = 0.04
\end{equation*}
and the z-statistic (using the normal distribution) for the sample statistic of p = 0.27 is%
\begin{equation*}
z = \frac{0.27 - 0.2}{0.04} = 1.75.
\end{equation*}
Remember, the alternate hypothesis has two tails so to determine the P value we need to determine from the normal distribution%
\begin{equation*}
P(Z \gt 1.75) + P(Z \lt -1.75)
\end{equation*}
and find that this has probability approximately 0.0392 + 0.0392 = 0.0784.  However, this P value is greater than our significance level \(\alpha = 0.05\) so we cannot reject the null hypothesis at the 5 percent significance level. However, if we had chosen initially to use a 10 percent significance level then we would have rejected the null hypothesis and accepted the alternate.%
\par
\hypertarget{p-1177}{}%
One tailed test for p using a "real" example...%
\par
\hypertarget{p-1178}{}%
\begin{inlineexercise}{WebWork.}{exercise-79}%
\hypertarget{p-1179}{}%
Two-tailed test for p.%
\par\medskip
\hypertarget{p-1180}{}%
An article in the Washington Post on March 16, 1993 stated that nearly 45 percent of  all Americans have brown eyes.  A random sample of \(n = 
78\) C of I students found 30 with brown eyes.%
\par
\hypertarget{p-1181}{}%
We test%
\par
\hypertarget{p-1182}{}%
\(H_0: p = .45\)%
\par
\hypertarget{p-1183}{}%
\(H_a: p \neq .45\)%
\par
\hypertarget{p-1184}{}%
(a) What is the \(z\)-statistic for this test? \(\)  \fillin{10}%
\par
\hypertarget{p-1185}{}%
(b) What is the P-value of the test?  \(\)  \fillin{10}%
\end{inlineexercise}
%
\par
\hypertarget{p-1186}{}%
\begin{inlineexercise}{WebWork.}{exercise-80}%
\hypertarget{p-1187}{}%
One-tailed test for p. Notice, in this case you will only compute z-score probability for one tail and not both tails.%
\par\medskip
\hypertarget{p-1188}{}%
A noted psychic was tested for ESP.  The psychic was presented with 220 cards face down and was asked to determine if the card was one of 5 symbols: a star, cross, circle, square, or three wavy lines.  The psychic was correct in 52 cases.  Let \(p\) represent the probability that the psychic correctly identifies the symbol on the card in a random trial. Assume the 220 trials can be treated as an SRS from the population of all guesses.%
\par
\hypertarget{p-1189}{}%
To see if there is evidence that the psychic is doing better than just guessing, we test%
\par
\hypertarget{p-1190}{}%
\(H_0: p = .2\)%
\par
\hypertarget{p-1191}{}%
\(H_a: p > .2\)%
\par
\hypertarget{p-1192}{}%
(a) What is the \(z\)-statistic for this test? \(\)  \fillin{10}%
\par
\hypertarget{p-1193}{}%
(b) What is the P-value of the test?  \(\)  \fillin{10}%
\end{inlineexercise}
%
%
%
\typeout{************************************************}
\typeout{Section 11.4 Hypothesis Test for one mean}
\typeout{************************************************}
%
\section[{Hypothesis Test for one mean}]{Hypothesis Test for one mean}\label{section-75}
\hypertarget{p-1194}{}%
In this section, you will consider the following options for null hypothesis and corresponding alternate hypothesis with respect to the unknown value \(\mu\):%
\begin{equation*}
H_0 : \mu = \mu_0 \\ H_a: \mu \neq \mu_0
\end{equation*}
or%
\begin{equation*}
H_0 : \mu \le \mu_0 \\ H_a: \mu \gt \mu_0
\end{equation*}
or%
\begin{equation*}
H_0 : \mu \ge \mu_0 \\ H_a:\mu \lt \mu_0
\end{equation*}
Again, we choose only one of these three options and as before the first is called a "two-tailed" test and the last two are called "one-tailed" tests.  Note that some people will write all of these null hypothesis options using equality but the alternate hypothesis determines the number of tails.%
\par
\hypertarget{p-1195}{}%
Once again, if the test sample size is sufficiently large and the standard deviation \(\sigma\) of the underlying distribution is known, one can use the normal distribution to compute probabilities. If the test sample size is relatively small or if \(\sigma\) is not known (and therefore is approximated by the sample standard deviation s), then the t-distribution can be utilized to compute probabilities. We will only consider using the t-distribution to compute p-values.%
\par
\hypertarget{p-1196}{}%
The Standard Error \(\sigma_e\) is given by%
\begin{equation*}
\sigma_e = \frac{s}{\sqrt{n}}
\end{equation*}
where s is the standard deviation of the sample. For this presentation, we will assume that the actual population is relatively large relative to the sample size. In cases where this is not true, an adjustment (not presented here) will need to be made when computing \(\sigma_e\).%
\par
\hypertarget{p-1197}{}%
To determine the p-value for a given sample with sample mean \(\overline{x}\),%
\begin{equation*}
t = \frac{\overline{x}-\mu}{\frac{s}{\sqrt{n}}}
\end{equation*}
is a t-variable with n-1 degrees of freedom. Therefore, probabilities on t can be computed using the t-distribution.%
\par
\hypertarget{p-1198}{}%
Consider a Two-tailed Hypothesis test for \(\mu\) using%
\begin{equation*}
H_0 : \mu = 200 \\ H_a: \mu \neq 200
\end{equation*}
using a sample of size n = 49 and with a resulting mean of \(\overline{x} = 206\), a sample standard deviation of \(s = 15\), and a significance level of \(\alpha = 0.01\).%
\par
\hypertarget{p-1199}{}%
The standard error for this test is%
\begin{equation*}
\sigma_e = \frac{15}{\sqrt{49}} = \frac{15}{7}
\end{equation*}
and so using the t-distribution with degrees of freedom n-1 = 48 yields a t-statistic of%
\begin{equation*}
t = \frac{\overline{x} - 200}{\sigma_e} = \frac{206-200}{\frac{15}{7}} = \frac{14}{5} = 2.80.
\end{equation*}
To compute the p-value,%
\begin{equation*}
P(t \gt 2.80) + P(t \lt 2.80) \approx 0.0037 + 0.0037 = 0.0074.
\end{equation*}
Since this p-value is less than our significance level \(\alpha = 0.01\) then you can reject the null hypothesis and accept the alternate.%
\par
\hypertarget{p-1200}{}%
Consider One-tailed Hypothesis test for \(\mu\) using an interesting application:%
\par
\hypertarget{p-1201}{}%
Suppose that a manufacturer bottling a delicious beverage and the label indicates that the bottle contains 16 fluid ounces. Since providing the customer too little product might cause a significant negative reaction relative to the modest additional cost of providing a little too much, consider the following hypothesis test:%
\begin{equation*}
H_0 : \mu = 16 \\ H_a: \mu \gt 200.
\end{equation*}
To test this hypothesis at significance level \(\alpha = 0.05\), you randomly pull out 20 bottles from the production line and accurately measure the amount of produce in each bottle. If the resulting average of these measurements is \(\overline{x} = 16.05\) ounces with a standard deviation of \(s = 0.08\) ounces, determine if you can safely make it known that more product is actually delivered in general to each consumer.%
\par
\hypertarget{p-1202}{}%
The standard error for this test is%
\begin{equation*}
\sigma_e = \frac{0.08}{\sqrt{20}} \approx 0.01789
\end{equation*}
and using the t-distribution with n-1 = 19 degrees of freedom yields a t-statistic of%
\begin{equation*}
t = \frac{\overline{x} - 16}{\sigma_e} = \frac{16.05-16}{0.01789} \approx 2.795.
\end{equation*}
To compute the p-value,%
\begin{equation*}
P(t \gt 2.795) \approx 0.0058.
\end{equation*}
Since this p-value is less than our significance level \(\alpha = 0.05\) (by a lot) then you can reject the null hypothesis and accept the alternate. It is safe therefore to say that customers can expect at least 16 ounces! However, note that some folks will still be stiffed since the standard deviation of 0.08 certainly means that some bottles have less than 16.05-0.08 ounces of beverage.%
%
%
\typeout{************************************************}
\typeout{Section 11.5 Hypothesis Test for one variance}
\typeout{************************************************}
%
\section[{Hypothesis Test for one variance}]{Hypothesis Test for one variance}\label{section-76}
\hypertarget{p-1203}{}%
Proceeding in a similar manner, we can also perform hypothesis testing on variances using the \(\Chi^2\)-distribution to determine probabilities.%
\begin{equation*}
H_0 : \sigma^2 = \sigma_0^2  \\ H_a: \sigma^2 \neq \sigma_0^2
\end{equation*}
or%
\begin{equation*}
H_0 : \sigma^2 \ge \sigma_0^2  \\ H_a: \sigma^2 \lt \sigma_0^2
\end{equation*}
or%
\begin{equation*}
H_0 : \sigma^2 \le \sigma_0^2  \\ H_a: \sigma^2 \gt \sigma_0^2
\end{equation*}
%
\begin{equation*}
test statistic = T = (n-1) \frac{s^2}{\sigma_0^2}
\end{equation*}
For two-tailed, reject if%
\begin{equation*}
T \gt \Chi_{1-\alpha/2,n-1}^2 \text{ or } T \lt \Chi_{\alpha/2,n-1}^2
\end{equation*}
and for one-tailed to the right if%
\begin{equation*}
T \gt \Chi_{1-\alpha,n-1}^2
\end{equation*}
and for one-tailed to the left if%
\begin{equation*}
T \lt \Chi_{\alpha,n-1}^2.
\end{equation*}
Technically, for the two-tailed test you could pick T-values so that the total probability sums to \(\alpha\) in any fashion but generally this probability is split evenly between the two tails as noted above.%
%
%
\typeout{************************************************}
\typeout{Section 11.6 Summary}
\typeout{************************************************}
%
\section[{Summary}]{Summary}\label{section-77}
\hypertarget{p-1204}{}%
TBA%
%
%
\typeout{************************************************}
\typeout{Section 11.7 Exercises}
\typeout{************************************************}
%
\section[{Exercises}]{Exercises}\label{section-78}
\hypertarget{p-1205}{}%
TBA%
%
%
\typeout{************************************************}
\typeout{Chapter 12 Review of Calculus}
\typeout{************************************************}
%
\chapter[{Review of Calculus}]{Review of Calculus}\label{PowerSeriesReview}
\hypertarget{p-1206}{}%
This chapter is a review of power series results from Calculus.%
%
%
\typeout{************************************************}
\typeout{Section 12.1 Geometric Series}
\typeout{************************************************}
%
\section[{Geometric Series}]{Geometric Series}\label{section-79}
\hypertarget{p-1207}{}%
Knowledge of the use of power series is very important when dealing with both probability functions.%
%
\begin{gather*}
S = \sum_{k=0}^{\infty} {x^k} = \frac{1}{1-x}
\end{gather*}
\hypertarget{p-1208}{}%
as is its extension know as the negative binomial series \(( n \in \mathbb{N} )\).%
%
\begin{gather*}
NB = \sum_{k=0}^{\infty} (-1)^k \binom{-n + k - 1}{k} {x^k b^{-n-k}} = \frac{1}{(x+b)^n}
\end{gather*}
\hypertarget{p-1209}{}%
In this section, we review this series, develop its properties, and explore some of its extensions.%
%
%
\typeout{************************************************}
\typeout{Subsection 12.1.1 Geometric Series}
\typeout{************************************************}
%
\subsection[{Geometric Series}]{Geometric Series}\label{subsection-7}
\begin{theorem}{}{}{theorem-GeomSeries}%
\(S = \sum_{k=0}^{\infty} {x^k} = \frac{1}{1-x}\)\end{theorem}
\begin{proof}\hypertarget{proof-75}{}
\hypertarget{p-1210}{}%
Consider the partial sum%
%
\begin{gather*}
S_n = \sum_{k=0}^{n} {x^k} = 1 + x + x^2 + ... + x^n \\
(1-x)S_n = S_n - x S_n = 1 + x + x^2 + ... + x^n - (x + x^2 + ... + x^n + x^{n+1}) = 1 - x^{n+1} \\
\Rightarrow S_n = \frac{1-x^{n+1}}{1-x} 
\end{gather*}
\hypertarget{p-1211}{}%
and so as \(n \rightarrow \infty \),%
%
\begin{gather*}
S_n \rightarrow S = \frac{1}{1-x} \qedhere
\end{gather*}
\end{proof}
\hypertarget{p-1212}{}%
The interactive activity below shows how well the partial sums approximate \(\frac{1}{1-x}\) as the number of terms increases.%
\begin{sageinput}
var('x,n,k')
f = 1/(1-x)
@interact
def _(n = slider(2,20,1,2)):
	Sn = sum(x^k,k,0,n)
	pretty_print(html('$S_n(x) = %s$'%str(latex(Sn))))
	G = plot(f,x,-1,0.9,color='black')
	G += plot(Sn,x,-1,0.9,color='blue')
	G += plot(abs(f-Sn),x,-1,0.9,color='red')
	G.show(title="Partial Sums (blue) vs Infinite Series (black) and Error (red)",figsize=(5,4))
\end{sageinput}
%
%
\typeout{************************************************}
\typeout{Subsection 12.1.2 Alternate Forms for the Geometric Series}
\typeout{************************************************}
%
\subsection[{Alternate Forms for the Geometric Series}]{Alternate Forms for the Geometric Series}\label{subsection-8}
\begin{theorem}{Generalized Geometric Series.}{}{theorem-79}%
\(k \in \mathbb{N}, \sum_{k=M}^{\infty} {x^k} = \frac{x^M}{1-x}\)\end{theorem}
\begin{proof}\hypertarget{proof-76}{}
%
\begin{align*}
\sum_{k=M}^{\infty} {x^k} & = x^M \sum_{k=0}^{\infty} {x^k}\\
& = x^M \frac{1}{1-x}\\
& = \frac{x^M}{1-x}\qedhere
\end{align*}
\end{proof}
\begin{example}{Integrating and Differentiating to get new Power Series.}{example-49}%
\hypertarget{p-1213}{}%
The geometric power series is a nice function which is relatively easily differentiated and integrated. In doing so, one can obtain new power series which might also be very useful in their own right.  Here we develop a few which are of special interest.%
\par
\hypertarget{p-1214}{}%
Let \(f(x) = \sum_{k=0}^\infty x^k = \frac{1}{1-x}\).  Then,%
%
\begin{gather*}
f'(x) = \sum_{k=1}^{\infty} {kx^{k-1}} = \frac{1}{(1-x)^2}\\
f''(x) = \sum_{k=2}^{\infty} {k(k-1)x^{k-1}} = \frac{2}{(1-x)^3}\\
f^{(n)}(x) = \sum_{k=n}^{\infty} {k(k-1)...(k-n+1)x^{k-n}} = \frac{n!}{(1-x)^{n+1}}\\
\int f(x) dx = \sum_{k=0}^{\infty} {\frac{x^{k+1}}{k+1}} = -ln(1-x)
\end{gather*}
\end{example}
\begin{example}{Playing with the base.}{example-50}%
%
\begin{align*}
\sum_{k=0}^{\infty} {a^k x^k} & = \sum_{k=0}^{\infty} {(ax)^k}\\
& = \frac{1}{1-ax}, |x| \lt \frac{1}{a}
\end{align*}
\hypertarget{p-1215}{}%
or perhaps%
%
\begin{gather*}
\sum_{k=0}^{\infty} {(x-b)^k} = \frac{1}{1-(x-b)}, |x-b| \lt 1
\end{gather*}
\end{example}
\begin{example}{Application: Converting repeating decimals to fractional form.}{example-51}%
\hypertarget{p-1216}{}%
Consider this example:%
%
\begin{align*}
2.48484848... & = 2 + 0.48 + 0.0048 + 0.000048 + ...\\
&  = 2 + 0.48(1 + 0.01 + 0.0001 + ... ) = 2 + 0.48 \sum_{k=0}^\infty (0.01)^k
\end{align*}
\hypertarget{p-1217}{}%
Therefore, applying the Geometric Series%
%
\begin{align*}
2.48484848... & = 2 + 0.48 \frac{1}{1-0.01} \\
& = 2 + 0.48 \frac{100}{99} = 2 + \frac{48}{99} 
\end{align*}
\end{example}
\begin{example}{Playing around with repeating decimals.}{example-52}%
\hypertarget{p-1218}{}%
Certainly most students would agree that \(0.333333... = \frac{1}{3} \). So, what about \(0.999999...\)? Simply follow the pattern above%
%
\begin{align*}
0.999999... & = 0.9 + 0.09 + 0.009 + 0.0009 + ... = 0.9(1 + 0.1 + 0.1^2 + 0.1^3 + ...\\
& = 0.9 \frac{1}{1-0.1} = 0.9 \frac{1}{0.9} = 1 
\end{align*}
\end{example}
%
%
\typeout{************************************************}
\typeout{Section 12.2 Binomial SumsBinomial SeriesTrinomial Series}
\typeout{************************************************}
%
\section[{Binomial SumsBinomial SeriesTrinomial Series}]{Binomial SumsBinomial SeriesTrinomial Series}\label{section-80}
\hypertarget{p-1219}{}%
The binomial series is also foundational. It is technically not a series since the sum_if finite but we won’t bother with that for now. It is given by%
%
\begin{gather*}
B = \sum_{k=0}^{n} {\binom{n}{k} a^k b^{n-k}}
\end{gather*}
\begin{theorem}{Binomial Theorem.}{}{theorem-Binomial}%
\(n \in \mathbb{N} \)\(\displaystyle {(a+b)^n = \sum_{k=0}^{n} {\binom{n}{k} a^k b^{n-k}}}\)\end{theorem}
\begin{proof}\hypertarget{proof-77}{}
\hypertarget{p-1220}{}%
By induction:%
\par
\hypertarget{p-1221}{}%
Basic Step: n = 1 is trivial%
\par
\hypertarget{p-1222}{}%
Inductive Step:  Assume the statement is true as given for some \(n \ge 1\). Show \((a+b)^{n+1} = \sum_{k=0}^{n+1} {\binom{n+1}{k} a^k b^{n+1-k}}\)%
%
\begin{align*}
(a+b)^{n+1} & = (a+b)(a+b)^n\\
& = (a+b)\sum_{k=0}^{n} {\binom{n}{k} a^k b^{n-k}}\\
& = \sum_{k=0}^n \binom{n}{k} a^{k+1} b^{n-k} + \sum_{k=0}^n \binom{n}{k} a^k b^{n-k+1}\\
& = \sum_{k=0}^{n-1} \binom{n}{k} a^{k+1} b^{n-k} + a^{n+1} + b^{n+1} + \sum_{k=1}^n \binom{n}{k} a^k b^{n-k+1}\\
& = \sum_{j=1}^n \binom{n}{j-1} a^j b^{n-(j-1)} + a^{n+1} + b^{n+1} + \sum_{k=1}^n \binom{n}{k} a^k b^{n+1-k}\\
& = b^{n+1} + \sum_{k=1}^n \left [ \binom{n}{k-1} + \binom{n}{k} \right ] a^k b^{n+1-k} + a^{n+1}\\
& = b^{n+1} + \sum_{k=1}^n \binom{n+1}{k} a^k b^{n+1-k} + a^{n+1}\\
& = \sum_{k=0}^{n+1} \binom{n+1 }{k} a^k b^{n+1-k}\qedhere
\end{align*}
\end{proof}
\hypertarget{p-1224}{}%
Consider \(B(a,b) = \sum_{k=0}^{n} {\binom{n}{k} a^k b^{n-k}}\). This finite sum_is known as the Binomial Series.%
\par
\hypertarget{p-1225}{}%
Show that \(B(a,b) = (a+b)^n\)%
\par
\hypertarget{p-1226}{}%
Show that \(B(1,1) = 2^n\)%
\par
\hypertarget{p-1227}{}%
Show that \(B(-1,1) = 0\)%
\par
\hypertarget{p-1228}{}%
Show that \(B(p,1-p) = 1\)%
\par
\hypertarget{p-1229}{}%
Easily, \(B(x,1) = \sum_{k=0}^{n} {\binom{n}{k} a^k}\)%
%
\begin{gather*}
(a+b+c)^n = \sum_{k_1+k_2+k_3=n}^{} {\binom{n}{k_1,k_2,k_3} a^{k_1} b^{k_2} c^{k_3}} 
\end{gather*}
\hypertarget{p-1230}{}%
where \(\binom{n}{k_1,k_2,k_3} = \frac{n!}{k_1!k_2!k_3!}\). This can be generalized to any number of terms to give what is know as a multinomial series.%
%
%
\typeout{************************************************}
\typeout{Section 12.3 Negative Binomial Series}
\typeout{************************************************}
%
\section[{Negative Binomial Series}]{Negative Binomial Series}\label{section-81}
\hypertarget{p-1231}{}%
\((a+b)^{-n} = \sum_{k=0}^{\infty} {\binom{-n}{k} a^k b^{-n-k}}\)%
\begin{theorem}{Alternate Form for Negative Binomial Series.}{}{theorem-81}%
\hypertarget{p-1232}{}%
\((a+b)^{-n} = \sum_{k=0}^{\infty} {(-1)^k \binom{n+k-1}{k} a^k b^{-n-k}}\)%
\end{theorem}
\end{document}