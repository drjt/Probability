<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the documentation of MathBook XML   -->
<!--                                                          -->
<!--    MathBook XML Author's Guide                           -->
<!--                                                          -->
<!-- Copyright (C) 2013-2016  Robert A. Beezer                -->
<!-- See the file COPYING for copying conditions.             -->

<chapter xml:id="NormalRelatedDistributions" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Normal Distributions</title>

<section xml:id="NormalRelatedDistributionsIntroduction"><title>Introduction</title>
<p>
You should have noticed by now that many distributions tend to have a bell-shaped graph as parameters are allowed to increase. Indeed, the formulas for skewness <m>\gamma_1</m> and kurtosis <m>\gamma_2</m> approach 0 and 3 respectively for the Hypergeometric, Binomial, Negative Binomial, Poisson, and Gamma Distributions. One might wonder if this is just a happy coincidence or is something more insidious at play. 
</p>

<p>
The answer by appealing to mathematics reveals that nothing sinister is going on but that it is indeed true that the eventual destiny for distributions is one that is bell-shaped. It is therefore of interest to figure out if that distribution has a nice form that can be accessed directly. The focus of this chapter is to consider this bell-shaped goal known as the "normal distribution."
</p>

<p>
We present the normal distribution by simply presenting it's probability function without derivation. In order to more carefully investigate the development of the normal distribution (and the Chi-Square Distribution) you will need to study "Moment Generating Functions" and some serious mathematics. Without supplying this rigor you can still utilize the results. 
</p>

</section>

<section xml:id="NormalDistribution"><title>The Normal Distribution</title>
	

<definition xml:id="NormalPF"><title>The Normal Distribution</title>
	<statement>
	<p>
	Given two parameters <m>\mu</m> and <m>\sigma</m>, a random variable X over <m>R = (-\infty,\infty)</m> has a normal distribution provided it has a probability function given by
		<me>
		f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{ -\left ( \frac{x-\mu}{\sigma} \right ) ^2 / 2}
		</me>
	</p>	
	</statement>
</definition>


<p>
The normal distribution is also sometimes referred to as the Gaussian Distribution (often by Physicists) or the Bell Curve (often by social scientists).
</p>


<sage>
	<input>
var('x,mu,sigma')
f(x) = e^(-((x-mu)/sigma)^2/2)/(sigma*sqrt(2*pi))
@interact
def _(m=slider(-10,10,1,0,label='$$\\mu$$'),s=slider(1/5,5,1/10,1,label='$$\\sigma$$')):
    titletext = "Normal Curve with mean "+str(m)+" and standard deviation "+str(s)
    G = plot(f(mu=m,sigma=s),(x,m-5*s,m+5*s))
    G += point((0,1),size=1)+point((12,0),size=1)+point((-12,0),size=1)
    G += point((m,f(x=m,mu=m,sigma=s)),color='red',size=20)
    G += point((m+s,f(x=m+s,mu=m,sigma=s)),color='green',size=20)
    G += point((m-s,f(x=m-s,mu=m,sigma=s)),color='green',size=20)    
    show(G,figsize=(5,3),title=titletext,ymin=0,ymax=1,xmin=-15,xmax=15)
    </input>
</sage>



<theorem xml:id="StandardNormalDistribution"><title>Standard Normal Distribution</title>
<statement>
<p>
If <m>\mu = 0</m> and <m>\sigma = 1</m>, then we say X has a standard normal distribution and often use Z as the variable name and will use <m>\Phi(z)</m> for the standard normal distribution function. In this case, the density function reduces to
		<me>f(z) = \frac{1}{\sqrt{2 \pi}} e^{ -z^2 / 2}</me>
</p>
</statement>
<proof>
		<p>Convert to "standard units" using the conversion 
			<me>z = \frac{x-\mu}{\sigma} = \frac{x-0}{1} = x.</me>
		</p>
</proof>
</theorem>


<p>
When in the standard normal distribution, you can use a graphing calculator's normalcdf(a,b) to approximate 
<me>P(a \le Z \le b) = \Phi(b) - \Phi(a).</me>  
Note that when no mean or standard deviation for normalcdf is provided, the calculator presumes standard normal.
</p>


	<p>
	Note that you can use a graphing calculator's 
	normalcdf(a,b) = <m> \Phi(b) - \Phi(a)</m> to compute probabilities in the standard normal distrubution.  
	If you have a normal distribution other than the standard and don't want to convert to standard, 
	then the graphing calculator usage is normalcdf(a,b,<m>\mu,\sigma</m>).
	</p>


<p>
<sage>
<input>
pretty_print("Calculator for Normal Probabilities")
@interact(layout=dict(top=[['a', 'b']],bottom=[['mu','sigma']]))
def _(a=input_box(-2,width=10,label='a = '),
      b=input_box(2,width=10,label='b = '),
      mu=input_box(0,width=8,label='$$\\mu = $$'),
      sigma=input_box(1,width=8,label='$$\\sigma = $$')):
    f = e^(-((x-mu)/sigma)^2/2)/(sigma*sqrt(2*pi))
    P = integral_numerical(f,a,b)[0]
    pretty_print(html("$$ P("+str(a)+" &lt; X &lt; "+str(b)+") \\approx "+str(P)+"$$"))
</input>
</sage>
</p>


<exercise><title>WebWork - Computing Standard Normal Probabilities</title>

		<webwork source="Library/Rochester/setProbability10NormalDist/ur_pb_10_1.pg">
		</webwork>
</exercise>

<exercise><title>WeBWorK - Computing Normal Probabilities when non-Standard</title>
	<webwork source="Library/UBC/STAT/Archive_old_questions/STAT203/hw03/hw03-q05.pg">
	</webwork>
</exercise>


<theorem><title>Verifying the normal probability function</title>
	<statement>
		<p>
		<me>\int_{-\infty}^{\infty} \frac{1}{\sigma \sqrt{2 \pi}} e^{ -\left ( \frac{x-\mu}{\sigma} \right ) ^2 / 2} dx = 1</me>
		</p>
	</statement>
<proof>
<p>
Note that you can convert the integral above to <xref ref="StandardUnits">standard units</xref> so that it is sufficient to show
<me>I = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{ -\frac{z^2}{2} } dz = 1</me>
Toward this end, consider <m>I^2</m> and change the variables to get
<md>
	<mrow>I^2 &amp; = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{ -\frac{u^2}{2} } du \cdot \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{ -\frac{v^2}{2} } dv</mrow>
	<mrow>&amp; = \frac{1}{2 \pi} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{ -\frac{u^2+v^2}{2} } du dv</mrow>
</md>
Converting to polar coordinates using 
<me> du dv = r dr d\theta </me> and
<me> u^2 + v^2 = r^2</me>
gives
<md>
	<mrow>I^2 &amp; = \frac{1}{2 \pi} \int_0^{2 \pi} \int_0^{\infty} e^{ -\frac{r^2}{2} } r dr d\theta</mrow>
	<mrow> &amp; = \frac{1}{2 \pi} \int_0^{2 \pi} -e^{ -\frac{r^2}{2} } \big |_0^{\infty} d\theta</mrow>
	<mrow> &amp; = \frac{1}{2 \pi} \int_0^{2 \pi} 1 \cdot d\theta</mrow>
	<mrow> &amp; = \frac{1}{2 \pi} \theta \big |_0^{2 \pi} = 1</mrow>
</md>
as desired.
</p>
</proof>
</theorem>
	
	
	

<theorem xml:id="NormalMean"><title>Verifying the normal probability mean</title>
<statement>
		<p>
		<me>E[X] = \int_{-\infty}^{\infty} x \cdot \frac{1}{\sigma \sqrt{2 \pi}} e^{ - \left ( \frac{x-\mu}{\sigma} \right ) ^2 / 2} dx = \mu</me>
		</p>
</statement>
<proof>
	<p>
	The definition of expected value for a continuous variable in this case gives an integral to evaluate since X is continuous. In that integral, it is useful to use a standard change of variables as in basic integral calculus to convert the integral to something easier to evaluate. In this case, you will want to convert the X variable to the standard units variable Z so that
	<me>z = \frac{x-\mu}{\sigma}</me>
	or by solving for x 
	<me> x = \mu + z \sigma.</me>
	So, it follows that
<md>
	<mrow>E[X] &amp;= \int_{-\infty}^{\infty} x \cdot \frac{1}{\sigma \sqrt{2 \pi}} e^{ - \left ( \frac{x-\mu}{\sigma} \right ) ^2 / 2} dx </mrow>
	<mrow> &amp;= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} (\mu + z\sigma) \cdot e^{ -z^2 / 2} dz</mrow>
	<mrow> &amp;= \mu \cdot \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{ -z^2 / 2} dz + \sigma \cdot \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} z \cdot e^{ -z^2 / 2} dz</mrow>
	<mrow> &amp;= \mu \cdot 1 + \sigma \cdot 0</mrow>
	<mrow> &amp; = \mu</mrow>
</md>
	and therefore the use of <m>\mu</m> as the parameter in the normal distribution probability function is warranted.
	</p>
</proof>
</theorem>
	
	
	

<theorem xml:id="NormalVariance"><title>Verifying the normal probability variance</title>
	<statement>
	<p>Using a similar change of variable as in the <xref ref="NormalMean">previous theorem</xref>,
		<me>E[(X-\mu)^2] = \int_{-\infty}^{\infty} (x-\mu)^2 \cdot \frac{1}{\sigma \sqrt{2 \pi}} e^{ - \left ( \frac{x-\mu}{\sigma} \right ) ^2 / 2} dx = \sigma^2</me>
	</p>
	</statement>
	<proof>
	<p>
	<md>
		<mrow>E[(X-\mu)^2] &amp; = \int_{-\infty}^{\infty} (x-\mu)^2 \cdot \frac{1}{\sigma \sqrt{2 \pi}} e^{ - \left ( \frac{x-\mu}{\sigma} \right ) ^2 / 2} dx</mrow>
		<mrow> &amp; = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} \sigma^2 z^2 \cdot  e^{ -z^2 / 2} dz</mrow>
		<mrow> &amp; = \frac{\sigma^2}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} z \cdot z e^{ -z^2 / 2} dz</mrow>

		<mrow> &amp; = \frac{\sigma^2}{\sqrt{2 \pi}} \cdot \big [ -z e^{-z^2 / 2} \big |_{-\infty}^{\infty} + \int_{-\infty}^{\infty}  e^{ -z^2 / 2}  dz \big ]</mrow>
		<mrow> &amp; = \frac{\sigma^2}{\sqrt{2 \pi}} \cdot \big [ 0 + \sqrt{2 \pi} \big ]</mrow>
		<mrow> &amp; = \sigma^2 </mrow>
	</md>
	using integration by parts.  So, the use of <m>\sigma</m> as the other parameter in the normal probability function is also warranted.
	</p>
	</proof>
</theorem>

<p>
Once again, you can use a graphing calculator's normalcdf(a,b,mu,sigma) to compute general normal probabilities.  Pay attention to the the order of terms. For normal distributions, the calculator function always requires an interval. If you are looking for a one-sided probability, such as <m>P(X \gt 4)</m> for a problem with mean <m>\mu = 2</m> and <m>\sigma = 3</m>, you can replace the infinite upper limit with "large" finite endpoint. Providing something that is more than 10 standard deviations above the mean is for all practical purposes infinity with respect to calculations. So, in this case <m>P(X \gt 4)</m> can be approximated by normalcdf(4,32,2,3).  If you are brave, you can go even higher and use normalcdf(4,100000,2,3) if desired. 
</p>


<exercise><title>WebWork - Computing Regular Normal Probabilities</title>

		<webwork source="Library/NAU/setStatistics/normal_dist2.pg">
		</webwork>
</exercise>

<p>
In the standard normal distribution, we have considered the case where you get the probability when given an interval.  However, what about the reverse problem of finding an interval that would result in a given probability?  That is, to solve for example the problem
<me>\Phi(b) - \Phi(a) = P(a &lt; z &lt; b) = 0.6217.</me>
To deal with this we need an "inverse function" <m>\Phi^{-1}</m>.  Toward that end, consider the simpler problem of solving
<me>\Phi(z_0) = P(z &lt; z_0) = \text{some given probability value} = \alpha.</me>
Then, technically the answer would be
<me>z_0 = \Phi^{-1}(\alpha).</me>
Since integrating the normal probability function is impossible you can expect that finding a nice formula for the inverse of that integration might also be challenging and that is certainly the case.  However, you have two options:
<ul>
<li>Guessing <m>z_0</m> until you get a probabilty that is close enough to <m>\alpha</m>.</li>
<li>Use a calculator that has the inverse built in!</li>
</ul>
For most graphing calculators, there is a function called "invNorm" and that is a way to compute values involving <m>\Phi^{-1}</m>.  Indeed, for example if you wanted to solve 
<me>\Phi(z_0) = P(z &lt; z_0) = 0.813</me>
then just use invNorm(0.413) to get <m>z_0 \approx -0.2198</m>. 


</p>



<exercise><title>WebWork - Inverse Standard Normal Calculations</title>

		<webwork source="Library/Rochester/setProbability10NormalDist/ur_pb_10_3.pg">
		</webwork>
</exercise>


	<p>
	While we are at it, can we "go backwards" and figure out the mean and variance if given some probabilities.
	</p>
	

<exercise><title>WebWork - Computing Regular Normal Probabilities</title>

		<webwork source="Library/NAU/setStatistics/normprob4.pg">
		</webwork>
</exercise>



<theorem xml:id="NormalCurveMax"><title>Normal Distribution Maximum</title>
<statement>
	<p>The maximum of the normal distribution probability function occurs when <m>x = \mu</m>
	</p>
</statement>
<proof>
	<p>
	Take the derivative of the probability function to get 
	<me>f'(x) = \frac{\sqrt{2} {\left(\mu - x\right)} e^{\left(-\frac{{\left(\mu - x\right)}^{2}}{2 \, \sigma^{2}}\right)}}{2 \, \sqrt{\pi} \sigma^{3}}</me>
	which is zero only when <m>x = \mu</m>.
	</p>
	<p>
	It is easy to see by evaluating to the left and right of this value that this critical value yields a maximum.
	</p>
</proof>
</theorem>


<p>
<theorem xml:id="NormalPOI"><title>Normal Distribution Points of Inflection</title>
<statement>
	<p>Points of Inflection for the normal distribution probability function occurs when <m>x = \mu + \sigma</m> and <m>x = \mu - \sigma</m>.
	</p>
</statement>
<proof>
	<p>Take the second derivative of the probability function to get 
	<me>f''(x) = \frac{\sqrt{2} {\left(\mu + \sigma - x\right)} {\left(\mu - \sigma - x\right)} e^{\left(-\frac{\mu^{2}}{2 \, \sigma^{2}} + \frac{\mu x}{\sigma^{2}} - \frac{x^{2}}{2 \, \sigma^{2}}\right)}}{2 \, \sqrt{\pi} \sigma^{5}}</me>
	which is zero only when <m>x = \mu \pm \sigma</m>.  
	</p>
	<p>
	It is easy to see by evaluating to the left and right of this value that these critical values yield points of inflection.
	</p>
</proof>
</theorem>
</p>


<p>
Notice that the work needed to complete the integrals over the entire domain above was pretty serious. To determine probabilities for a given interval is however not possible in general and therefore approximations are needed. When using TI graphing calculators, you can use 
<me>P( a \lt x \lt b ) = \text{normalcdf}(a,b,\mu, \sigma).</me>
Or you can use the calculator below.
</p>
	



<exercise><title>WebWork - Applying Normal Probabilities</title>

		<webwork source="Library/Rochester/setProbability10NormalDist/ur_pb_10_9.pg">
		</webwork>
</exercise>


</section>



<section xml:id="ChiSquareDistribution"><title>Chi-Square Distribution</title>
<p>The following distribution is related to both the Normal Distribution and to the Gamma Distribution. Initially, consider a gamma distribution with probability function given by the formula
	
	<me>y = \frac{x^{r-1} \cdot e^{-x / \mu}}{\Gamma(r) \cdot \mu^r}.</me>

Replacing <m>\mu = 2</m> and r with r/2 gives the special case

	<me>y = \frac{x^{r/2-1} \cdot e^{-x/2}}{\Gamma(r/2) \cdot 2^{r/2}}</me>

which is given a special name below.
</p>
	
<p>
<definition xml:id="ChiSquarePF"><title>Chi-Square Probability Function</title>
<statement>
<p>Given an natural number r, suppose X is a random variable over the space <m>R = (0,\infty)</m> with probability function given by
	<me>f(x) = \frac{x^{r/2-1} e^{-x/2} }{\Gamma(r/2) 2^{r/2}}.</me>
	Then X has a Chi-Square distribution with r degrees of freedom. This is often denoted <m>\chi^2(r)</m>.
</p>
</statement>
</definition>
</p>

<p>
<sage>
<input>
# Chi-Square Grapher
@interact
def _(r=slider(1,20,1,3,label='r =')):
    f = x^(r/2-1)*e^(-x/2)/(gamma(r/2)*2^(r/2))
    plot(f,x,0,40).show(title="Chi-Square Graph",figsize=[5,3])
</input>
</sage>
</p>

<p>
<theorem xml:id="ChiSquareStatistics"><title><m>\chi^2</m> statistics</title>
<statement>
		<me>\mu = r</me>
		<me>\sigma^2 = 2r</me>
		<me>\gamma_1 = 2 \sqrt{2/r}</me>
		<me>\gamma_2 = \frac{12}{r} + 3</me>
</statement>
</theorem>
</p>

<p>	
<theorem xml:id="NormalVsChiSquare"><title>Relationship between Normal and <m>\chi^2</m></title>
<statement>
	If <m>Z_1, Z_2, ..., Z_r</m> are r standard normal variables, then
	<me> X = \sum_{k=1}^r Z_k^2</me>
	is <m>\chi^2(r)</m>.
</statement>
</theorem>
</p>

<p>	
It also can be difficult to compute Chi-Square probabilities manually so you will perhaps want to use a numerical approximation in this case as well. The TI graphing calculator can be used with <m>\chi ^2</m>cdf(a,b,r).  Or, you can use the calculator below.
</p>

<p>
On your graphing calculator, you can use <m>P(a \le X \le b) \approx \chi^2</m>cdf(a,b,r).
</p>

<p>
<sage>
<input>
# Chi-Square Calculator
pretty_print("Calculator for Chi Square Probabilities")
@interact(layout=dict(top=[['a', 'b']],bottom=[['r']]))
def _(a=input_box(0,width=10,label='a = '),b=input_box(2,width=10,label='b = '),r=input_box(2,width=8,label='r =')):
    f = x^(r/2-1)*e^(-x/2)/(gamma(r/2)*2^(r/2))
    P = numerical_integral(f,a,b)[0]
    pretty_print(html("$$ P("+str(a)+" &lt; X &lt; "+str(b)+") \\approx "+str(P)+"$$"))
</input>
</sage>
</p>

	
</section>

<section xml:id="OtherBellShapedDistributions"><title>Other "Bell Shaped" distributions</title>
<p>The Normal distribution discussed above is very important when doing statistical analysis. It however is not the only distribution that is symmetrical about the mean and looks like a bell.  In this section, we consider two other options--one which is virtually useless and another which is very useful.
</p>

<p>
<definition xml:id="CauchyPF"><title>The Cauchy Distribution</title>
<statement>
<p>
Consider a continuous random variable on the real numbers defined by
<me>f(x) = \frac{1/\pi}{1+x^2}.</me>
A random variable with this probability function is said to be a Cauchy Distribution.
</p>
</statement>
</definition>
</p>


<theorem xml:id="CauchyStatistics"><title>The Cauchy Distribution</title>
<statement><me>f(x) = \frac{1/\pi}{1+x^2}</me>
is a probability function on <m>(-\infty, \infty)</m>.
</statement>
<proof>
Easily, note that 
<me>\int_{-\infty}^{\infty} \frac{1}{1+x^2} dx = tan^{-1}(\infty) - tan{-1}(-\infty) = \pi/2 - (-\pi/2) = \pi.</me>
Dividing by <m>\pi</m> gives the Cauchy probability function integrates to 1.
</proof>
</theorem>

<p>
Now that we have a probability function, it is important to determine its mean and variance. It should be obvious that when doing so using the Cauchy probability function, problems quickly arise.  Indeed,
<me>\int_{-\infty}^{\infty} x \frac{1}{1+x^2} dx = (1/2) ( \ln( | \infty |) - \ln( | -\infty |)</me>
which is problematic.  Further, even assuming that the distribution is symmetrical and therefore has a mean of 0, for the variance
<me>\int_{-\infty}^{\infty} x^2 \frac{1}{1+x^2} dx </me>
and note that the integrand does not converge to 0 at the endpoints and therefore the integral is automatically considered divergent.  Thus it is reasonable to note that the Cauchy distribution has no variance.
</p>
<p>On the other hand, there is another "bell-shaped" distribution that is useful and its random variable can be created by using a mixture of a normal variable and a <m>\Chi^2</m> variable. 
</p>

<p>
<definition xml:id="StudenttPF"><title>Student-t Distribution</title>
<statement>
<p>
Suppose Z is a standard normal variable and Y is <m>\Chi^2(r)</m> with Y and Z independent.  Define a new random variable
<me>T = \frac{Z}{\sqrt{Y/r}}.</me>
Then, T is said to have a (Student) t distribution with probability function given by 
<me>\frac{\Gamma \left ( \frac{n+1}{2} \right ) }{\sqrt{n \pi} \; \Gamma \left ( \frac{n}{2} \right ) } \left ( 1 + \frac{x^2}{n}\right )^{ - \left ( \frac{n+1}{2} \right )}  </me>
</p>  
</statement>
</definition>

The good news is that this distribution is useful and its properties are presented below without proof.

<theorem xml:id="StudenttStatistics"><title>Student t-distribution properties</title>
<statement>
<p>
For the Student t variable T defined above, 
<me> \mu = 0</me>
and if r>2
<me> \sigma^2 = \frac{r}{r-2}</me>
and if r>3
<me>\gamma_1 = 0</me>
and if r>4
<me>\gamma_2 = \frac{6}{r-4} + 3.</me>
</p>
</statement>
</theorem>
</p>

<p>
<example><title>Similarity between Normal and t-distributions for larger n</title>
<p>
Consider the probabilities <m>P(-2 \le Z \le 2)</m> vs <m>P(-2 \le T \le 2)</m> for a t-distribution with r=30 degrees of freedom.
</p>
<p>
For normal, <me>P(-2 \le Z \le 2) = \Phi(2) - \Phi(-2) = 0.9545</me>
while for t, <me>P(-2 \le T \le 2) = 0.9454.</me>
</p>
</example>
</p>

<p>
<sage language='r'>
<input>
# Display the Student's t distributions with various
# degrees of freedom and compare to the normal distribution
# Copied from www.statmethods.net

x &lt;- seq(-4, 4, length=100)
hx &lt;- dnorm(x)

degf &lt;- c(1, 3, 8, 30)
colors &lt;- c("red", "blue", "darkgreen", "gold", "black")
labels &lt;- c("df=1", "df=3", "df=8", "df=30", "normal")

plot(x, hx, type="l", lty=2, xlab="x value",
  ylab="Density", main="Comparison of t Distributions")

for (i in 1:4){
  lines(x, dt(x,degf[i]), lwd=2, col=colors[i])
}

legend("topright", inset=.05, title="Distributions",
  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)
</input>
</sage>
</p>

</section>


<section xml:id="NormalAssociatedGeneratingFunctions"><title>Generating Functions for Normal and Associated Distributions</title>


<p>
<xref ref="DefnMomentGeneratingFunction">Moment Generating Functions</xref> can be derived for each of the distributions in this chapter.
</p>


<p>

<theorem xml:id="NormalMGF"><title>Moment Generating Function for Normal</title>
<statement>
<p>Presuming <m>t \gt 0</m> and
<me>M(t) = e^{t \mu+\frac{1}{2}t^2\sigma^2}</me>
</p>
</statement>
<proof>
<p>Using the <xref ref="NormalPF">Normal probability function</xref>,
<md>
<mrow>M(t) &amp; = \int_{-\infty}^{\infty} e^{tx} \frac{1}{\sigma \sqrt{2 \pi}} e^{ -\left ( \frac{x-\mu}{\sigma} \right ) ^2 / 2}</mrow>
<mrow> &amp; = \int_{-\infty}^{\infty} e^{tx} \frac{1}{\sigma \sqrt{2 \pi}} e^{ -\left ( \frac{x-\mu}{\sigma} \right ) ^2 / 2} dx</mrow>
<mrow> &amp; = \int_{-\infty}^{\infty} e^{t(z \sigma + \mu)} \frac{1}{\sqrt{2 \pi}} e^{  -z ^2 / 2} dz</mrow>
<mrow> &amp; = e^{t \mu} \int_{-\infty}^{\infty}  \frac{1}{\sqrt{2 \pi}} e^{  -(z ^2 - 2t z \sigma ) / 2} dz</mrow>
<mrow> &amp; = = e^{t \mu} \int_{-\infty}^{\infty}  \frac{1}{\sqrt{2 \pi}} e^{  -(z ^2 - 2t z \sigma + t^2 \sigma^2 - t^2 \sigma^2 ) / 2} dz</mrow>
<mrow> &amp; = e^{t \mu+\frac{1}{2}t^2\sigma^2} \int_{-\infty}^{\infty}  \frac{1}{\sqrt{2 \pi}} e^{  -\left (z - t \sigma \right )^2 / 2} dz</mrow>
<mrow> &amp; = e^{t \mu+\frac{1}{2}t^2\sigma^2} \cdot 1 </mrow>
</md>
where the final integral is just a shifted standard normal and therefore has value 1.
</p>
</proof>
</theorem>

<corollary><title>Normal Properties via Moment Generating Function</title>
<statement>
<p>For the Normal variable X,
<me> M(0) = 1</me>
<me> M'(0) = \mu</me>
<me> M''(0) = \sigma^2 + \mu^2</me>
</p>
</statement>
<proof>
<p>
<me> M(0) = e^{0 \mu+\frac{1}{2}0^2\sigma^2} = e^0 = 1.</me>
Continuing,
<me> M'(t) = {\left(\sigma^2 t + \mu \right)} e^{\left(\frac{1}{2} \sigma^2 t^{2} + \mu t \right)}
</me>
and therefore
<me> M'(0) = {\left(\sigma^2 0 + \mu \right)} e^{\left(\frac{1}{2} \sigma^2 0^{2} + \mu 0 \right)} = \mu e^0 = \mu.</me>
Continuing with the second derivative,
<me> M''(t) = {\left(\sigma^2 t + \mu\right)}^2 e^{\left(\frac{1}{2} \sigma^2 t^2 + \mu t\right)} + \sigma^2 e^{\left(\frac{1}{2} \sigma^2 t^2 + \mu t\right)}
</me>
and therefore
<me> M''(0) = {\left(\sigma^2 0 + \mu\right)}^2 e^{\left(\frac{1}{2} \sigma^2 0^2 + \mu 0\right)} + \sigma^2 e^{\left(\frac{1}{2} \sigma^2 0^2 + \mu 0\right)} = \mu^2 + \sigma^2 </me>
which is the squared mean plus the variance for the normal distribution.
</p>
</proof>
</corollary>

</p>


<p>

<theorem xml:id="ChiSquareMGF"><title>Moment Generating Function for Chi-Square</title>
<statement>
<p>Presuming <m>t \gt 0</m> and
<me>M(t) = \left ( \frac{1}{1-2t} \right )^{r/2}</me>
</p>
</statement>
<proof>
<p>Using the <xref ref="ChiSquarePF">Chi-Square probability function</xref>,
<md>
<mrow>M(t) &amp; = \int_0^{\infty} e^{tx} \frac{x^{r/2-1} e^{-x/2} }{\Gamma(r/2) 2^{r/2} } dx</mrow>
<mrow> &amp; = \int_0^{\infty} \frac{x^{r/2-1} e^{-x(1-2 t)/2} }{\Gamma(r/2) 2^{r/2}} dx</mrow>
<mrow> &amp; = \int_0^{\infty} \frac{\left ( \frac{u}{1-2t} \right )^{r/2-1} e^{-u/2} }{\Gamma(r/2) 2^{r/2} } \frac{1}{1-2t} du</mrow>
<mrow> &amp; = \left ( \frac{1}{1-2t} \right )^{r/2} \int_0^{\infty} \frac{u^{r/2-1} e^{-u/2} }{\Gamma(r/2) 2^{r/2}} du</mrow>
<mrow> &amp; = \left ( \frac{1}{1-2t} \right )^{r/2}</mrow>
</md>
where the final integral is again Chi-Square and therefore has value 1.
</p>
</proof>
</theorem>

<corollary><title>Chi-Square Properties via Moment Generating Function</title>
<statement>
<p>For the Chi-Square variable X, 
<me> M(0) = 1</me>
<me> M'(0) = r = \mu</me>
<me> M''(0) = 2r + r^2 = \sigma^2 + \mu^2</me>
</p>
</statement>
<proof>
<p>
<me> M(0) = \left ( \frac{1}{1-2 \cdot 0} \right )^{r/2} = 1.</me>
Continuing,
<me> M'(t) = \frac{r {\left(-2 t + 1\right)}^{\frac{1}{2} r - 1}}{{\left(-2 t + 1 \right)}^{r}}
</me>
and therefore
<me> M'(0) = \frac{r {\left(-2 \cdot 0 + 1\right)}^{\frac{1}{2} r - 1}}{{\left(-2 \cdot 0 + 1 \right)}^{r}} = \frac{r}{1} = r.</me>
Continuing with the second derivative,
<me> M''(t) = -\frac{{\left(r - 2\right)} r {\left(-2 t + 1\right)}^{\frac{1}{2} r - 2}}{{\left(-2 t + 1\right)}^{r}} + \frac{2 r^{2} {\left(-2 t + 1\right)}^{r - 2}}{{\left(-2 t + 1\right)}^{\frac{3}{2} r}}

</me>
and therefore
<md> 
<mrow>M''(0) &amp; = -\frac{{\left(r - 2\right)} r {\left(-2 \cdot 0 + 1\right)}^{\frac{1}{2} r - 2}}{{\left(-2 \cdot 0 + 1\right)}^{r}} + \frac{2 r^{2} {\left(-2 \cdot 0 + 1\right)}^{r - 2}}{{\left(-2 \cdot 0 + 1\right)}^{\frac{3}{2} r}} </mrow>
<mrow> &amp; = -\frac{{\left(r - 2\right)} r }{1} + \frac{2 r^{2} }{1}= 2r + r^2 = \sigma^2 + \mu^2</mrow>
</md>
which is the squared mean plus the variance for the normal distribution.
</p>
</proof>
</corollary>

</p>

<p>It is interesting to note that the moment generating functions are not defined for the <xref ref="CauchyPF">Cauchy Distribution</xref> or for the <xref ref="StudenttPF">Student's t distribution</xref>.
</p>
</section>



<section xml:id="NormalAsLimitingDistribution"><title>Normal Distribution as a Limiting Distribution</title>
<p>Over the past several chapters you should have noticed that many distributions have skewness and kurtosis formulae which have limiting values of 0 and 3 respectively. This means that each of those distributions which can be approximated by the normal distribution for "large" parameter values.
</p>


<p>To see how this works, consider a "random" distribution in the following two interactive experiments.  For the first graph below, a sequence of N random samples, each of size r, ranging from 0 to "Range" is generated and graphed as small data points.  As the number of samples N and the sample size r increase, notice that the data seems to cover the entire range of possible values relatively uniformly.  (For this scatter plot note that each row represents the data for one sample of size r.  The larger the N, the greater the number of rows.)  Each row is averaged and that mean value is plotted on the graph as a red circle.  If you check the "Show_Mean" box, the mean of these circles is indicated by the green line in the middle of the plot.
</p>

<p>
For the second graph below, the means are collected and the relative frequency of each is plotted.  As N increases, you should see that the results begin to show an interesting tendency.   As you increase the data range, you may notice this graph has a larger number of data values.  Smoothing groups this data into intervals of length two for perhaps a graph with less variability.
</p>

<p>
Consider each of the following:
<ul>
<li>
As N increases with single digit values of r, what appears to happen to the mean and range of the means?  How does increasing the data range from 1-100 to 1-200 or 1-300 affect these results?</li>
<li>As N increases (say, for a middle value of r), what appears to happen to the means?  How does increasing the data range from 1-100 to 1-200 or 1-300 affect these results?</li>
<li>As r increases (say, for a middle value of N), what appears to happen to the range of the averages?  Does your conclusion actually depend upon the value of N?  (Look at the graph and don't worry about the actual numerical values.)
How does increasing N for the second graph affect the skewness and kurtosis of that graph?  Do things change significantly as r is increased?  </li>
</ul>
</p>

<p>
<sage>
<input>
var('n,k')
from sage.finance.time_series import TimeSeries

@interact(layout=dict(top=[['Range'],['Show_Mean', 'Smoothing']],  
bottom=[['N'],['r']]))

def _(Range=[100,200,300,500],
      N=slider(5,200,2,2,label="N = Number of Samples"),
      r=slider(3,200,1,2,label="r = Sample Size"),
      Show_Mean=False,Smoothing=False):
    R=[1..N]     #  R ranges over the number of samples...will point to the list of averages
    rangemax = Range

    data = random_matrix(ZZ,N,r,x=rangemax)
    datapoints = []
    avg_values = []
    avg_string = []
    averages = []
    for n in range(N):
        temp = 0
        for k in range(r):
            datapoints += [(data[n][k],n)]
            temp += data[n][k]
        avg_values.append(round(temp/r))
        if Smoothing:
            avg_string.append(str(2*round((temp/r)/2)))    
        else:
            avg_string.append(str(round(temp/r)))
            
        averages += [(round(temp/r),n)]   #  make these averages integers for use in grouping later
    SCAT = scatter_plot(datapoints,markersize=2,edgecolor='red',figsize=(10,4))
    AVGS = scatter_plot(averages,markersize=50,edgecolor='blue',marker='o',figsize=(7,4))
    
    freqslist = frequency_distribution(avg_string,1).function().items()
       
       
# compute sample statistics for the raw data as well as for the N averages
    Mean_data = (sum(sum(data))/(N*r)).n()
#    STD_data = sqrt(sum(sum( (data-Mean_data)^2 ))/(N*r)).n()
    Mean_averages = mean(avg_values).n()
#    STD_averages = sqrt(variance(avg_values).n())
#    print "Data mean =",Mean_data," vs Mean of the averages =",Mean_averages
#    print "Data STD = ",STD_data," vs Standard Dev of avgs =", STD_averages
    if Show_Mean:
        avg_line = line([(Mean_data,0),(Mean_data,N-1)],rgbcolor='green',thickness=10)
        avg_text = text('xbar',(Mean_data,N),horizontal_alignment='right',rgbcolor='green')
    else:
        avg_line = Graphics()
        avg_text = Graphics()
            
#  Plot a scatter plot exhibiting uniformly random data and the collection of averages 
    print("The random data plot below shows each row representing a sample with size determined by")
    print("the slider above and each circle representing the average for that particular sample.")
    print("First, keep sample size relatively low and increase the number of samples.  Then,")
    print("watch what happens when you slowly increase the sample size.")

    
#  Plot the relative frequencies of the grouped sample averages
    print(html("Now, the averages (ie. the circles) from above are collected and counted\n"+
         "with the relative frequency of each average graphed below.  For a relatively large number of\n"+
         "samples, notice what seems to happen to these averages as the sample size increases."))
    if Smoothing:
        binRange = Range//2
    else:
        binRange = Range
    
    # normed=True  # if you want to have relative frequencies below
    
    his_low = 2*rangemax/7
    his_high = 5*rangemax/7
    
    T = histogram(avg_values,density=False,bins=binRange,range=(his_low,his_high),title='Sample Averages vs Frequency') 
    
    (SCAT+AVGS+avg_line+avg_text).show(figsize=(5,4),title='Sample values vs Sample number')

    T.show(figsize=(5,2))
</input>
</sage>
</p>



<p>
So, even with random data, if you are to consider the arrangement of the collected means rather than the arrangement of the actual data then the means appear to have a bell-shaped distribution as well.
</p>

</section>


<section xml:id="CentralLimitTheoremSection"><title>Central Limit Theorem</title>
<p>
Often, when one wants to solve various scientific problems, several assumptions will be made regarding the nature of the underlying setting and base their conclusions on those assumptions.  Indeed, if one is going to use a Binomial Distribution or a Negative Binomial Distribution, an assumption on the value of p is necessary.  For Poisson and Exponential Distributions, one must know the mean.  For Normal Distributions, one must assume values for both the mean and the standard deviation.   Where do these values come from?  Often, one may perform a preliminary study and obtain a sample statistic...such as a sample mean or a relative frequency and use these values for μ or p.</p>

<p>
But what is the underlying distribution of these sample statistics?  The Central Limit Theorem gives the answer...</p>


<p>The results from the previous section illustrate the tendency for bell-shaped distributions. This tendency can be described more mathematically through the following theorem. It is presented here without proof.
</p>

<p>
<theorem xml:id="CentralLimitTheorem"><title>Central Limit Theorem</title>
	<statement>
	<p>
	Presume X is a random variable from a distribution with known mean <m>\mu</m> and known variance <m>\sigma_x^2</m>.  
	For some natural number n, sample the distribution repeatedly creating a string of random variables denoted <m>X_1, X_2, ... , X_n</m> and set <m>\overline{X} = \frac{\sum X_k}{n}</m>. 
	</p>
	<p> 
	Then, <m>\overline{X}</m> is approximately normally distributed with mean <m>\mu</m> and variance <m>\sigma^2 = \frac{\sigma_x^2}{n}</m>.
	</p>
	</statement>
</theorem>
</p>



<p>
Often the Central Limit Theorem is stated more formally using a conversion to standard units. Indeed, the theorem indicates that the random variable <m>\overline{X}</m> has variance <m>\frac{\sigma^2}{n}</m> which means as n grows this variance approaches 0. So, the limiting random variable has a zero variance and therefore is no longer a random variable. To avoid this issue, the Central Limit Theorem is often stated as:
</p>
<p>For random variables
<me>W_n = \frac{\overline{X} - \mu}{\sigma/ \sqrt{n}}</me>
with corresponding distribution function <m>F_n(W_n)</m>,
<me>\lim_{n \rightarrow \infty} F_n(c) = \int_{-\infty}^c \frac{1}{\sqrt{2 \pi}} e^{-z^2/2} dz = \Phi(c)</me>
that is, the standard normal distribution function.
</p>

	<p>
	While we are at it, we can again "go backwards" and figure out the mean and variance if given some probabilities.
	</p>
	

<exercise><title>WebWork - Computing probabilities on <m>\overline{X}</m></title>

		<webwork source="Library/UVA-Stat/setStat212-Homework07/stat212-HW07-06.pg">
		</webwork>
</exercise>


<p>
<example><title>Exponential X vs Normal <m>\overline{X}</m></title>
<p>Consider an exponential variable X with mean time till first success of <m>\mu = 4</m>.  Then, <m>\sigma = 2</m> using the exponential formulas.
</p>
<p>You can use the exponential probability function to compute probabilities dealing with X. Indeed,
<me>P(X \lt 3.9) = F(3.9) = 1 - e^{-3.9/4} \approx 0.6228 .</me>
</p>
<p>
If instead you plan to sample from this distribution n=32 times, the Central Limit Theorem implies that you will get a random variable <m>\overline{X}</m> which has an approximate normal distribution with the same mean but with new variance <m>\sigma_{\overline{X}}^2 = \frac{4}{32} = \frac{1}{8}</m>.  Therefore
<me>P( \overline{X} \lt 3.9 ) \\ \approx \text{normalcdf}(0,3.9,4,sqrt(1/8)) = 0.3886 .</me>
</p>
</example>
</p>

<p>When converting probability problems from continuous (such as exponential or uniform) then no adjustment to the question is needed since you are approximating one area with another area. However, when converting probability problems from discrete (such as binomial or geometric) then you need to consider how the interval would need to be adjusted so that histogram areas for the discrete problem would relate to areas under the normal curve. Generally, you will need to expand the stated interval each way by 1/2.
</p>


<p>
The Central Limit Theorem provides that regardless of the distribution of X, the distribution of an average of X's is approximately normally distributed. However, it also shows why X may also be approximated for some distributions using the normal distribution as certain parameters are allowed to increase. Below, you can see how Binomial and Poisson distributions can be approximated directly using the Normal distribution.
</p>

<p>Toward that end, for <m>0 \lt p \lt 1</m> consider a sequence of Bernoulli trials <m>Y_1, Y_2, ..., Y_n</m> with each over the space {0,1}. Then, 
<me>X = \sum_{k=1}^n Y_k</me>
is a Binomial variable.
</p>

<theorem xml:id="BinomialApproximatelyNormal"><title>The Binomial Distribution approximately Normal if n large.</title>
<statement>
<p>
Given a Binomial variable X with <m>\mu = np</m> and <m>\sigma^2 = np(1-p)</m>, then X is approximately also normal with the same mean and variance so long as <m>np \gt 5</m> and <m>n(1-p) \gt 5</m>.
</p>
</statement>
<proof>
<p>
Using the Bernoulli variables <m>Y_k</m> each with mean p and variance p(1-p), note that the Central Limit Theorem applied to <m>\overline{X} = \frac{\sum Y_k}{n}</m> gives that
<me>\frac{\overline{X}-p}{\sqrt{p(1-p)/n}}</me>
is approximately standard normal. By multiplying top and bottom by n yields
<me>\frac{\sum Y_k - np}{\sqrt{np(1-p)}}</me>
is approximately standard normal. But <m>\sum Y_k</m> actually is the sum of the number of successes in n trials and is therefore a Binomial variable.
</p>
</proof>
</theorem>


<example><title>Binomial as Normal</title>
<p>Binomial becomes normal as <m> n \rightarrow \infty</m>.  Consider n = 50 and p = 0.3.  Then, <m>\mu = 15</m> and <m>\sigma^2 = 10.5</m>.   
</p>
<p>Using the binomial formulas, for example,
<me>P( X = 16 ) = \binom{50}{16} 0.3^{16} \cdot 0.7^{34} \approx 0.11470</me>
Using the normal distribution,
<md>
	<mrow>P( X = 16 ) &amp; = P( 15.5 \lt X \lt 16.5) </mrow>
	<mrow> &amp; \approx normalcdf(15.5,16.5,15,sqrt(10.5)) </mrow>
	<mrow> &amp; = 0.11697</mrow>
</md>
Notice that these are very close.
</p>
</example>



<corollary xml:id="PoissonApproximatelyNormal"><title>The Poisson Distribution is approximately normal with <m>\mu</m> large.</title>
<statement>
<p>
Given a Poisson variable X with <m>\mu</m> and <m>\sigma^2 = \mu</m> given, then X is approximately also normal with the same mean and variance so long as <m>\mu \gt 5</m>.
</p>
</statement>
<proof>
<p>
Note from before that the Poisson distribution function was derived by approximating with Binomial and letting n approach infinity. Therefore, by the previous theorem, the Poisson variable is also approximately Normal using the Poisson mean and variance rather than the binomial's. Indeed, in standard units
<me>\frac{Y - \mu}{\sqrt{\mu}}</me>
is approximately normal for large <m>\mu</m>.
</p>
</proof>
</corollary>
	
	
	
<example><title>Poisson as Normal</title>
<p>Poisson becomes normal as <m> \mu \rightarrow \infty</m>.  Consider <m>\mu = 20</m>.  Then, <m>\sigma^2 = \mu = 20</m>.   
</p>
<p>Using the Poisson formulas, for example,
<me>P( X = 19 ) = \frac{20^{19} e^{-20}}{19!} \approx 0.08883</me>
Using the normal distribution,
<md>
	<mrow>P( X = 19 ) &amp; = P( 18.5 \lt X \lt 19.5) </mrow>
	<mrow> &amp; \approx normalcdf(18.5,19.5,20,sqrt(20)) </mrow>
	<mrow> &amp; = 0.08683</mrow>
</md>
Again, these are very close.
</p>
</example>

	
	
<theorem xml:id="GammaApproximatelyNormal"><title>The Gamma is approximate Normal for <m>\mu</m> large.</title>
<statement>
<p>
Given a Gamma variable X with mean <m>r \mu</m> and variance <m> r BLOB</m> given, then X is approximately also normal with the same mean and variance so long as CONDITION????.
</p>
</statement>
</theorem>
	
	

<example><title>Gamma as Normal</title>
<p>Gamma becomes normal as <m> r \rightarrow \infty</m>.  Assume that the average time till a first success is 12 minutes and that <m>r = 8</m>.  Then, the mean for the Gamma distribution is <m>\mu = 12 \cdot 8 = 96</m> and <m>\sigma^2 = 8 \cdot 12^2 = 1152</m> and so <m>\sigma \approx 33.9411</m>.   
</p>
<p>Using the Gamma formulas, 
<md>
	<mrow>P( 90 \le X \le 100 ) &amp; = \int_{90}^{100} f(x) dx </mrow>
	<mrow> &amp; = 0.59252 - 0.47536 = 0.11716.</mrow>
</md>
Using the normal distribution,
<me>P( 90 \le X \le 100) \approx normalcdf(90,100,96,33.9411) = 0.11707.</me>
Amazingly, these are also very close.
</p>
</example>
	

<example><title>Uniform X vs Normal <m>\overline{X}</m></title>
<p>Consider a discrete uniform variable X over R = {1,2,...,20}.  Then, <m>\mu = 10.5</m> and <m>\sigma = \frac{20^2-1^2}{20}</m> using the uniform formulas.
</p>
<p>You can use the uniform probability function to compute probabilities dealing with X. Indeed,
<me>P(8 \le X \lt 12) = P(X \in \{8,9,10,11 \} = \frac{4}{20} = 1/5.</me>
</p>
<p>
If instead you plan to sample from this distribution n=49 times, the Central Limit Theorem implies that you will get a random variable <m>\overline{X}</m> which has an approximate normal distribution with the same mean but with new variance <m>\sigma_{\overline{X}}^2 = \frac{199/20}{49} = \frac{199}{580}</m>.  Therefore, expanding the interval to include the boundaries of the corresponding histogram areas,
<me>P( 8 \le \overline{X} \lt 12 ) = P(7.5 \le \overline{X} \le 11.5) \approx normalcdf(7.5,11.5,10.5,0.585750) \approx 0.9561 .</me>
</p>
</example>


<p>
As these examples illustrate, you will have increasing success in approximating the desired probabilities so long as the distribution's corresponding parameter is allowed to be "sufficiently large". The mathematical reasoning this is true is not provided but depends upon the "Central Limit Theorem" discussed in the next section.
</p>


<p>
The above theorems allow you to utilize the normal distribution to compute approximate probabilities for the variable X in the stated distributions. This is not always true for all distributions since some do not have parameters which allow for approaching normality. However, regardless of the distribution the Central Limit Theorem always allows you to approximate probabilities if they involve an average of repeated attempts...that is, for variable <m>\overline{X}</m>. This usefulness is illustrated in the examples below.
</p>

</section>


<section xml:id="NormalRelatedDistributionsSummary"><title>Summary</title>

<p>
Here is a summary of the major points in this chapter:
</p>

<p>
TBA
</p>
</section>


<section xml:id="NormalRelatedDistributionsExercises"><title>Exercises</title>


<exercise><title> - Computing basic standard normal probabilities</title>
<p>
Compute
<ul>
	<li><me>P( Z \gt 0)</me></li>
	<li><me>P( Z \lt 0.892)</me></li>
	<li><me>P( Z \lt -0.892)</me></li>
	<li><me>P( -1.45 \lt Z \lt 2.37)</me></li>
	<li><me>P( -1 \lt Z \lt 1)</me>
which is the probability of lying within 1 standard deviation of the mean.</li>
	<li><me>P( -2 \lt Z \lt 2)</me>
which is the probability of lying within 2 standard deviations of the mean.</li>
	<li><me>P( -3 \lt Z \lt 3)</me>
which is the probability of lying within 3 standard deviations of the mean.</li>
	<li>A value for a so that <me>P( Z \lt a) = 0.8</me>
which would be the location of the 80th percentile.</li>
</ul>
</p>
</exercise>

<exercise><title> - Computing basic normal probabilities</title>
<p>Given <m>\mu = 25</m> and <m>\sigma = 4</m> compute
<me>P(X \lt \mu)</me>
<me>P( X \gt 26)</me>
<me>P( X \gt 22)</me>
<me>P( 20 \le X \le 26)</me>
</p>
</exercise>

<exercise><title> - IQ values</title>
<p>The Intelligence Quotient (IQ) is a measure of your ability to think and reason. Presuming that IQ scores are normally distributed with mean 100 and standard deviation 15, determine the location of the 90th percentile.  That is, the IQ score below which you will find approximately 90% of other IQ scores.
</p>
</exercise>

</section>


</chapter>
